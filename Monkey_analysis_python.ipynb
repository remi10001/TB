{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Time Since TB Infection in Macaques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to apply deep learning algorithms to analyzing the monkey data. I need to:\n",
    "- Transfer over files for middle and late infection, just the microarray data in one file, and the clinical data in another file, only for those monkeys\n",
    "- Set up a training and test set.\n",
    "    - I want 3 latent and 3 active in test set\n",
    "- Before I set up a 10-fold cross-validation scheme, I think it is okay to just see if I can get a model to train on the training set. I definitely want to train a model just on the training set as opposed to the whole dataset together, to start off with at least some good practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "#from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = \"/master/rault/TB\"\n",
    "data_path = path + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/master/rault/TB/data\n",
      "Monkey_PhenoData_middle-late.txt\r\n",
      "Monkey_Processed_ExpressionData_middle-late.txt\r\n"
     ]
    }
   ],
   "source": [
    "%cd $data_path\n",
    "%ls \n",
    "\n",
    "pheno = pd.read_table(\"Monkey_PhenoData_middle-late.txt\")\n",
    "expres = pd.read_table(\"Monkey_Processed_ExpressionData_middle-late.txt\")\n",
    "#Monkey_PhenoData_middle-late.txt\n",
    "#Monkey_Processed_ExpressionData_middle-late.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to be consistent\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "# select the latent monkeys\n",
    "latent_monkeys = pheno.loc[pheno[\"clinical.status\"] == \"Latent\"][\"monkeyid\"].tolist()\n",
    "\n",
    "# select the active monkeys\n",
    "active_monkeys = pheno.loc[pheno[\"clinical.status\"] == \"Active\"][\"monkeyid\"].tolist()\n",
    "\n",
    "# set(latent_monkeys) & set(active_monkeys) #-> They are correctly disjoint\n",
    "\n",
    "# Randomly select 3 latent monkeys\n",
    "test_latent_monkeys = random.sample(latent_monkeys, 3)\n",
    "\n",
    "# randomly select 3 active monkeys\n",
    "test_active_monkeys = random.sample(active_monkeys, 3)\n",
    "\n",
    "test_monkeys = test_latent_monkeys + test_active_monkeys\n",
    "\n",
    "# remove these monkeys from the training set  and put in a test set (both the clinical variables and the expression)\n",
    "train_pheno = pheno.loc[pheno[\"monkeyid\"].isin(set(pheno[\"monkeyid\"]) - set(test_monkeys))]\n",
    "test_pheno = pheno.loc[pheno[\"monkeyid\"].isin(test_monkeys)]\n",
    "\n",
    "#set(train_set[\"monkeyid\"]) & set(test_set[\"monkeyid\"]) #-> They are correctly disjoint\n",
    "\n",
    "train_exprs = expres[expres.index.isin(list(train_pheno.index))]\n",
    "test_exprs = expres[expres.index.isin(list(test_pheno.index))]\n",
    "\n",
    "train_exprs = train_exprs.astype(float)\n",
    "test_exprs = test_exprs.astype(float)\n",
    "\n",
    "train_exprs = train_exprs.as_matrix()\n",
    "test_exprs = test_exprs.as_matrix()\n",
    "#DataFrame.as_matrix\n",
    "#X = dataset[:,0:4].astype(float)\n",
    "# set(test_exprs.index) & set(train_exprs.index) #-> They are correctly disjoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ChIP</th>\n",
       "      <th>hyb.chamber</th>\n",
       "      <th>dataset</th>\n",
       "      <th>synchroset</th>\n",
       "      <th>monkeyid</th>\n",
       "      <th>time.point</th>\n",
       "      <th>infection.time</th>\n",
       "      <th>clinical.status</th>\n",
       "      <th>description</th>\n",
       "      <th>description.1</th>\n",
       "      <th>time.point.comb</th>\n",
       "      <th>time.period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSM2227796</th>\n",
       "      <td>M19_56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Training</td>\n",
       "      <td>No</td>\n",
       "      <td>M19</td>\n",
       "      <td>56</td>\n",
       "      <td>D56</td>\n",
       "      <td>Active</td>\n",
       "      <td>M19_56</td>\n",
       "      <td>6303256020_D.AVG_Signal</td>\n",
       "      <td>56</td>\n",
       "      <td>middle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title  ChIP  hyb.chamber   dataset synchroset monkeyid  \\\n",
       "GSM2227796  M19_56     1            1  Training         No      M19   \n",
       "\n",
       "            time.point infection.time clinical.status description  \\\n",
       "GSM2227796          56            D56          Active      M19_56   \n",
       "\n",
       "                      description.1  time.point.comb time.period  \n",
       "GSM2227796  6303256020_D.AVG_Signal               56      middle  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_set.index\n",
    "train_set[train_set.index.isin(['GSM2227796'])]  # This somehow works! so can subset by the rows in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.39462752,  4.48762888,  5.96935506, ...,  3.87268578,\n",
       "         4.32225507,  6.96685288],\n",
       "       [ 4.12352933,  4.15348654,  5.53672825, ...,  5.24086738,\n",
       "         5.91572326,  6.71413883],\n",
       "       [ 3.39462752,  3.42627115,  5.63217959, ...,  3.87268578,\n",
       "         3.54491484,  7.23525856],\n",
       "       ..., \n",
       "       [ 3.25470823,  3.90686403,  5.60626114, ...,  3.98340472,\n",
       "         6.18424082,  6.8451059 ],\n",
       "       [ 3.59531892,  4.6930189 ,  5.71572579, ...,  5.22307736,\n",
       "         5.31778927,  6.75539068],\n",
       "       [ 3.25470823,  4.58647098,  6.24496345, ...,  4.99904663,\n",
       "         6.18319178,  6.72783263]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_exprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for loading into keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This website from Jason Brownlee has excellent tutorial on using pandas to load in data and then use keras. I can use his code to help me\n",
    "\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_pheno[\"time.period\"])\n",
    "encoded_Y_train = encoder.transform(train_pheno[\"time.period\"])\n",
    "encoded_Y_test = encoder.transform(test_pheno[\"time.period\"])\n",
    "\n",
    "train_Y = np_utils.to_categorical(encoded_Y_train)\n",
    "test_Y = np_utils.to_categorical(encoded_Y_test)\n",
    "# encode class values as integers\n",
    "#encoder = LabelEncoder()\n",
    "#encoder.fit(Y)\n",
    "#encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# One-hot encode the output\n",
    "#train_pheno[\"time.period\"].dtype\n",
    "#Y_train = np_utils.to_categorical(train_pheno[\"time.period\"].cat.codes, 2)\n",
    "\n",
    "#dataframe['c'].cat.codes\n",
    "\n",
    "#Y_train = np_utils.to_categorical(y_train, 10)\n",
    "#Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0]\n",
      "GSM2227796    middle\n",
      "GSM2227797      late\n",
      "GSM2227799    middle\n",
      "GSM2227800      late\n",
      "GSM2227801    middle\n",
      "GSM2227805      late\n",
      "GSM2227806      late\n",
      "GSM2227807    middle\n",
      "GSM2227808      late\n",
      "GSM2227809    middle\n",
      "GSM2227810      late\n",
      "GSM2227812    middle\n",
      "GSM2227814      late\n",
      "GSM2227815    middle\n",
      "GSM2227816      late\n",
      "GSM2227818    middle\n",
      "GSM2227820      late\n",
      "GSM2227823      late\n",
      "GSM2227825    middle\n",
      "GSM2227827      late\n",
      "GSM2227828    middle\n",
      "GSM2227832    middle\n",
      "GSM2227834    middle\n",
      "GSM2227835    middle\n",
      "GSM2227836    middle\n",
      "GSM2227837    middle\n",
      "GSM2227839      late\n",
      "GSM2227842    middle\n",
      "GSM2227843      late\n",
      "GSM2227844    middle\n",
      "               ...  \n",
      "GSM2228190      late\n",
      "GSM2228192      late\n",
      "GSM2228193      late\n",
      "GSM2228197    middle\n",
      "GSM2228201    middle\n",
      "GSM2228203      late\n",
      "GSM2228204      late\n",
      "GSM2228208      late\n",
      "GSM2228211    middle\n",
      "GSM2228212      late\n",
      "GSM2228215    middle\n",
      "GSM2228218    middle\n",
      "GSM2228224    middle\n",
      "GSM2228225    middle\n",
      "GSM2228228    middle\n",
      "GSM2228229      late\n",
      "GSM2228232    middle\n",
      "GSM2228237      late\n",
      "GSM2228238    middle\n",
      "GSM2228239      late\n",
      "GSM2228240      late\n",
      "GSM2228243    middle\n",
      "GSM2228244    middle\n",
      "GSM2228245    middle\n",
      "GSM2228246    middle\n",
      "GSM2228247    middle\n",
      "GSM2228251    middle\n",
      "GSM2228253    middle\n",
      "GSM2228258    middle\n",
      "GSM2228261      late\n",
      "Name: time.period, Length: 246, dtype: object\n",
      "[0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 0 1 0 0 0 0 1 1 0 0 1]\n",
      "GSM2227793      late\n",
      "GSM2227802      late\n",
      "GSM2227813      late\n",
      "GSM2227833      late\n",
      "GSM2227841    middle\n",
      "GSM2227859    middle\n",
      "GSM2227864      late\n",
      "GSM2227871      late\n",
      "GSM2227886      late\n",
      "GSM2227895    middle\n",
      "GSM2227913      late\n",
      "GSM2227943    middle\n",
      "GSM2227945    middle\n",
      "GSM2227963    middle\n",
      "GSM2228019    middle\n",
      "GSM2228023    middle\n",
      "GSM2228036      late\n",
      "GSM2228037    middle\n",
      "GSM2228056    middle\n",
      "GSM2228058    middle\n",
      "GSM2228059      late\n",
      "GSM2228075    middle\n",
      "GSM2228077      late\n",
      "GSM2228085    middle\n",
      "GSM2228097    middle\n",
      "GSM2228105      late\n",
      "GSM2228108    middle\n",
      "GSM2228112      late\n",
      "GSM2228115    middle\n",
      "GSM2228136      late\n",
      "GSM2228139      late\n",
      "GSM2228144    middle\n",
      "GSM2228145      late\n",
      "GSM2228147    middle\n",
      "GSM2228167      late\n",
      "GSM2228170    middle\n",
      "GSM2228176    middle\n",
      "GSM2228196      late\n",
      "GSM2228199    middle\n",
      "GSM2228202      late\n",
      "GSM2228223      late\n",
      "GSM2228230      late\n",
      "GSM2228231      late\n",
      "GSM2228242    middle\n",
      "GSM2228250    middle\n",
      "GSM2228254      late\n",
      "GSM2228255      late\n",
      "GSM2228257    middle\n",
      "Name: time.period, dtype: object\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_Y_train)\n",
    "print(train_pheno[\"time.period\"])\n",
    "print(encoded_Y_test)\n",
    "print(test_pheno[\"time.period\"])\n",
    "print(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 9050)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_exprs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.0\n",
    "\n",
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=train_exprs.shape[1:]),\n",
    "    Dense(5000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(500, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "246/246 [==============================] - 1s - loss: 0.7610 - acc: 0.5610\n",
      "Epoch 2/9\n",
      "246/246 [==============================] - 1s - loss: 0.4108 - acc: 0.8130\n",
      "Epoch 3/9\n",
      "246/246 [==============================] - 1s - loss: 0.2481 - acc: 0.9553\n",
      "Epoch 4/9\n",
      "246/246 [==============================] - 1s - loss: 0.1910 - acc: 0.9919\n",
      "Epoch 5/9\n",
      "246/246 [==============================] - 1s - loss: 0.1647 - acc: 0.9959\n",
      "Epoch 6/9\n",
      "246/246 [==============================] - 1s - loss: 0.1497 - acc: 1.0000\n",
      "Epoch 7/9\n",
      "246/246 [==============================] - 1s - loss: 0.1388 - acc: 1.0000\n",
      "Epoch 8/9\n",
      "246/246 [==============================] - 1s - loss: 0.1308 - acc: 1.0000\n",
      "Epoch 9/9\n",
      "246/246 [==============================] - 1s - loss: 0.1250 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafd16d5908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr=0.001\n",
    "model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_exprs, [[y] for y in encoded_Y_train], batch_size=train_exprs.shape[0], epochs=9)\n",
    "\n",
    "#model.fit(train_exprs, encoded_Y_train, validation_data = (test_exprs, test_Y), batch_size=train_exprs.shape[0], epochs=30)\n",
    "# I was getting problems from train_exprs being a pandas object. Probably can learn how to change that closer to \n",
    "\n",
    "\n",
    "#da_dis_model = Sequential(get_my_layers(p))\n",
    "#da_dis_model.compile(optimizer=Adam(lr=0.001),\n",
    "#             loss=\"categorical_crossentropy\",\n",
    "#             metrics=['accuracy'])\n",
    "\n",
    "#da_dis_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    " #                   validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/246 [==========================>...] - ETA: 0s[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Now what is ground truth for training data\n",
      "[1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(train_exprs))\n",
    "# So the dense 2 activation just predicts all one class for the test data\n",
    "#print(model.predict(test_exprs))\n",
    "\n",
    "# It predicts all the same class for both. How can that be?\n",
    "print(\"Now what is ground truth for training data\")\n",
    "print(encoded_Y_train)\n",
    "\n",
    "# I still don't understand what the model is outputing . It doesn't seem that the predictions that I get from model.predict match the labels that I gave, it is not in a strict 0, 1 prediction\n",
    "# Something is definitely messed up. I don't know what it is. But if randomforest can get 70% accuracy, then I've got to be able to get something.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is definitely  predicting all 0's for the output, so I definitely still have some trouble with how I am giving the data because it says 100% accuracy when it is not in fact 100% accuracy. I need to figure this out. Basically everything I learned today was incorrect because there is a bug in mapping my outputs to inputs. Maybe If i just put the numbers in a list comprehension it will work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "246/246 [==============================] - 1s - loss: 0.2698 - acc: 0.9024\n",
      "Epoch 2/10\n",
      "246/246 [==============================] - 1s - loss: 0.1895 - acc: 0.9512\n",
      "Epoch 3/10\n",
      "246/246 [==============================] - 1s - loss: 0.1540 - acc: 0.9715\n",
      "Epoch 4/10\n",
      "246/246 [==============================] - 1s - loss: 0.1316 - acc: 0.9878\n",
      "Epoch 5/10\n",
      "246/246 [==============================] - 1s - loss: 0.1164 - acc: 0.9959\n",
      "Epoch 6/10\n",
      "246/246 [==============================] - 1s - loss: 0.1053 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "246/246 [==============================] - 1s - loss: 0.0977 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "246/246 [==============================] - 2s - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "246/246 [==============================] - 1s - loss: 0.0855 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "246/246 [==============================] - 1s - loss: 0.0804 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4173b1e828>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_exprs, train_Y, batch_size=train_exprs.shape[0], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what! The following model architecture worked wonderfully to fit. I can fit the training data perfectly. Now we will see if I can fit test data. I can do a first step with a validation split, maybe 80-20, just to see\n",
    "\n",
    "Also, is my batch normalization helping on the training? Before in R I remember training taking forever. Okay, batch normalization in the middle layesr does speed up training a bit, but it still reliably trains. Okay, when I don't do the batch normalization on the INITIAL layer, then my model doesn't go anywhere from the beginning. Then I have to fiddle with the learning rate. Starting off at 1e-6 then going to 0.001 and then to 0.00001 (when 0.001 didn't really budge) went okay. Thus, the initial batchnormalization (i.e. normalization) was HUGELY critical in getting the model to fit easily, and the batch-normalizations in the middle sped up training.\n",
    "\n",
    "ALSO, REMEMBER! IN CROSS VALIDATION, I IDEALLY NEED TO SEPARATE ACCORDING TO MONKEY, NOT JUST RANDOMLY, SO RANDOM IS NOT GOING TO WORK. But we can try anyway\n",
    "\n",
    "Okay, with 80-20 validation split (among samples, not monkeys), I get 60% accuracy on validation, even as the training data is totally fit. Therefore, huge overfitting. Let's add dropout to see what happens.\n",
    "0.8 Dropout totally killed my ability to train. \n",
    "0.5 dropout gets to 91% accuracy in 30 epochs with 80% of the training set, but over no epoch is validation accuracy changed.\n",
    "\n",
    "Now, using my test data as my validation data, just to start out:\n",
    "0.5 dropout, in 30 epochs I get 91.55 accuracy in full training set, 50% accuracy in test set at every epoch. It is totally training on noise. How about if I lower the complexity of the model\n",
    "\n",
    "One hidden layer with 5000 hidden units gets 98.78% accuracy on training set in 30 epochs, no budge on test (50% accuracy). I wonder if the data is somehow in wrong or randomized. I get same result with just 10 hidden units. I am going to see if random forus works, as I know it works in R.\n",
    "Great fits well at first model:\n",
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=train_exprs.shape[1:]), # this line needs work\n",
    "    Dense(5000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(500, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check: Try RandomForest with R default parameters (expect 70% test accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs so fast! A lot faster than in R on my computer. The RandomForests classifier trained on the full training set and used to predict on the full test set obtains 72.9% accuracy. Therefore, my data is intact. I don't know why "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.10455459e-04   3.21039856e-05   5.36269603e-05 ...,   3.87808318e-04\n",
      "   2.47868704e-04   3.68127795e-04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, oob_score=True, bootstrap=True, max_features=\"sqrt\")\n",
    "#clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(train_exprs, encoded_Y_train)\n",
    "#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "#            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#            min_samples_leaf=1, min_samples_split=2,\n",
    "#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "#            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "print(clf.feature_importances_)\n",
    "#print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  7]\n",
      " [ 6 18]]\n",
      "0.729166666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "test_pred = clf.predict(test_exprs)\n",
    "print(confusion_matrix(encoded_Y_test, test_pred)) \n",
    "print(accuracy_score(encoded_Y_test, test_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the incorrect loss display of keras with the monkey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras shows increasing accuracy on the training set when it predicts all of one class at the end of training on the training set.\n",
    "\n",
    "### To debug this I am just going to try to do standard keras with the IRIS dataset, another structured dataset\n",
    "### I am using code from Jason Brownlee found at:\n",
    "\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
    "#dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first I will do his cross-validation code just to reproduce what he did. Then I will do it without cross-validation. Though cross-validation may be the way to go to really show whether my model is working correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.8666666746139526, total=   3.2s\n",
      "Baseline: 96.67% (4.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   32.3s finished\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold, verbose=3)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There appears to be nothing wrong with Keras and Sci-kit learn, as I was able to run this prediction correctly. The next step is to break the IRIS dataset up into a training set and a small test set, like I have done, then use the same training and validation code, then predict on training and predict on test.\n",
    "\n",
    "### If this works, then I need to copy this code line by line to my code and retry it, if that doesn't work, then I should go ahead and go straight to 10-fold cross-validation on my training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from my state farm distracted driver code\n",
    "import random\n",
    "random.seed(100)   # So subjects selected are consistent\n",
    "b =set(np.random.permutation(a['subject']))\n",
    "subs_val = random.sample(b - set('p072'), 3)# Decided on 3 drivers with further consultation from Jeremy Howard's notebook\n",
    "print(\"Validation subjects: \" + ', '.join(subs_val))\n",
    "\n",
    "a['val.file'] = a[['classname', 'img']].apply(lambda x: '/'.join(x), axis=1)\n",
    "tab_val = a.loc[a['subject'].isin(subs_val)]\n",
    "val_files =tab_val['val.file'].tolist()\n",
    "val_files[0:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
