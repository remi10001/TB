{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Time Since TB Infection in Macaques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to apply deep learning algorithms to analyzing the monkey data. I need to:\n",
    "- Transfer over files for middle and late infection, just the microarray data in one file, and the clinical data in another file, only for those monkeys\n",
    "- Set up a training and test set.\n",
    "    - I want 3 latent and 3 active in test set\n",
    "- Before I set up a 10-fold cross-validation scheme, I think it is okay to just see if I can get a model to train on the training set. I definitely want to train a model just on the training set as opposed to the whole dataset together, to start off with at least some good practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Progress/Questions:\n",
    " - I learned that batch normalization was causing my training bugs in the MLP model. The IRIS dataset helped me determine this. One task is to learn why batch normalization was causing these problems in keras. I think I will postpone this for now\n",
    " - Just start applying MLP models to the monkey data, to see if it can be trained.\n",
    " - Set up systematic 10-fold cross-validation experiments on the monkey data. Stratify folds by TB status. Search over hyperparameters, perhaps with python script as opposed to notebook\n",
    " - Go through Jeremy Howard's code on his structured data lecture to learn how he used keras, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "#from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = \"/master/rault/TB\"\n",
    "data_path = path + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/master/rault/TB/data\n",
      "Monkey_PhenoData_middle-late.txt\r\n",
      "Monkey_Processed_ExpressionData_middle-late.txt\r\n"
     ]
    }
   ],
   "source": [
    "%cd $data_path\n",
    "%ls \n",
    "\n",
    "pheno = pd.read_table(\"Monkey_PhenoData_middle-late.txt\")\n",
    "expres = pd.read_table(\"Monkey_Processed_ExpressionData_middle-late.txt\")\n",
    "#Monkey_PhenoData_middle-late.txt\n",
    "#Monkey_Processed_ExpressionData_middle-late.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to be consistent\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "# select the latent monkeys\n",
    "latent_monkeys = pheno.loc[pheno[\"clinical.status\"] == \"Latent\"][\"monkeyid\"].tolist()\n",
    "\n",
    "# select the active monkeys\n",
    "active_monkeys = pheno.loc[pheno[\"clinical.status\"] == \"Active\"][\"monkeyid\"].tolist()\n",
    "\n",
    "# set(latent_monkeys) & set(active_monkeys) #-> They are correctly disjoint\n",
    "\n",
    "# Randomly select 3 latent monkeys\n",
    "test_latent_monkeys = random.sample(latent_monkeys, 3)\n",
    "\n",
    "# randomly select 3 active monkeys\n",
    "test_active_monkeys = random.sample(active_monkeys, 3)\n",
    "\n",
    "test_monkeys = test_latent_monkeys + test_active_monkeys\n",
    "\n",
    "# remove these monkeys from the training set  and put in a test set (both the clinical variables and the expression)\n",
    "train_pheno = pheno.loc[pheno[\"monkeyid\"].isin(set(pheno[\"monkeyid\"]) - set(test_monkeys))]\n",
    "test_pheno = pheno.loc[pheno[\"monkeyid\"].isin(test_monkeys)]\n",
    "\n",
    "#set(train_set[\"monkeyid\"]) & set(test_set[\"monkeyid\"]) #-> They are correctly disjoint\n",
    "\n",
    "train_exprs = expres[expres.index.isin(list(train_pheno.index))]\n",
    "test_exprs = expres[expres.index.isin(list(test_pheno.index))]\n",
    "\n",
    "train_exprs = train_exprs.astype(float)\n",
    "test_exprs = test_exprs.astype(float)\n",
    "\n",
    "train_exprs = train_exprs.as_matrix()\n",
    "test_exprs = test_exprs.as_matrix()\n",
    "#DataFrame.as_matrix\n",
    "#X = dataset[:,0:4].astype(float)\n",
    "# set(test_exprs.index) & set(train_exprs.index) #-> They are correctly disjoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-057bfc9e6b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training_set.index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GSM2227796'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# This somehow works! so can subset by the rows in this way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "#training_set.index\n",
    "train_set[train_set.index.isin(['GSM2227796'])]  # This somehow works! so can subset by the rows in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.39462752,  4.48762888,  5.96935506, ...,  3.87268578,\n",
       "         4.32225507,  6.96685288],\n",
       "       [ 4.12352933,  4.15348654,  5.53672825, ...,  5.24086738,\n",
       "         5.91572326,  6.71413883],\n",
       "       [ 3.39462752,  3.42627115,  5.63217959, ...,  3.87268578,\n",
       "         3.54491484,  7.23525856],\n",
       "       ..., \n",
       "       [ 3.25470823,  3.90686403,  5.60626114, ...,  3.98340472,\n",
       "         6.18424082,  6.8451059 ],\n",
       "       [ 3.59531892,  4.6930189 ,  5.71572579, ...,  5.22307736,\n",
       "         5.31778927,  6.75539068],\n",
       "       [ 3.25470823,  4.58647098,  6.24496345, ...,  4.99904663,\n",
       "         6.18319178,  6.72783263]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_exprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for loading into keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This website from Jason Brownlee has excellent tutorial on using pandas to load in data and then use keras. I can use his code to help me\n",
    "\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_pheno[\"time.period\"])\n",
    "encoded_Y_train = encoder.transform(train_pheno[\"time.period\"])\n",
    "encoded_Y_test = encoder.transform(test_pheno[\"time.period\"])\n",
    "\n",
    "monkey_encoder = LabelEncoder()\n",
    "monkey_encoder.fit(pheno[\"monkeyid\"])\n",
    "enc_monkey_train = monkey_encoder.transform(train_pheno[\"monkeyid\"])\n",
    "enc_monkey_test = monkey_encoder.transform(test_pheno[\"monkeyid\"])\n",
    "\n",
    "# One-hot encoding\n",
    "train_Y = np_utils.to_categorical(encoded_Y_train)\n",
    "test_Y = np_utils.to_categorical(encoded_Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0]\n",
      "GSM2227796    middle\n",
      "GSM2227797      late\n",
      "GSM2227799    middle\n",
      "GSM2227800      late\n",
      "GSM2227801    middle\n",
      "GSM2227805      late\n",
      "GSM2227806      late\n",
      "GSM2227807    middle\n",
      "GSM2227808      late\n",
      "GSM2227809    middle\n",
      "GSM2227810      late\n",
      "GSM2227812    middle\n",
      "GSM2227814      late\n",
      "GSM2227815    middle\n",
      "GSM2227816      late\n",
      "GSM2227818    middle\n",
      "GSM2227820      late\n",
      "GSM2227823      late\n",
      "GSM2227825    middle\n",
      "GSM2227827      late\n",
      "GSM2227828    middle\n",
      "GSM2227832    middle\n",
      "GSM2227834    middle\n",
      "GSM2227835    middle\n",
      "GSM2227836    middle\n",
      "GSM2227837    middle\n",
      "GSM2227839      late\n",
      "GSM2227842    middle\n",
      "GSM2227843      late\n",
      "GSM2227844    middle\n",
      "               ...  \n",
      "GSM2228190      late\n",
      "GSM2228192      late\n",
      "GSM2228193      late\n",
      "GSM2228197    middle\n",
      "GSM2228201    middle\n",
      "GSM2228203      late\n",
      "GSM2228204      late\n",
      "GSM2228208      late\n",
      "GSM2228211    middle\n",
      "GSM2228212      late\n",
      "GSM2228215    middle\n",
      "GSM2228218    middle\n",
      "GSM2228224    middle\n",
      "GSM2228225    middle\n",
      "GSM2228228    middle\n",
      "GSM2228229      late\n",
      "GSM2228232    middle\n",
      "GSM2228237      late\n",
      "GSM2228238    middle\n",
      "GSM2228239      late\n",
      "GSM2228240      late\n",
      "GSM2228243    middle\n",
      "GSM2228244    middle\n",
      "GSM2228245    middle\n",
      "GSM2228246    middle\n",
      "GSM2228247    middle\n",
      "GSM2228251    middle\n",
      "GSM2228253    middle\n",
      "GSM2228258    middle\n",
      "GSM2228261      late\n",
      "Name: time.period, Length: 246, dtype: object\n",
      "[0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 0 1 0 0 0 0 1 1 0 0 1]\n",
      "GSM2227793      late\n",
      "GSM2227802      late\n",
      "GSM2227813      late\n",
      "GSM2227833      late\n",
      "GSM2227841    middle\n",
      "GSM2227859    middle\n",
      "GSM2227864      late\n",
      "GSM2227871      late\n",
      "GSM2227886      late\n",
      "GSM2227895    middle\n",
      "GSM2227913      late\n",
      "GSM2227943    middle\n",
      "GSM2227945    middle\n",
      "GSM2227963    middle\n",
      "GSM2228019    middle\n",
      "GSM2228023    middle\n",
      "GSM2228036      late\n",
      "GSM2228037    middle\n",
      "GSM2228056    middle\n",
      "GSM2228058    middle\n",
      "GSM2228059      late\n",
      "GSM2228075    middle\n",
      "GSM2228077      late\n",
      "GSM2228085    middle\n",
      "GSM2228097    middle\n",
      "GSM2228105      late\n",
      "GSM2228108    middle\n",
      "GSM2228112      late\n",
      "GSM2228115    middle\n",
      "GSM2228136      late\n",
      "GSM2228139      late\n",
      "GSM2228144    middle\n",
      "GSM2228145      late\n",
      "GSM2228147    middle\n",
      "GSM2228167      late\n",
      "GSM2228170    middle\n",
      "GSM2228176    middle\n",
      "GSM2228196      late\n",
      "GSM2228199    middle\n",
      "GSM2228202      late\n",
      "GSM2228223      late\n",
      "GSM2228230      late\n",
      "GSM2228231      late\n",
      "GSM2228242    middle\n",
      "GSM2228250    middle\n",
      "GSM2228254      late\n",
      "GSM2228255      late\n",
      "GSM2228257    middle\n",
      "Name: time.period, dtype: object\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "GSM2227796    M19\n",
      "GSM2227797    M19\n",
      "GSM2227799    M18\n",
      "GSM2227800    M15\n",
      "GSM2227801     M1\n",
      "GSM2227805     M6\n",
      "GSM2227806    M14\n",
      "GSM2227807    M15\n",
      "GSM2227808    M17\n",
      "GSM2227809     M2\n",
      "GSM2227810     M9\n",
      "GSM2227812     M9\n",
      "GSM2227814     M3\n",
      "GSM2227815     M3\n",
      "GSM2227816    M19\n",
      "GSM2227818     M1\n",
      "GSM2227820    M18\n",
      "GSM2227823     M8\n",
      "GSM2227825     M4\n",
      "GSM2227827     M9\n",
      "GSM2227828     M1\n",
      "GSM2227832     M4\n",
      "GSM2227834    M18\n",
      "GSM2227835     M6\n",
      "GSM2227836     M2\n",
      "GSM2227837    M14\n",
      "GSM2227839     M5\n",
      "GSM2227842    M19\n",
      "GSM2227843    M12\n",
      "GSM2227844     M7\n",
      "             ... \n",
      "GSM2228190    M31\n",
      "GSM2228192    M36\n",
      "GSM2228193    M31\n",
      "GSM2228197    M31\n",
      "GSM2228201    M37\n",
      "GSM2228203    M26\n",
      "GSM2228204    M25\n",
      "GSM2228208    M37\n",
      "GSM2228211    M20\n",
      "GSM2228212    M37\n",
      "GSM2228215    M22\n",
      "GSM2228218    M22\n",
      "GSM2228224    M33\n",
      "GSM2228225    M26\n",
      "GSM2228228    M20\n",
      "GSM2228229    M33\n",
      "GSM2228232    M38\n",
      "GSM2228237    M35\n",
      "GSM2228238    M26\n",
      "GSM2228239    M22\n",
      "GSM2228240    M32\n",
      "GSM2228243    M33\n",
      "GSM2228244    M28\n",
      "GSM2228245    M35\n",
      "GSM2228246    M20\n",
      "GSM2228247    M36\n",
      "GSM2228251    M22\n",
      "GSM2228253    M25\n",
      "GSM2228258    M21\n",
      "GSM2228261    M21\n",
      "Name: monkeyid, Length: 246, dtype: object\n",
      "[10 10  9  6  0 34  5  6  8 11 37 37 22 22 10  0  9 36 32 37  0 32  9 34 11\n",
      "  5 33 10  3 35  0 33 11 22  3 35 10 22  9  8 32  6  9 37 34 36  5 37 22  1\n",
      " 33  2 35 34 11 36  6 35 34  2  5  3  0  3  6 33 35 33 34  1 37  1 37  6 22\n",
      " 35  8  0 33  3  5 36 10 32 10  6  9 11 32 22 37 36  8  9 32  6  0  2 32 36\n",
      " 11  2 35  5  1  2 33  0 35 11  2  1  3  3  8  5  3 36 36 32 34 22 33  8  2\n",
      "  8  5  2  8 34 11 25 12 15 21 20 21 12 20 26 12 14 29 15 21 18 21 31 13 17\n",
      " 14 21 24 18 28 29 20 30 26 12 29 17 15 13 18 13 14 13 25 18 30 31 17 15 24\n",
      " 13 14 26 31 28 12 20 20 28 24 21 21 28 30 13 29 20 17 25 17 25 26 15 25 29\n",
      " 31 21 17 31 30 28 31 18 30 26 28 20 29 24 25 25 24 29 24 24 30 18 17 30 12\n",
      " 30 14 14 26 18 12 26 31 28 18 14 25 26 20 28 12 29 14 17 13 13]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_Y_train)\n",
    "print(train_pheno[\"time.period\"])\n",
    "print(encoded_Y_test)\n",
    "print(test_pheno[\"time.period\"])\n",
    "print(train_Y)\n",
    "print(train_pheno[\"monkeyid\"])\n",
    "print(enc_monkey_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up 10-fold cross-validation, stratifying by monkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorials to help me in grid-searching\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "    https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to look up how to do a stratified k-fold according to both class (middle vs. late infection) but also monkey. I think I will worry about this before I do grid search. I have to have a valid division of the data before being able to evaluate the performance of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedKFold can be used to make divisions with balanced classes. However, I need to make sure that monkey is completely balanced across classes. I may have to do my own custom k-fold division.\n",
    "\n",
    "Ahh, what I need is to make K-fold cross-validation of monkeys, then balance active, latent TB. in each fold include all samples for k-fold.\n",
    "\n",
    "This website may be able to tell me:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "GroupKFold is the general class of techniques I need. Though I need also the stratification of status, if possible.\n",
    "\n",
    "It looks like sck-kit learn doesn't have built in cross-validation code to combine GroupKFold with stratification based on a factor. With GroupKFold I will automatically ensure that middle and late infection are stratified appropriately across k-folds (since all monkeys have middle and late infection time points). Active and Latent TB will be something I hope fall out on their own. Post-hoc error analysi can evaluation the effect of TB status anyway. ALSO, perhaps doing too much stratification will effectively make my cross-validation loop not valid. I will go for GroupKFold for now. It will certainly NOT understimate my generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 9050)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold) # , verbose=3)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(p=0.0):\n",
    "\n",
    "#p = 0.0\n",
    "\n",
    "    model = Sequential([\n",
    "        #BatchNormalization(input_shape=train_exprs.shape[1:]),\n",
    "        Dense(5000, activation=\"relu\", input_shape=train_exprs.shape[1:]),\n",
    "        #BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        #BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(50, activation=\"relu\"),\n",
    "        #BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation=\"relu\"),\n",
    "        #BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(Adam(lr=0.00004), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first, doing this with random seed=100, the model flat-lined at 0.6931 loss.\n",
    "### When i next ran the same model on the whole training set, the model fit as the first time I did it\n",
    "\n",
    "Now trying with random seed 50, the model flat-lines again, I think I just need to let it go, see how it works.\n",
    "\n",
    "So on the first training run it flat-lined, but on all subsequent ones the model is able to train. Maybe I need to randomize the seed to before I start training.\n",
    "\n",
    "This takes a long time to train for 200 epochs, though at least for now 200 epochs are needed. I need to do the orignial train test validation to see when the test error starts to creep up. I need to do something to decrease the training time if I am going to efficiently find a better model! Maybe something like learning rate annealing will work. I need to find an automatic learning rate finder in keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "222/222 [==============================] - 1s - loss: 4.4627 - acc: 0.5180\n",
      "Epoch 2/200\n",
      "222/222 [==============================] - 1s - loss: 4.7699 - acc: 0.4820\n",
      "Epoch 3/200\n",
      "222/222 [==============================] - 1s - loss: 1.4014 - acc: 0.4820\n",
      "Epoch 4/200\n",
      "222/222 [==============================] - 1s - loss: 1.3639 - acc: 0.5180\n",
      "Epoch 5/200\n",
      "222/222 [==============================] - 1s - loss: 1.3781 - acc: 0.5180\n",
      "Epoch 6/200\n",
      "222/222 [==============================] - 1s - loss: 1.2771 - acc: 0.5180\n",
      "Epoch 7/200\n",
      "222/222 [==============================] - 1s - loss: 1.1416 - acc: 0.5180\n",
      "Epoch 8/200\n",
      "222/222 [==============================] - 1s - loss: 0.9873 - acc: 0.5180\n",
      "Epoch 9/200\n",
      "222/222 [==============================] - 1s - loss: 0.8489 - acc: 0.5180\n",
      "Epoch 10/200\n",
      "222/222 [==============================] - 1s - loss: 0.7529 - acc: 0.5180\n",
      "Epoch 11/200\n",
      "222/222 [==============================] - 1s - loss: 0.7057 - acc: 0.5180\n",
      "Epoch 12/200\n",
      "222/222 [==============================] - 1s - loss: 0.6918 - acc: 0.5315\n",
      "Epoch 13/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 14/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 15/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 16/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 17/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 18/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 19/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 20/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 21/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 22/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 23/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 24/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 25/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 26/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 27/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 28/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 29/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 30/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 31/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 32/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 33/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 34/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 35/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 36/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 37/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 38/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 39/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 40/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 41/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 42/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 43/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 44/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 45/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 46/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 47/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 48/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 49/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 50/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 51/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 52/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 53/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 54/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 55/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 56/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 57/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 58/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 59/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 60/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 61/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 62/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 63/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 64/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 65/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 66/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 67/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 68/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 69/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 70/200\n",
      "222/222 [==============================] - 1s - loss: 0.6932 - acc: 0.4820\n",
      "Epoch 71/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 72/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 73/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 74/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 75/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 76/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 77/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 78/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 79/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 80/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 81/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 82/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 83/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 84/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 85/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 86/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 87/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 88/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 89/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 90/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 91/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 92/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 93/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 94/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 95/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 96/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 97/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 98/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 99/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 100/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.4820\n",
      "Epoch 101/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 102/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 103/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 104/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 105/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 106/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 107/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 108/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 109/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 110/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 111/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 112/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 113/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 114/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 115/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 116/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 117/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 118/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 119/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 120/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 121/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 122/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 123/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 124/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 125/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 126/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 127/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 128/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 129/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 130/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 131/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 132/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 133/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 134/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 135/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 136/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 137/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 138/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 139/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 140/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 141/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 142/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 143/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 144/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 145/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 146/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 147/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 148/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 149/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 150/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 151/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 152/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 153/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 154/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 155/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 156/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 157/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 158/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 159/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 160/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 161/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 162/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 163/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 164/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 165/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 166/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 167/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 168/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 169/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 170/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 171/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 172/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 173/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 174/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 175/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 176/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 177/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 178/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 179/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 180/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 181/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 182/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 183/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 184/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 185/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 187/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 188/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 189/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 190/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 191/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 192/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 193/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 194/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 195/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 196/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 197/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 198/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 199/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "Epoch 200/200\n",
      "222/222 [==============================] - 1s - loss: 0.6931 - acc: 0.5180\n",
      "24/24 [==============================] - 0s\n",
      "Epoch 1/200\n",
      "222/222 [==============================] - 1s - loss: 1.8867 - acc: 0.5180\n",
      "Epoch 2/200\n",
      "222/222 [==============================] - 1s - loss: 0.7036 - acc: 0.5360\n",
      "Epoch 3/200\n",
      "222/222 [==============================] - 1s - loss: 0.8589 - acc: 0.4820\n",
      "Epoch 4/200\n",
      "222/222 [==============================] - 1s - loss: 1.8216 - acc: 0.5180\n",
      "Epoch 5/200\n",
      "222/222 [==============================] - 1s - loss: 0.7609 - acc: 0.5180\n",
      "Epoch 6/200\n",
      "222/222 [==============================] - 1s - loss: 1.6911 - acc: 0.4820\n",
      "Epoch 7/200\n",
      "222/222 [==============================] - 1s - loss: 1.2428 - acc: 0.4820\n",
      "Epoch 8/200\n",
      "222/222 [==============================] - 1s - loss: 0.8622 - acc: 0.5180\n",
      "Epoch 9/200\n",
      "222/222 [==============================] - 1s - loss: 1.2261 - acc: 0.5180\n",
      "Epoch 10/200\n",
      "222/222 [==============================] - 1s - loss: 0.7279 - acc: 0.5180\n",
      "Epoch 11/200\n",
      "222/222 [==============================] - 1s - loss: 0.9963 - acc: 0.4820\n",
      "Epoch 12/200\n",
      "222/222 [==============================] - 1s - loss: 1.0091 - acc: 0.4820\n",
      "Epoch 13/200\n",
      "222/222 [==============================] - 1s - loss: 0.6585 - acc: 0.5991\n",
      "Epoch 14/200\n",
      "222/222 [==============================] - 1s - loss: 0.9419 - acc: 0.5180\n",
      "Epoch 15/200\n",
      "222/222 [==============================] - 1s - loss: 0.8617 - acc: 0.5180\n",
      "Epoch 16/200\n",
      "222/222 [==============================] - 1s - loss: 0.6605 - acc: 0.6577\n",
      "Epoch 17/200\n",
      "222/222 [==============================] - 1s - loss: 0.8766 - acc: 0.4820\n",
      "Epoch 18/200\n",
      "222/222 [==============================] - 1s - loss: 0.7605 - acc: 0.4910\n",
      "Epoch 19/200\n",
      "222/222 [==============================] - 1s - loss: 0.6761 - acc: 0.5180\n",
      "Epoch 20/200\n",
      "222/222 [==============================] - 1s - loss: 0.8235 - acc: 0.5180\n",
      "Epoch 21/200\n",
      "222/222 [==============================] - 1s - loss: 0.6914 - acc: 0.5180\n",
      "Epoch 22/200\n",
      "222/222 [==============================] - 1s - loss: 0.6919 - acc: 0.5450\n",
      "Epoch 23/200\n",
      "222/222 [==============================] - 1s - loss: 0.7691 - acc: 0.4910\n",
      "Epoch 24/200\n",
      "222/222 [==============================] - 1s - loss: 0.6483 - acc: 0.6622\n",
      "Epoch 25/200\n",
      "222/222 [==============================] - 1s - loss: 0.7071 - acc: 0.5180\n",
      "Epoch 26/200\n",
      "222/222 [==============================] - 1s - loss: 0.7162 - acc: 0.5180\n",
      "Epoch 27/200\n",
      "222/222 [==============================] - 1s - loss: 0.6360 - acc: 0.6757\n",
      "Epoch 28/200\n",
      "222/222 [==============================] - 1s - loss: 0.7064 - acc: 0.5225\n",
      "Epoch 29/200\n",
      "222/222 [==============================] - 1s - loss: 0.6666 - acc: 0.5811\n",
      "Epoch 30/200\n",
      "222/222 [==============================] - 1s - loss: 0.6447 - acc: 0.5721\n",
      "Epoch 31/200\n",
      "222/222 [==============================] - 1s - loss: 0.6898 - acc: 0.5180\n",
      "Epoch 32/200\n",
      "222/222 [==============================] - 1s - loss: 0.6337 - acc: 0.6126\n",
      "Epoch 33/200\n",
      "222/222 [==============================] - 1s - loss: 0.6580 - acc: 0.6036\n",
      "Epoch 34/200\n",
      "222/222 [==============================] - 1s - loss: 0.6586 - acc: 0.6036\n",
      "Epoch 35/200\n",
      "222/222 [==============================] - 1s - loss: 0.6259 - acc: 0.6171\n",
      "Epoch 36/200\n",
      "222/222 [==============================] - 1s - loss: 0.6593 - acc: 0.5360\n",
      "Epoch 37/200\n",
      "222/222 [==============================] - 1s - loss: 0.6286 - acc: 0.6126\n",
      "Epoch 38/200\n",
      "222/222 [==============================] - 1s - loss: 0.6354 - acc: 0.6532\n",
      "Epoch 39/200\n",
      "222/222 [==============================] - 1s - loss: 0.6413 - acc: 0.6351\n",
      "Epoch 40/200\n",
      "222/222 [==============================] - 1s - loss: 0.6183 - acc: 0.6486\n",
      "Epoch 41/200\n",
      "222/222 [==============================] - 1s - loss: 0.6389 - acc: 0.5766\n",
      "Epoch 42/200\n",
      "222/222 [==============================] - 1s - loss: 0.6193 - acc: 0.6261\n",
      "Epoch 43/200\n",
      "222/222 [==============================] - 1s - loss: 0.6247 - acc: 0.6847\n",
      "Epoch 44/200\n",
      "222/222 [==============================] - 1s - loss: 0.6253 - acc: 0.6802\n",
      "Epoch 45/200\n",
      "222/222 [==============================] - 1s - loss: 0.6132 - acc: 0.6351\n",
      "Epoch 46/200\n",
      "222/222 [==============================] - 1s - loss: 0.6248 - acc: 0.6081\n",
      "Epoch 47/200\n",
      "222/222 [==============================] - 1s - loss: 0.6103 - acc: 0.6622\n",
      "Epoch 48/200\n",
      "222/222 [==============================] - 1s - loss: 0.6176 - acc: 0.6847\n",
      "Epoch 49/200\n",
      "222/222 [==============================] - 1s - loss: 0.6120 - acc: 0.6937\n",
      "Epoch 50/200\n",
      "222/222 [==============================] - 1s - loss: 0.6095 - acc: 0.6351\n",
      "Epoch 51/200\n",
      "222/222 [==============================] - 1s - loss: 0.6128 - acc: 0.6261\n",
      "Epoch 52/200\n",
      "222/222 [==============================] - 1s - loss: 0.6044 - acc: 0.7342\n",
      "Epoch 53/200\n",
      "222/222 [==============================] - 1s - loss: 0.6104 - acc: 0.6937\n",
      "Epoch 54/200\n",
      "222/222 [==============================] - 1s - loss: 0.6027 - acc: 0.7207\n",
      "Epoch 55/200\n",
      "222/222 [==============================] - 1s - loss: 0.6060 - acc: 0.6306\n",
      "Epoch 56/200\n",
      "222/222 [==============================] - 1s - loss: 0.6022 - acc: 0.6577\n",
      "Epoch 57/200\n",
      "222/222 [==============================] - 1s - loss: 0.6014 - acc: 0.7207\n",
      "Epoch 58/200\n",
      "222/222 [==============================] - 1s - loss: 0.6013 - acc: 0.7117\n",
      "Epoch 59/200\n",
      "222/222 [==============================] - 1s - loss: 0.5978 - acc: 0.6847\n",
      "Epoch 60/200\n",
      "222/222 [==============================] - 1s - loss: 0.5996 - acc: 0.6441\n",
      "Epoch 61/200\n",
      "222/222 [==============================] - 1s - loss: 0.5952 - acc: 0.7297\n",
      "Epoch 62/200\n",
      "222/222 [==============================] - 1s - loss: 0.5971 - acc: 0.7207\n",
      "Epoch 63/200\n",
      "222/222 [==============================] - 1s - loss: 0.5934 - acc: 0.7297\n",
      "Epoch 64/200\n",
      "222/222 [==============================] - 1s - loss: 0.5944 - acc: 0.6757\n",
      "Epoch 65/200\n",
      "222/222 [==============================] - 1s - loss: 0.5918 - acc: 0.6847\n",
      "Epoch 66/200\n",
      "222/222 [==============================] - 1s - loss: 0.5917 - acc: 0.7387\n",
      "Epoch 67/200\n",
      "222/222 [==============================] - 1s - loss: 0.5902 - acc: 0.7387\n",
      "Epoch 68/200\n",
      "222/222 [==============================] - 1s - loss: 0.5892 - acc: 0.6982\n",
      "Epoch 69/200\n",
      "222/222 [==============================] - 1s - loss: 0.5885 - acc: 0.6982\n",
      "Epoch 70/200\n",
      "222/222 [==============================] - 1s - loss: 0.5869 - acc: 0.7297\n",
      "Epoch 71/200\n",
      "222/222 [==============================] - 1s - loss: 0.5866 - acc: 0.7387\n",
      "Epoch 72/200\n",
      "222/222 [==============================] - 1s - loss: 0.5848 - acc: 0.7207\n",
      "Epoch 73/200\n",
      "222/222 [==============================] - 1s - loss: 0.5847 - acc: 0.7072\n",
      "Epoch 74/200\n",
      "222/222 [==============================] - 1s - loss: 0.5828 - acc: 0.7342\n",
      "Epoch 75/200\n",
      "222/222 [==============================] - 1s - loss: 0.5827 - acc: 0.7477\n",
      "Epoch 76/200\n",
      "222/222 [==============================] - 1s - loss: 0.5809 - acc: 0.7387\n",
      "Epoch 77/200\n",
      "222/222 [==============================] - 1s - loss: 0.5807 - acc: 0.7162\n",
      "Epoch 78/200\n",
      "222/222 [==============================] - 1s - loss: 0.5791 - acc: 0.7387\n",
      "Epoch 79/200\n",
      "222/222 [==============================] - 1s - loss: 0.5787 - acc: 0.7432\n",
      "Epoch 80/200\n",
      "222/222 [==============================] - 1s - loss: 0.5772 - acc: 0.7432\n",
      "Epoch 81/200\n",
      "222/222 [==============================] - 1s - loss: 0.5767 - acc: 0.7162\n",
      "Epoch 82/200\n",
      "222/222 [==============================] - 1s - loss: 0.5754 - acc: 0.7432\n",
      "Epoch 83/200\n",
      "222/222 [==============================] - 1s - loss: 0.5748 - acc: 0.7387\n",
      "Epoch 84/200\n",
      "222/222 [==============================] - 1s - loss: 0.5735 - acc: 0.7477\n",
      "Epoch 85/200\n",
      "222/222 [==============================] - 1s - loss: 0.5729 - acc: 0.7342\n",
      "Epoch 86/200\n",
      "222/222 [==============================] - 1s - loss: 0.5717 - acc: 0.7432\n",
      "Epoch 87/200\n",
      "222/222 [==============================] - 1s - loss: 0.5710 - acc: 0.7477\n",
      "Epoch 88/200\n",
      "222/222 [==============================] - 1s - loss: 0.5699 - acc: 0.7477\n",
      "Epoch 89/200\n",
      "222/222 [==============================] - 1s - loss: 0.5691 - acc: 0.7387\n",
      "Epoch 90/200\n",
      "222/222 [==============================] - 1s - loss: 0.5680 - acc: 0.7477\n",
      "Epoch 91/200\n",
      "222/222 [==============================] - 1s - loss: 0.5673 - acc: 0.7523\n",
      "Epoch 92/200\n",
      "222/222 [==============================] - 1s - loss: 0.5662 - acc: 0.7477\n",
      "Epoch 93/200\n",
      "222/222 [==============================] - 1s - loss: 0.5654 - acc: 0.7477\n",
      "Epoch 94/200\n",
      "222/222 [==============================] - 1s - loss: 0.5644 - acc: 0.7477\n",
      "Epoch 95/200\n",
      "222/222 [==============================] - 1s - loss: 0.5636 - acc: 0.7477\n",
      "Epoch 96/200\n",
      "222/222 [==============================] - 1s - loss: 0.5625 - acc: 0.7477\n",
      "Epoch 97/200\n",
      "222/222 [==============================] - 1s - loss: 0.5617 - acc: 0.7477\n",
      "Epoch 98/200\n",
      "222/222 [==============================] - 1s - loss: 0.5607 - acc: 0.7477\n",
      "Epoch 99/200\n",
      "222/222 [==============================] - 1s - loss: 0.5598 - acc: 0.7477\n",
      "Epoch 100/200\n",
      "222/222 [==============================] - 1s - loss: 0.5589 - acc: 0.7477\n",
      "Epoch 101/200\n",
      "222/222 [==============================] - 1s - loss: 0.5580 - acc: 0.7523\n",
      "Epoch 102/200\n",
      "222/222 [==============================] - 1s - loss: 0.5571 - acc: 0.7477\n",
      "Epoch 103/200\n",
      "222/222 [==============================] - 1s - loss: 0.5561 - acc: 0.7477\n",
      "Epoch 104/200\n",
      "222/222 [==============================] - 1s - loss: 0.5552 - acc: 0.7523\n",
      "Epoch 105/200\n",
      "222/222 [==============================] - 1s - loss: 0.5543 - acc: 0.7523\n",
      "Epoch 106/200\n",
      "222/222 [==============================] - 1s - loss: 0.5534 - acc: 0.7477\n",
      "Epoch 107/200\n",
      "222/222 [==============================] - 1s - loss: 0.5525 - acc: 0.7568\n",
      "Epoch 108/200\n",
      "222/222 [==============================] - 1s - loss: 0.5516 - acc: 0.7523\n",
      "Epoch 109/200\n",
      "222/222 [==============================] - 1s - loss: 0.5506 - acc: 0.7613\n",
      "Epoch 110/200\n",
      "222/222 [==============================] - 1s - loss: 0.5497 - acc: 0.7613\n",
      "Epoch 111/200\n",
      "222/222 [==============================] - 1s - loss: 0.5488 - acc: 0.7613\n",
      "Epoch 112/200\n",
      "222/222 [==============================] - 1s - loss: 0.5479 - acc: 0.7523\n",
      "Epoch 113/200\n",
      "222/222 [==============================] - 1s - loss: 0.5469 - acc: 0.7613\n",
      "Epoch 114/200\n",
      "222/222 [==============================] - 1s - loss: 0.5460 - acc: 0.7658\n",
      "Epoch 115/200\n",
      "222/222 [==============================] - 1s - loss: 0.5451 - acc: 0.7613\n",
      "Epoch 116/200\n",
      "222/222 [==============================] - 1s - loss: 0.5442 - acc: 0.7613\n",
      "Epoch 117/200\n",
      "222/222 [==============================] - 1s - loss: 0.5433 - acc: 0.7658\n",
      "Epoch 118/200\n",
      "222/222 [==============================] - 1s - loss: 0.5423 - acc: 0.7703\n",
      "Epoch 119/200\n",
      "222/222 [==============================] - 1s - loss: 0.5414 - acc: 0.7613\n",
      "Epoch 120/200\n",
      "222/222 [==============================] - 1s - loss: 0.5405 - acc: 0.7703\n",
      "Epoch 121/200\n",
      "222/222 [==============================] - 1s - loss: 0.5395 - acc: 0.7658\n",
      "Epoch 122/200\n",
      "222/222 [==============================] - 1s - loss: 0.5386 - acc: 0.7658\n",
      "Epoch 123/200\n",
      "222/222 [==============================] - 1s - loss: 0.5377 - acc: 0.7703\n",
      "Epoch 124/200\n",
      "222/222 [==============================] - 1s - loss: 0.5368 - acc: 0.7658\n",
      "Epoch 125/200\n",
      "222/222 [==============================] - 1s - loss: 0.5358 - acc: 0.7703\n",
      "Epoch 126/200\n",
      "222/222 [==============================] - 1s - loss: 0.5349 - acc: 0.7658\n",
      "Epoch 127/200\n",
      "222/222 [==============================] - 1s - loss: 0.5339 - acc: 0.7658\n",
      "Epoch 128/200\n",
      "222/222 [==============================] - 1s - loss: 0.5330 - acc: 0.7658\n",
      "Epoch 129/200\n",
      "222/222 [==============================] - 1s - loss: 0.5321 - acc: 0.7793\n",
      "Epoch 130/200\n",
      "222/222 [==============================] - 1s - loss: 0.5311 - acc: 0.7793\n",
      "Epoch 131/200\n",
      "222/222 [==============================] - 1s - loss: 0.5302 - acc: 0.7703\n",
      "Epoch 132/200\n",
      "222/222 [==============================] - 1s - loss: 0.5292 - acc: 0.7748\n",
      "Epoch 133/200\n",
      "222/222 [==============================] - 1s - loss: 0.5283 - acc: 0.7793\n",
      "Epoch 134/200\n",
      "222/222 [==============================] - 1s - loss: 0.5273 - acc: 0.7793\n",
      "Epoch 135/200\n",
      "222/222 [==============================] - 1s - loss: 0.5264 - acc: 0.7793\n",
      "Epoch 136/200\n",
      "222/222 [==============================] - 1s - loss: 0.5255 - acc: 0.7838\n",
      "Epoch 137/200\n",
      "222/222 [==============================] - 1s - loss: 0.5245 - acc: 0.7793\n",
      "Epoch 138/200\n",
      "222/222 [==============================] - 1s - loss: 0.5236 - acc: 0.7793\n",
      "Epoch 139/200\n",
      "222/222 [==============================] - 1s - loss: 0.5226 - acc: 0.7748\n",
      "Epoch 140/200\n",
      "222/222 [==============================] - 1s - loss: 0.5217 - acc: 0.7748\n",
      "Epoch 141/200\n",
      "222/222 [==============================] - 1s - loss: 0.5207 - acc: 0.7793\n",
      "Epoch 142/200\n",
      "222/222 [==============================] - 1s - loss: 0.5198 - acc: 0.7793\n",
      "Epoch 143/200\n",
      "222/222 [==============================] - 1s - loss: 0.5188 - acc: 0.7928\n",
      "Epoch 144/200\n",
      "222/222 [==============================] - 1s - loss: 0.5179 - acc: 0.7928\n",
      "Epoch 145/200\n",
      "222/222 [==============================] - 1s - loss: 0.5169 - acc: 0.7928\n",
      "Epoch 146/200\n",
      "222/222 [==============================] - 1s - loss: 0.5160 - acc: 0.7928\n",
      "Epoch 147/200\n",
      "222/222 [==============================] - 1s - loss: 0.5150 - acc: 0.7973\n",
      "Epoch 148/200\n",
      "222/222 [==============================] - 1s - loss: 0.5141 - acc: 0.8018\n",
      "Epoch 149/200\n",
      "222/222 [==============================] - 1s - loss: 0.5131 - acc: 0.8018\n",
      "Epoch 150/200\n",
      "222/222 [==============================] - 1s - loss: 0.5122 - acc: 0.8018\n",
      "Epoch 151/200\n",
      "222/222 [==============================] - 1s - loss: 0.5112 - acc: 0.8018\n",
      "Epoch 152/200\n",
      "222/222 [==============================] - 1s - loss: 0.5102 - acc: 0.8018\n",
      "Epoch 153/200\n",
      "222/222 [==============================] - 1s - loss: 0.5093 - acc: 0.8018\n",
      "Epoch 154/200\n",
      "222/222 [==============================] - 1s - loss: 0.5083 - acc: 0.8018\n",
      "Epoch 155/200\n",
      "222/222 [==============================] - 1s - loss: 0.5073 - acc: 0.8018\n",
      "Epoch 156/200\n",
      "222/222 [==============================] - 1s - loss: 0.5064 - acc: 0.8018\n",
      "Epoch 157/200\n",
      "222/222 [==============================] - 1s - loss: 0.5054 - acc: 0.8018\n",
      "Epoch 158/200\n",
      "222/222 [==============================] - 1s - loss: 0.5044 - acc: 0.8018\n",
      "Epoch 159/200\n",
      "222/222 [==============================] - 1s - loss: 0.5034 - acc: 0.8018\n",
      "Epoch 160/200\n",
      "222/222 [==============================] - 1s - loss: 0.5025 - acc: 0.8018\n",
      "Epoch 161/200\n",
      "222/222 [==============================] - 1s - loss: 0.5015 - acc: 0.8018\n",
      "Epoch 162/200\n",
      "222/222 [==============================] - 1s - loss: 0.5005 - acc: 0.8018\n",
      "Epoch 163/200\n",
      "222/222 [==============================] - 1s - loss: 0.4995 - acc: 0.8063\n",
      "Epoch 164/200\n",
      "222/222 [==============================] - 1s - loss: 0.4985 - acc: 0.8063\n",
      "Epoch 165/200\n",
      "222/222 [==============================] - 1s - loss: 0.4976 - acc: 0.8063\n",
      "Epoch 166/200\n",
      "222/222 [==============================] - 1s - loss: 0.4966 - acc: 0.8063\n",
      "Epoch 167/200\n",
      "222/222 [==============================] - 1s - loss: 0.4956 - acc: 0.8063\n",
      "Epoch 168/200\n",
      "222/222 [==============================] - 1s - loss: 0.4946 - acc: 0.8063\n",
      "Epoch 169/200\n",
      "222/222 [==============================] - 1s - loss: 0.4936 - acc: 0.8063\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 1s - loss: 0.4926 - acc: 0.8063\n",
      "Epoch 171/200\n",
      "222/222 [==============================] - 1s - loss: 0.4916 - acc: 0.8063\n",
      "Epoch 172/200\n",
      "222/222 [==============================] - 1s - loss: 0.4906 - acc: 0.8063\n",
      "Epoch 173/200\n",
      "222/222 [==============================] - 1s - loss: 0.4896 - acc: 0.8063\n",
      "Epoch 174/200\n",
      "222/222 [==============================] - 1s - loss: 0.4886 - acc: 0.8063\n",
      "Epoch 175/200\n",
      "222/222 [==============================] - 1s - loss: 0.4876 - acc: 0.8063\n",
      "Epoch 176/200\n",
      "222/222 [==============================] - 1s - loss: 0.4866 - acc: 0.8063\n",
      "Epoch 177/200\n",
      "222/222 [==============================] - 1s - loss: 0.4856 - acc: 0.8063\n",
      "Epoch 178/200\n",
      "222/222 [==============================] - 1s - loss: 0.4846 - acc: 0.8063\n",
      "Epoch 179/200\n",
      "222/222 [==============================] - 1s - loss: 0.4836 - acc: 0.8063\n",
      "Epoch 180/200\n",
      "222/222 [==============================] - 1s - loss: 0.4826 - acc: 0.8108\n",
      "Epoch 181/200\n",
      "222/222 [==============================] - 1s - loss: 0.4816 - acc: 0.8108\n",
      "Epoch 182/200\n",
      "222/222 [==============================] - 1s - loss: 0.4805 - acc: 0.8108\n",
      "Epoch 183/200\n",
      "222/222 [==============================] - 1s - loss: 0.4795 - acc: 0.8108\n",
      "Epoch 184/200\n",
      "222/222 [==============================] - 1s - loss: 0.4785 - acc: 0.8108\n",
      "Epoch 185/200\n",
      "222/222 [==============================] - 1s - loss: 0.4775 - acc: 0.8108\n",
      "Epoch 186/200\n",
      "222/222 [==============================] - 1s - loss: 0.4765 - acc: 0.8108\n",
      "Epoch 187/200\n",
      "222/222 [==============================] - 1s - loss: 0.4754 - acc: 0.8108\n",
      "Epoch 188/200\n",
      "222/222 [==============================] - 1s - loss: 0.4744 - acc: 0.8108\n",
      "Epoch 189/200\n",
      "222/222 [==============================] - 1s - loss: 0.4734 - acc: 0.8108\n",
      "Epoch 190/200\n",
      "222/222 [==============================] - 1s - loss: 0.4723 - acc: 0.8108\n",
      "Epoch 191/200\n",
      "222/222 [==============================] - 1s - loss: 0.4713 - acc: 0.8108\n",
      "Epoch 192/200\n",
      "222/222 [==============================] - 1s - loss: 0.4703 - acc: 0.8108\n",
      "Epoch 193/200\n",
      "222/222 [==============================] - 1s - loss: 0.4692 - acc: 0.8153\n",
      "Epoch 194/200\n",
      "222/222 [==============================] - 1s - loss: 0.4682 - acc: 0.8153\n",
      "Epoch 195/200\n",
      "222/222 [==============================] - 1s - loss: 0.4671 - acc: 0.8153\n",
      "Epoch 196/200\n",
      "222/222 [==============================] - 1s - loss: 0.4661 - acc: 0.8198\n",
      "Epoch 197/200\n",
      "222/222 [==============================] - 1s - loss: 0.4650 - acc: 0.8198\n",
      "Epoch 198/200\n",
      "222/222 [==============================] - 1s - loss: 0.4640 - acc: 0.8198\n",
      "Epoch 199/200\n",
      "222/222 [==============================] - 1s - loss: 0.4629 - acc: 0.8243\n",
      "Epoch 200/200\n",
      "222/222 [==============================] - 1s - loss: 0.4619 - acc: 0.8288\n",
      "24/24 [==============================] - 0s\n",
      "Epoch 1/200\n",
      "222/222 [==============================] - 1s - loss: 2.3560 - acc: 0.5180\n",
      "Epoch 2/200\n",
      "222/222 [==============================] - 1s - loss: 8.1226 - acc: 0.4820\n",
      "Epoch 3/200\n",
      "222/222 [==============================] - 1s - loss: 1.7599 - acc: 0.4820\n",
      "Epoch 4/200\n",
      "222/222 [==============================] - 1s - loss: 3.5388 - acc: 0.5180\n",
      "Epoch 5/200\n",
      "222/222 [==============================] - 1s - loss: 3.1705 - acc: 0.5180\n",
      "Epoch 6/200\n",
      "222/222 [==============================] - 1s - loss: 1.1296 - acc: 0.5180\n",
      "Epoch 7/200\n",
      "222/222 [==============================] - 1s - loss: 1.4304 - acc: 0.4820\n",
      "Epoch 8/200\n",
      "222/222 [==============================] - 1s - loss: 1.7905 - acc: 0.4820\n",
      "Epoch 9/200\n",
      "222/222 [==============================] - 1s - loss: 0.9394 - acc: 0.4820\n",
      "Epoch 10/200\n",
      "222/222 [==============================] - 1s - loss: 1.0552 - acc: 0.5180\n",
      "Epoch 11/200\n",
      "222/222 [==============================] - 1s - loss: 1.3810 - acc: 0.5180\n",
      "Epoch 12/200\n",
      "222/222 [==============================] - 1s - loss: 0.8858 - acc: 0.5180\n",
      "Epoch 13/200\n",
      "222/222 [==============================] - 1s - loss: 0.8021 - acc: 0.4820\n",
      "Epoch 14/200\n",
      "222/222 [==============================] - 1s - loss: 1.0768 - acc: 0.4820\n",
      "Epoch 15/200\n",
      "222/222 [==============================] - 1s - loss: 0.7486 - acc: 0.4820\n",
      "Epoch 16/200\n",
      "222/222 [==============================] - 1s - loss: 0.7983 - acc: 0.5180\n",
      "Epoch 17/200\n",
      "222/222 [==============================] - 1s - loss: 0.9537 - acc: 0.5180\n",
      "Epoch 18/200\n",
      "222/222 [==============================] - 1s - loss: 0.7002 - acc: 0.5180\n",
      "Epoch 19/200\n",
      "222/222 [==============================] - 1s - loss: 0.7755 - acc: 0.4820\n",
      "Epoch 20/200\n",
      "222/222 [==============================] - 1s - loss: 0.8684 - acc: 0.4820\n",
      "Epoch 21/200\n",
      "222/222 [==============================] - 1s - loss: 0.6662 - acc: 0.5676\n",
      "Epoch 22/200\n",
      "222/222 [==============================] - 1s - loss: 0.7624 - acc: 0.5180\n",
      "Epoch 23/200\n",
      "222/222 [==============================] - 1s - loss: 0.7987 - acc: 0.5180\n",
      "Epoch 24/200\n",
      "222/222 [==============================] - 1s - loss: 0.6496 - acc: 0.6396\n",
      "Epoch 25/200\n",
      "222/222 [==============================] - 1s - loss: 0.7455 - acc: 0.4820\n",
      "Epoch 26/200\n",
      "222/222 [==============================] - 1s - loss: 0.7391 - acc: 0.4820\n",
      "Epoch 27/200\n",
      "222/222 [==============================] - 1s - loss: 0.6416 - acc: 0.6667\n",
      "Epoch 28/200\n",
      "222/222 [==============================] - 1s - loss: 0.7261 - acc: 0.5180\n",
      "Epoch 29/200\n",
      "222/222 [==============================] - 1s - loss: 0.6905 - acc: 0.5180\n",
      "Epoch 30/200\n",
      "222/222 [==============================] - 1s - loss: 0.6418 - acc: 0.6712\n",
      "Epoch 31/200\n",
      "222/222 [==============================] - 1s - loss: 0.7076 - acc: 0.4955\n",
      "Epoch 32/200\n",
      "222/222 [==============================] - 1s - loss: 0.6520 - acc: 0.5946\n",
      "Epoch 33/200\n",
      "222/222 [==============================] - 1s - loss: 0.6501 - acc: 0.5405\n",
      "Epoch 34/200\n",
      "222/222 [==============================] - 1s - loss: 0.6831 - acc: 0.5225\n",
      "Epoch 35/200\n",
      "222/222 [==============================] - 1s - loss: 0.6286 - acc: 0.6486\n",
      "Epoch 36/200\n",
      "222/222 [==============================] - 1s - loss: 0.6576 - acc: 0.5721\n",
      "Epoch 37/200\n",
      "222/222 [==============================] - 1s - loss: 0.6527 - acc: 0.5856\n",
      "Epoch 38/200\n",
      "222/222 [==============================] - 1s - loss: 0.6236 - acc: 0.6802\n",
      "Epoch 39/200\n",
      "222/222 [==============================] - 1s - loss: 0.6546 - acc: 0.5315\n",
      "Epoch 40/200\n",
      "222/222 [==============================] - 1s - loss: 0.6261 - acc: 0.6441\n",
      "Epoch 41/200\n",
      "222/222 [==============================] - 1s - loss: 0.6295 - acc: 0.6712\n",
      "Epoch 42/200\n",
      "222/222 [==============================] - 1s - loss: 0.6383 - acc: 0.6171\n",
      "Epoch 43/200\n",
      "222/222 [==============================] - 1s - loss: 0.6139 - acc: 0.7387\n",
      "Epoch 44/200\n",
      "222/222 [==============================] - 1s - loss: 0.6324 - acc: 0.5946\n",
      "Epoch 45/200\n",
      "222/222 [==============================] - 1s - loss: 0.6175 - acc: 0.6532\n",
      "Epoch 46/200\n",
      "222/222 [==============================] - 1s - loss: 0.6162 - acc: 0.6892\n",
      "Epoch 47/200\n",
      "222/222 [==============================] - 1s - loss: 0.6226 - acc: 0.6667\n",
      "Epoch 48/200\n",
      "222/222 [==============================] - 1s - loss: 0.6069 - acc: 0.7297\n",
      "Epoch 49/200\n",
      "222/222 [==============================] - 1s - loss: 0.6183 - acc: 0.6486\n",
      "Epoch 50/200\n",
      "222/222 [==============================] - 1s - loss: 0.6073 - acc: 0.6847\n",
      "Epoch 51/200\n",
      "222/222 [==============================] - 1s - loss: 0.6085 - acc: 0.6982\n",
      "Epoch 52/200\n",
      "222/222 [==============================] - 1s - loss: 0.6094 - acc: 0.6937\n",
      "Epoch 53/200\n",
      "222/222 [==============================] - 1s - loss: 0.6011 - acc: 0.7207\n",
      "Epoch 54/200\n",
      "222/222 [==============================] - 1s - loss: 0.6075 - acc: 0.6486\n",
      "Epoch 55/200\n",
      "222/222 [==============================] - 1s - loss: 0.5986 - acc: 0.7252\n",
      "Epoch 56/200\n",
      "222/222 [==============================] - 1s - loss: 0.6021 - acc: 0.6937\n",
      "Epoch 57/200\n",
      "222/222 [==============================] - 1s - loss: 0.5985 - acc: 0.7027\n",
      "Epoch 58/200\n",
      "222/222 [==============================] - 1s - loss: 0.5962 - acc: 0.7117\n",
      "Epoch 59/200\n",
      "222/222 [==============================] - 1s - loss: 0.5977 - acc: 0.7072\n",
      "Epoch 60/200\n",
      "222/222 [==============================] - 1s - loss: 0.5923 - acc: 0.7387\n",
      "Epoch 61/200\n",
      "222/222 [==============================] - 1s - loss: 0.5951 - acc: 0.7027\n",
      "Epoch 62/200\n",
      "222/222 [==============================] - 1s - loss: 0.5901 - acc: 0.7297\n",
      "Epoch 63/200\n",
      "222/222 [==============================] - 1s - loss: 0.5915 - acc: 0.7207\n",
      "Epoch 64/200\n",
      "222/222 [==============================] - 1s - loss: 0.5888 - acc: 0.7252\n",
      "Epoch 65/200\n",
      "222/222 [==============================] - 1s - loss: 0.5879 - acc: 0.7342\n",
      "Epoch 66/200\n",
      "222/222 [==============================] - 1s - loss: 0.5874 - acc: 0.7297\n",
      "Epoch 67/200\n",
      "222/222 [==============================] - 1s - loss: 0.5848 - acc: 0.7432\n",
      "Epoch 68/200\n",
      "222/222 [==============================] - 1s - loss: 0.5854 - acc: 0.7297\n",
      "Epoch 69/200\n",
      "222/222 [==============================] - 1s - loss: 0.5824 - acc: 0.7432\n",
      "Epoch 70/200\n",
      "222/222 [==============================] - 1s - loss: 0.5829 - acc: 0.7342\n",
      "Epoch 71/200\n",
      "222/222 [==============================] - 1s - loss: 0.5803 - acc: 0.7297\n",
      "Epoch 72/200\n",
      "222/222 [==============================] - 1s - loss: 0.5804 - acc: 0.7297\n",
      "Epoch 73/200\n",
      "222/222 [==============================] - 1s - loss: 0.5786 - acc: 0.7387\n",
      "Epoch 74/200\n",
      "222/222 [==============================] - 1s - loss: 0.5779 - acc: 0.7432\n",
      "Epoch 75/200\n",
      "222/222 [==============================] - 1s - loss: 0.5768 - acc: 0.7477\n",
      "Epoch 76/200\n",
      "222/222 [==============================] - 1s - loss: 0.5756 - acc: 0.7432\n",
      "Epoch 77/200\n",
      "222/222 [==============================] - 1s - loss: 0.5748 - acc: 0.7432\n",
      "Epoch 78/200\n",
      "222/222 [==============================] - 1s - loss: 0.5733 - acc: 0.7342\n",
      "Epoch 79/200\n",
      "222/222 [==============================] - 1s - loss: 0.5727 - acc: 0.7477\n",
      "Epoch 80/200\n",
      "222/222 [==============================] - 1s - loss: 0.5712 - acc: 0.7387\n",
      "Epoch 81/200\n",
      "222/222 [==============================] - 1s - loss: 0.5707 - acc: 0.7432\n",
      "Epoch 82/200\n",
      "222/222 [==============================] - 1s - loss: 0.5692 - acc: 0.7432\n",
      "Epoch 83/200\n",
      "222/222 [==============================] - 1s - loss: 0.5687 - acc: 0.7523\n",
      "Epoch 84/200\n",
      "222/222 [==============================] - 1s - loss: 0.5673 - acc: 0.7342\n",
      "Epoch 85/200\n",
      "222/222 [==============================] - 1s - loss: 0.5667 - acc: 0.7432\n",
      "Epoch 86/200\n",
      "222/222 [==============================] - 1s - loss: 0.5654 - acc: 0.7432\n",
      "Epoch 87/200\n",
      "222/222 [==============================] - 1s - loss: 0.5647 - acc: 0.7523\n",
      "Epoch 88/200\n",
      "222/222 [==============================] - 1s - loss: 0.5635 - acc: 0.7568\n",
      "Epoch 89/200\n",
      "222/222 [==============================] - 1s - loss: 0.5627 - acc: 0.7477\n",
      "Epoch 90/200\n",
      "222/222 [==============================] - 1s - loss: 0.5617 - acc: 0.7432\n",
      "Epoch 91/200\n",
      "222/222 [==============================] - 1s - loss: 0.5608 - acc: 0.7613\n",
      "Epoch 92/200\n",
      "222/222 [==============================] - 1s - loss: 0.5598 - acc: 0.7613\n",
      "Epoch 93/200\n",
      "222/222 [==============================] - 1s - loss: 0.5589 - acc: 0.7432\n",
      "Epoch 94/200\n",
      "222/222 [==============================] - 1s - loss: 0.5579 - acc: 0.7432\n",
      "Epoch 95/200\n",
      "222/222 [==============================] - 1s - loss: 0.5570 - acc: 0.7613\n",
      "Epoch 96/200\n",
      "222/222 [==============================] - 1s - loss: 0.5561 - acc: 0.7613\n",
      "Epoch 97/200\n",
      "222/222 [==============================] - 1s - loss: 0.5552 - acc: 0.7477\n",
      "Epoch 98/200\n",
      "222/222 [==============================] - 1s - loss: 0.5542 - acc: 0.7477\n",
      "Epoch 99/200\n",
      "222/222 [==============================] - 1s - loss: 0.5533 - acc: 0.7658\n",
      "Epoch 100/200\n",
      "222/222 [==============================] - 1s - loss: 0.5524 - acc: 0.7658\n",
      "Epoch 101/200\n",
      "222/222 [==============================] - 1s - loss: 0.5515 - acc: 0.7477\n",
      "Epoch 102/200\n",
      "222/222 [==============================] - 1s - loss: 0.5506 - acc: 0.7568\n",
      "Epoch 103/200\n",
      "222/222 [==============================] - 1s - loss: 0.5496 - acc: 0.7658\n",
      "Epoch 104/200\n",
      "222/222 [==============================] - 1s - loss: 0.5487 - acc: 0.7658\n",
      "Epoch 105/200\n",
      "222/222 [==============================] - 1s - loss: 0.5478 - acc: 0.7658\n",
      "Epoch 106/200\n",
      "222/222 [==============================] - 1s - loss: 0.5469 - acc: 0.7658\n",
      "Epoch 107/200\n",
      "222/222 [==============================] - 1s - loss: 0.5460 - acc: 0.7658\n",
      "Epoch 108/200\n",
      "222/222 [==============================] - 1s - loss: 0.5451 - acc: 0.7703\n",
      "Epoch 109/200\n",
      "222/222 [==============================] - 1s - loss: 0.5442 - acc: 0.7658\n",
      "Epoch 110/200\n",
      "222/222 [==============================] - 1s - loss: 0.5432 - acc: 0.7658\n",
      "Epoch 111/200\n",
      "222/222 [==============================] - 1s - loss: 0.5424 - acc: 0.7748\n",
      "Epoch 112/200\n",
      "222/222 [==============================] - 1s - loss: 0.5415 - acc: 0.7748\n",
      "Epoch 113/200\n",
      "222/222 [==============================] - 1s - loss: 0.5406 - acc: 0.7658\n",
      "Epoch 114/200\n",
      "222/222 [==============================] - 1s - loss: 0.5397 - acc: 0.7703\n",
      "Epoch 115/200\n",
      "222/222 [==============================] - 1s - loss: 0.5388 - acc: 0.7748\n",
      "Epoch 116/200\n",
      "222/222 [==============================] - 1s - loss: 0.5379 - acc: 0.7748\n",
      "Epoch 117/200\n",
      "222/222 [==============================] - 1s - loss: 0.5370 - acc: 0.7748\n",
      "Epoch 118/200\n",
      "222/222 [==============================] - 1s - loss: 0.5361 - acc: 0.7748\n",
      "Epoch 119/200\n",
      "222/222 [==============================] - 1s - loss: 0.5352 - acc: 0.7748\n",
      "Epoch 120/200\n",
      "222/222 [==============================] - 1s - loss: 0.5343 - acc: 0.7793\n",
      "Epoch 121/200\n",
      "222/222 [==============================] - 1s - loss: 0.5334 - acc: 0.7748\n",
      "Epoch 122/200\n",
      "222/222 [==============================] - 1s - loss: 0.5326 - acc: 0.7748\n",
      "Epoch 123/200\n",
      "222/222 [==============================] - 1s - loss: 0.5317 - acc: 0.7748\n",
      "Epoch 124/200\n",
      "222/222 [==============================] - 1s - loss: 0.5308 - acc: 0.7793\n",
      "Epoch 125/200\n",
      "222/222 [==============================] - 1s - loss: 0.5299 - acc: 0.7793\n",
      "Epoch 126/200\n",
      "222/222 [==============================] - 1s - loss: 0.5290 - acc: 0.7793\n",
      "Epoch 127/200\n",
      "222/222 [==============================] - 1s - loss: 0.5282 - acc: 0.7793\n",
      "Epoch 128/200\n",
      "222/222 [==============================] - 1s - loss: 0.5273 - acc: 0.7793\n",
      "Epoch 129/200\n",
      "222/222 [==============================] - 1s - loss: 0.5264 - acc: 0.7793\n",
      "Epoch 130/200\n",
      "222/222 [==============================] - 1s - loss: 0.5255 - acc: 0.7793\n",
      "Epoch 131/200\n",
      "222/222 [==============================] - 1s - loss: 0.5246 - acc: 0.7793\n",
      "Epoch 132/200\n",
      "222/222 [==============================] - 1s - loss: 0.5238 - acc: 0.7838\n",
      "Epoch 133/200\n",
      "222/222 [==============================] - 1s - loss: 0.5229 - acc: 0.7793\n",
      "Epoch 134/200\n",
      "222/222 [==============================] - 1s - loss: 0.5220 - acc: 0.7838\n",
      "Epoch 135/200\n",
      "222/222 [==============================] - 1s - loss: 0.5211 - acc: 0.7883\n",
      "Epoch 136/200\n",
      "222/222 [==============================] - 1s - loss: 0.5202 - acc: 0.7883\n",
      "Epoch 137/200\n",
      "222/222 [==============================] - 1s - loss: 0.5194 - acc: 0.7883\n",
      "Epoch 138/200\n",
      "222/222 [==============================] - 1s - loss: 0.5185 - acc: 0.7883\n",
      "Epoch 139/200\n",
      "222/222 [==============================] - 1s - loss: 0.5176 - acc: 0.7928\n",
      "Epoch 140/200\n",
      "222/222 [==============================] - 1s - loss: 0.5167 - acc: 0.7928\n",
      "Epoch 141/200\n",
      "222/222 [==============================] - 1s - loss: 0.5158 - acc: 0.7883\n",
      "Epoch 142/200\n",
      "222/222 [==============================] - 1s - loss: 0.5150 - acc: 0.7928\n",
      "Epoch 143/200\n",
      "222/222 [==============================] - 1s - loss: 0.5141 - acc: 0.7928\n",
      "Epoch 144/200\n",
      "222/222 [==============================] - 1s - loss: 0.5132 - acc: 0.7928\n",
      "Epoch 145/200\n",
      "222/222 [==============================] - 1s - loss: 0.5123 - acc: 0.7928\n",
      "Epoch 146/200\n",
      "222/222 [==============================] - 1s - loss: 0.5114 - acc: 0.7928\n",
      "Epoch 147/200\n",
      "222/222 [==============================] - 1s - loss: 0.5106 - acc: 0.7928\n",
      "Epoch 148/200\n",
      "222/222 [==============================] - 1s - loss: 0.5097 - acc: 0.7928\n",
      "Epoch 149/200\n",
      "222/222 [==============================] - 1s - loss: 0.5088 - acc: 0.7928\n",
      "Epoch 150/200\n",
      "222/222 [==============================] - 1s - loss: 0.5079 - acc: 0.7973\n",
      "Epoch 151/200\n",
      "222/222 [==============================] - 1s - loss: 0.5071 - acc: 0.7973\n",
      "Epoch 152/200\n",
      "222/222 [==============================] - 1s - loss: 0.5062 - acc: 0.7973\n",
      "Epoch 153/200\n",
      "222/222 [==============================] - 1s - loss: 0.5053 - acc: 0.7973\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 1s - loss: 0.5044 - acc: 0.7973\n",
      "Epoch 155/200\n",
      "222/222 [==============================] - 1s - loss: 0.5035 - acc: 0.8018\n",
      "Epoch 156/200\n",
      "222/222 [==============================] - 1s - loss: 0.5027 - acc: 0.7973\n",
      "Epoch 157/200\n",
      "222/222 [==============================] - 1s - loss: 0.5018 - acc: 0.8018\n",
      "Epoch 158/200\n",
      "222/222 [==============================] - 1s - loss: 0.5009 - acc: 0.8018\n",
      "Epoch 159/200\n",
      "222/222 [==============================] - 1s - loss: 0.5000 - acc: 0.8018\n",
      "Epoch 160/200\n",
      "222/222 [==============================] - 1s - loss: 0.4991 - acc: 0.8018\n",
      "Epoch 161/200\n",
      "222/222 [==============================] - 1s - loss: 0.4982 - acc: 0.8018\n",
      "Epoch 162/200\n",
      "222/222 [==============================] - 1s - loss: 0.4974 - acc: 0.7973\n",
      "Epoch 163/200\n",
      "222/222 [==============================] - 1s - loss: 0.4965 - acc: 0.7973\n",
      "Epoch 164/200\n",
      "222/222 [==============================] - 1s - loss: 0.4956 - acc: 0.7973\n",
      "Epoch 165/200\n",
      "222/222 [==============================] - 1s - loss: 0.4947 - acc: 0.7973\n",
      "Epoch 166/200\n",
      "222/222 [==============================] - 1s - loss: 0.4938 - acc: 0.7973\n",
      "Epoch 167/200\n",
      "222/222 [==============================] - 1s - loss: 0.4930 - acc: 0.8018\n",
      "Epoch 168/200\n",
      "222/222 [==============================] - 1s - loss: 0.4921 - acc: 0.8018\n",
      "Epoch 169/200\n",
      "222/222 [==============================] - 1s - loss: 0.4912 - acc: 0.8018\n",
      "Epoch 170/200\n",
      "222/222 [==============================] - 1s - loss: 0.4903 - acc: 0.8018\n",
      "Epoch 171/200\n",
      "222/222 [==============================] - 1s - loss: 0.4894 - acc: 0.8018\n",
      "Epoch 172/200\n",
      "222/222 [==============================] - 1s - loss: 0.4885 - acc: 0.8018\n",
      "Epoch 173/200\n",
      "222/222 [==============================] - 1s - loss: 0.4876 - acc: 0.8018\n",
      "Epoch 174/200\n",
      "222/222 [==============================] - 1s - loss: 0.4868 - acc: 0.8063\n",
      "Epoch 175/200\n",
      "222/222 [==============================] - 1s - loss: 0.4859 - acc: 0.8063\n",
      "Epoch 176/200\n",
      "222/222 [==============================] - 1s - loss: 0.4850 - acc: 0.8108\n",
      "Epoch 177/200\n",
      "222/222 [==============================] - 1s - loss: 0.4841 - acc: 0.8108\n",
      "Epoch 178/200\n",
      "222/222 [==============================] - 1s - loss: 0.4832 - acc: 0.8108\n",
      "Epoch 179/200\n",
      "222/222 [==============================] - 1s - loss: 0.4823 - acc: 0.8108\n",
      "Epoch 180/200\n",
      "222/222 [==============================] - 1s - loss: 0.4814 - acc: 0.8108\n",
      "Epoch 181/200\n",
      "222/222 [==============================] - 1s - loss: 0.4806 - acc: 0.8108\n",
      "Epoch 182/200\n",
      "222/222 [==============================] - 1s - loss: 0.4797 - acc: 0.8108\n",
      "Epoch 183/200\n",
      "222/222 [==============================] - 1s - loss: 0.4788 - acc: 0.8108\n",
      "Epoch 184/200\n",
      "222/222 [==============================] - 1s - loss: 0.4779 - acc: 0.8108\n",
      "Epoch 185/200\n",
      "222/222 [==============================] - 1s - loss: 0.4770 - acc: 0.8108\n",
      "Epoch 186/200\n",
      "222/222 [==============================] - 1s - loss: 0.4761 - acc: 0.8108\n",
      "Epoch 187/200\n",
      "222/222 [==============================] - 1s - loss: 0.4752 - acc: 0.8108\n",
      "Epoch 188/200\n",
      "222/222 [==============================] - 1s - loss: 0.4743 - acc: 0.8108\n",
      "Epoch 189/200\n",
      "222/222 [==============================] - 1s - loss: 0.4734 - acc: 0.8108\n",
      "Epoch 190/200\n",
      "222/222 [==============================] - 1s - loss: 0.4726 - acc: 0.8108\n",
      "Epoch 191/200\n",
      "222/222 [==============================] - 1s - loss: 0.4717 - acc: 0.8108\n",
      "Epoch 192/200\n",
      "222/222 [==============================] - 1s - loss: 0.4708 - acc: 0.8108\n",
      "Epoch 193/200\n",
      "222/222 [==============================] - 1s - loss: 0.4699 - acc: 0.8108\n",
      "Epoch 194/200\n",
      "222/222 [==============================] - 1s - loss: 0.4690 - acc: 0.8108\n",
      "Epoch 195/200\n",
      "222/222 [==============================] - 1s - loss: 0.4681 - acc: 0.8108\n",
      "Epoch 196/200\n",
      "222/222 [==============================] - 1s - loss: 0.4672 - acc: 0.8108\n",
      "Epoch 197/200\n",
      "222/222 [==============================] - 1s - loss: 0.4663 - acc: 0.8108\n",
      "Epoch 198/200\n",
      "222/222 [==============================] - 1s - loss: 0.4654 - acc: 0.8108\n",
      "Epoch 199/200\n",
      "222/222 [==============================] - 1s - loss: 0.4645 - acc: 0.8108\n",
      "Epoch 200/200\n",
      "222/222 [==============================] - 1s - loss: 0.4637 - acc: 0.8108\n",
      "24/24 [==============================] - 0s\n",
      "Epoch 1/200\n",
      "222/222 [==============================] - 1s - loss: 0.8478 - acc: 0.4820\n",
      "Epoch 2/200\n",
      "222/222 [==============================] - 1s - loss: 3.9975 - acc: 0.5180\n",
      "Epoch 3/200\n",
      "222/222 [==============================] - 1s - loss: 1.3951 - acc: 0.5180\n",
      "Epoch 4/200\n",
      "222/222 [==============================] - 1s - loss: 0.8017 - acc: 0.4820\n",
      "Epoch 5/200\n",
      "222/222 [==============================] - 1s - loss: 0.9006 - acc: 0.4820\n",
      "Epoch 6/200\n",
      "222/222 [==============================] - 1s - loss: 0.7390 - acc: 0.4820\n",
      "Epoch 7/200\n",
      "222/222 [==============================] - 1s - loss: 0.6983 - acc: 0.5180\n",
      "Epoch 8/200\n",
      "222/222 [==============================] - 1s - loss: 0.7516 - acc: 0.5180\n",
      "Epoch 9/200\n",
      "222/222 [==============================] - 1s - loss: 0.7435 - acc: 0.5180\n",
      "Epoch 10/200\n",
      "222/222 [==============================] - 1s - loss: 0.6956 - acc: 0.5180\n",
      "Epoch 11/200\n",
      "222/222 [==============================] - 1s - loss: 0.6994 - acc: 0.4820\n",
      "Epoch 12/200\n",
      "222/222 [==============================] - 1s - loss: 0.7277 - acc: 0.4820\n",
      "Epoch 13/200\n",
      "222/222 [==============================] - 1s - loss: 0.7133 - acc: 0.4820\n",
      "Epoch 14/200\n",
      "222/222 [==============================] - 1s - loss: 0.6859 - acc: 0.5901\n",
      "Epoch 15/200\n",
      "222/222 [==============================] - 1s - loss: 0.6919 - acc: 0.5180\n",
      "Epoch 16/200\n",
      "222/222 [==============================] - 1s - loss: 0.7075 - acc: 0.5180\n",
      "Epoch 17/200\n",
      "222/222 [==============================] - 1s - loss: 0.6977 - acc: 0.5180\n",
      "Epoch 18/200\n",
      "222/222 [==============================] - 1s - loss: 0.6805 - acc: 0.5315\n",
      "Epoch 19/200\n",
      "222/222 [==============================] - 1s - loss: 0.6849 - acc: 0.5180\n",
      "Epoch 20/200\n",
      "222/222 [==============================] - 1s - loss: 0.6950 - acc: 0.4820\n",
      "Epoch 21/200\n",
      "222/222 [==============================] - 1s - loss: 0.6871 - acc: 0.5000\n",
      "Epoch 22/200\n",
      "222/222 [==============================] - 1s - loss: 0.6758 - acc: 0.6441\n",
      "Epoch 23/200\n",
      "222/222 [==============================] - 1s - loss: 0.6801 - acc: 0.5180\n",
      "Epoch 24/200\n",
      "222/222 [==============================] - 1s - loss: 0.6854 - acc: 0.5180\n",
      "Epoch 25/200\n",
      "222/222 [==============================] - 1s - loss: 0.6780 - acc: 0.5180\n",
      "Epoch 26/200\n",
      "222/222 [==============================] - 1s - loss: 0.6717 - acc: 0.6486\n",
      "Epoch 27/200\n",
      "222/222 [==============================] - 1s - loss: 0.6761 - acc: 0.5991\n",
      "Epoch 28/200\n",
      "222/222 [==============================] - 1s - loss: 0.6775 - acc: 0.5631\n",
      "Epoch 29/200\n",
      "222/222 [==============================] - 1s - loss: 0.6707 - acc: 0.6351\n",
      "Epoch 30/200\n",
      "222/222 [==============================] - 1s - loss: 0.6684 - acc: 0.5721\n",
      "Epoch 31/200\n",
      "222/222 [==============================] - 1s - loss: 0.6718 - acc: 0.5270\n",
      "Epoch 32/200\n",
      "222/222 [==============================] - 1s - loss: 0.6699 - acc: 0.5225\n",
      "Epoch 33/200\n",
      "222/222 [==============================] - 1s - loss: 0.6649 - acc: 0.5991\n",
      "Epoch 34/200\n",
      "222/222 [==============================] - 1s - loss: 0.6654 - acc: 0.6441\n",
      "Epoch 35/200\n",
      "222/222 [==============================] - 1s - loss: 0.6667 - acc: 0.6532\n",
      "Epoch 36/200\n",
      "222/222 [==============================] - 1s - loss: 0.6631 - acc: 0.6441\n",
      "Epoch 37/200\n",
      "222/222 [==============================] - 1s - loss: 0.6609 - acc: 0.6351\n",
      "Epoch 38/200\n",
      "222/222 [==============================] - 1s - loss: 0.6622 - acc: 0.5631\n",
      "Epoch 39/200\n",
      "222/222 [==============================] - 1s - loss: 0.6608 - acc: 0.5631\n",
      "Epoch 40/200\n",
      "222/222 [==============================] - 1s - loss: 0.6576 - acc: 0.6757\n",
      "Epoch 41/200\n",
      "222/222 [==============================] - 1s - loss: 0.6575 - acc: 0.6577\n",
      "Epoch 42/200\n",
      "222/222 [==============================] - 1s - loss: 0.6573 - acc: 0.6532\n",
      "Epoch 43/200\n",
      "222/222 [==============================] - 1s - loss: 0.6548 - acc: 0.6892\n",
      "Epoch 44/200\n",
      "222/222 [==============================] - 1s - loss: 0.6539 - acc: 0.6532\n",
      "Epoch 45/200\n",
      "222/222 [==============================] - 1s - loss: 0.6539 - acc: 0.5946\n",
      "Epoch 46/200\n",
      "222/222 [==============================] - 1s - loss: 0.6520 - acc: 0.6532\n",
      "Epoch 47/200\n",
      "222/222 [==============================] - 1s - loss: 0.6507 - acc: 0.7027\n",
      "Epoch 48/200\n",
      "222/222 [==============================] - 1s - loss: 0.6505 - acc: 0.6847\n",
      "Epoch 49/200\n",
      "222/222 [==============================] - 1s - loss: 0.6490 - acc: 0.6757\n",
      "Epoch 50/200\n",
      "222/222 [==============================] - 1s - loss: 0.6476 - acc: 0.7027\n",
      "Epoch 51/200\n",
      "222/222 [==============================] - 1s - loss: 0.6472 - acc: 0.6577\n",
      "Epoch 52/200\n",
      "222/222 [==============================] - 1s - loss: 0.6460 - acc: 0.6712\n",
      "Epoch 53/200\n",
      "222/222 [==============================] - 1s - loss: 0.6446 - acc: 0.6937\n",
      "Epoch 54/200\n",
      "222/222 [==============================] - 1s - loss: 0.6440 - acc: 0.6892\n",
      "Epoch 55/200\n",
      "222/222 [==============================] - 1s - loss: 0.6430 - acc: 0.6892\n",
      "Epoch 56/200\n",
      "222/222 [==============================] - 1s - loss: 0.6416 - acc: 0.6892\n",
      "Epoch 57/200\n",
      "222/222 [==============================] - 1s - loss: 0.6409 - acc: 0.6892\n",
      "Epoch 58/200\n",
      "222/222 [==============================] - 1s - loss: 0.6400 - acc: 0.6937\n",
      "Epoch 59/200\n",
      "222/222 [==============================] - 1s - loss: 0.6387 - acc: 0.6892\n",
      "Epoch 60/200\n",
      "222/222 [==============================] - 1s - loss: 0.6380 - acc: 0.7072\n",
      "Epoch 61/200\n",
      "222/222 [==============================] - 1s - loss: 0.6370 - acc: 0.7027\n",
      "Epoch 62/200\n",
      "222/222 [==============================] - 1s - loss: 0.6358 - acc: 0.6982\n",
      "Epoch 63/200\n",
      "222/222 [==============================] - 1s - loss: 0.6350 - acc: 0.7027\n",
      "Epoch 64/200\n",
      "222/222 [==============================] - 1s - loss: 0.6340 - acc: 0.6982\n",
      "Epoch 65/200\n",
      "222/222 [==============================] - 1s - loss: 0.6329 - acc: 0.6982\n",
      "Epoch 66/200\n",
      "222/222 [==============================] - 1s - loss: 0.6321 - acc: 0.7027\n",
      "Epoch 67/200\n",
      "222/222 [==============================] - 1s - loss: 0.6311 - acc: 0.7072\n",
      "Epoch 68/200\n",
      "222/222 [==============================] - 1s - loss: 0.6301 - acc: 0.6982\n",
      "Epoch 69/200\n",
      "222/222 [==============================] - 1s - loss: 0.6292 - acc: 0.6982\n",
      "Epoch 70/200\n",
      "222/222 [==============================] - 1s - loss: 0.6282 - acc: 0.6982\n",
      "Epoch 71/200\n",
      "222/222 [==============================] - 1s - loss: 0.6272 - acc: 0.7117\n",
      "Epoch 72/200\n",
      "222/222 [==============================] - 1s - loss: 0.6264 - acc: 0.7027\n",
      "Epoch 73/200\n",
      "222/222 [==============================] - 1s - loss: 0.6254 - acc: 0.7117\n",
      "Epoch 74/200\n",
      "222/222 [==============================] - 1s - loss: 0.6244 - acc: 0.6982\n",
      "Epoch 75/200\n",
      "222/222 [==============================] - 1s - loss: 0.6235 - acc: 0.6982\n",
      "Epoch 76/200\n",
      "222/222 [==============================] - 1s - loss: 0.6225 - acc: 0.7162\n",
      "Epoch 77/200\n",
      "222/222 [==============================] - 1s - loss: 0.6216 - acc: 0.7072\n",
      "Epoch 78/200\n",
      "222/222 [==============================] - 1s - loss: 0.6207 - acc: 0.7072\n",
      "Epoch 79/200\n",
      "222/222 [==============================] - 1s - loss: 0.6197 - acc: 0.7162\n",
      "Epoch 80/200\n",
      "222/222 [==============================] - 1s - loss: 0.6189 - acc: 0.7162\n",
      "Epoch 81/200\n",
      "222/222 [==============================] - 1s - loss: 0.6179 - acc: 0.7162\n",
      "Epoch 82/200\n",
      "222/222 [==============================] - 1s - loss: 0.6170 - acc: 0.7117\n",
      "Epoch 83/200\n",
      "222/222 [==============================] - 1s - loss: 0.6161 - acc: 0.7072\n",
      "Epoch 84/200\n",
      "222/222 [==============================] - 1s - loss: 0.6152 - acc: 0.7207\n",
      "Epoch 85/200\n",
      "222/222 [==============================] - 1s - loss: 0.6143 - acc: 0.7207\n",
      "Epoch 86/200\n",
      "222/222 [==============================] - 1s - loss: 0.6134 - acc: 0.7207\n",
      "Epoch 87/200\n",
      "222/222 [==============================] - 1s - loss: 0.6125 - acc: 0.7162\n",
      "Epoch 88/200\n",
      "222/222 [==============================] - 1s - loss: 0.6116 - acc: 0.7162\n",
      "Epoch 89/200\n",
      "222/222 [==============================] - 1s - loss: 0.6107 - acc: 0.7207\n",
      "Epoch 90/200\n",
      "222/222 [==============================] - 1s - loss: 0.6098 - acc: 0.7207\n",
      "Epoch 91/200\n",
      "222/222 [==============================] - 1s - loss: 0.6089 - acc: 0.7207\n",
      "Epoch 92/200\n",
      "222/222 [==============================] - 1s - loss: 0.6080 - acc: 0.7162\n",
      "Epoch 93/200\n",
      "222/222 [==============================] - 1s - loss: 0.6071 - acc: 0.7162\n",
      "Epoch 94/200\n",
      "222/222 [==============================] - 1s - loss: 0.6062 - acc: 0.7207\n",
      "Epoch 95/200\n",
      "222/222 [==============================] - 1s - loss: 0.6053 - acc: 0.7252\n",
      "Epoch 96/200\n",
      "222/222 [==============================] - 1s - loss: 0.6044 - acc: 0.7252\n",
      "Epoch 97/200\n",
      "222/222 [==============================] - 1s - loss: 0.6035 - acc: 0.7207\n",
      "Epoch 98/200\n",
      "222/222 [==============================] - 1s - loss: 0.6027 - acc: 0.7207\n",
      "Epoch 99/200\n",
      "222/222 [==============================] - 1s - loss: 0.6018 - acc: 0.7252\n",
      "Epoch 100/200\n",
      "222/222 [==============================] - 1s - loss: 0.6009 - acc: 0.7207\n",
      "Epoch 101/200\n",
      "222/222 [==============================] - 1s - loss: 0.6000 - acc: 0.7252\n",
      "Epoch 102/200\n",
      "222/222 [==============================] - 1s - loss: 0.5991 - acc: 0.7252\n",
      "Epoch 103/200\n",
      "222/222 [==============================] - 1s - loss: 0.5982 - acc: 0.7252\n",
      "Epoch 104/200\n",
      "222/222 [==============================] - 1s - loss: 0.5973 - acc: 0.7252\n",
      "Epoch 105/200\n",
      "222/222 [==============================] - 1s - loss: 0.5964 - acc: 0.7252\n",
      "Epoch 106/200\n",
      "222/222 [==============================] - 1s - loss: 0.5955 - acc: 0.7252\n",
      "Epoch 107/200\n",
      "222/222 [==============================] - 1s - loss: 0.5947 - acc: 0.7252\n",
      "Epoch 108/200\n",
      "222/222 [==============================] - 1s - loss: 0.5938 - acc: 0.7252\n",
      "Epoch 109/200\n",
      "222/222 [==============================] - 1s - loss: 0.5929 - acc: 0.7207\n",
      "Epoch 110/200\n",
      "222/222 [==============================] - 1s - loss: 0.5920 - acc: 0.7207\n",
      "Epoch 111/200\n",
      "222/222 [==============================] - 1s - loss: 0.5911 - acc: 0.7252\n",
      "Epoch 112/200\n",
      "222/222 [==============================] - 1s - loss: 0.5902 - acc: 0.7252\n",
      "Epoch 113/200\n",
      "222/222 [==============================] - 1s - loss: 0.5894 - acc: 0.7207\n",
      "Epoch 114/200\n",
      "222/222 [==============================] - 1s - loss: 0.5885 - acc: 0.7207\n",
      "Epoch 115/200\n",
      "222/222 [==============================] - 1s - loss: 0.5876 - acc: 0.7207\n",
      "Epoch 116/200\n",
      "222/222 [==============================] - 1s - loss: 0.5867 - acc: 0.7342\n",
      "Epoch 117/200\n",
      "222/222 [==============================] - 1s - loss: 0.5858 - acc: 0.7207\n",
      "Epoch 118/200\n",
      "222/222 [==============================] - 1s - loss: 0.5849 - acc: 0.7207\n",
      "Epoch 119/200\n",
      "222/222 [==============================] - 1s - loss: 0.5841 - acc: 0.7297\n",
      "Epoch 120/200\n",
      "222/222 [==============================] - 1s - loss: 0.5832 - acc: 0.7342\n",
      "Epoch 121/200\n",
      "222/222 [==============================] - 1s - loss: 0.5823 - acc: 0.7252\n",
      "Epoch 122/200\n",
      "222/222 [==============================] - 1s - loss: 0.5814 - acc: 0.7342\n",
      "Epoch 123/200\n",
      "222/222 [==============================] - 1s - loss: 0.5805 - acc: 0.7342\n",
      "Epoch 124/200\n",
      "222/222 [==============================] - 1s - loss: 0.5796 - acc: 0.7342\n",
      "Epoch 125/200\n",
      "222/222 [==============================] - 1s - loss: 0.5787 - acc: 0.7342\n",
      "Epoch 126/200\n",
      "222/222 [==============================] - 1s - loss: 0.5778 - acc: 0.7342\n",
      "Epoch 127/200\n",
      "222/222 [==============================] - 1s - loss: 0.5769 - acc: 0.7342\n",
      "Epoch 128/200\n",
      "222/222 [==============================] - 1s - loss: 0.5760 - acc: 0.7342\n",
      "Epoch 129/200\n",
      "222/222 [==============================] - 1s - loss: 0.5751 - acc: 0.7342\n",
      "Epoch 130/200\n",
      "222/222 [==============================] - 1s - loss: 0.5742 - acc: 0.7342\n",
      "Epoch 131/200\n",
      "222/222 [==============================] - 1s - loss: 0.5733 - acc: 0.7342\n",
      "Epoch 132/200\n",
      "222/222 [==============================] - 1s - loss: 0.5724 - acc: 0.7342\n",
      "Epoch 133/200\n",
      "222/222 [==============================] - 1s - loss: 0.5715 - acc: 0.7342\n",
      "Epoch 134/200\n",
      "222/222 [==============================] - 1s - loss: 0.5706 - acc: 0.7342\n",
      "Epoch 135/200\n",
      "222/222 [==============================] - 1s - loss: 0.5696 - acc: 0.7342\n",
      "Epoch 136/200\n",
      "222/222 [==============================] - 1s - loss: 0.5687 - acc: 0.7342\n",
      "Epoch 137/200\n",
      "222/222 [==============================] - 1s - loss: 0.5678 - acc: 0.7342\n",
      "Epoch 138/200\n",
      "222/222 [==============================] - 1s - loss: 0.5669 - acc: 0.7387"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import random\n",
    "random.seed(50) \n",
    "\n",
    "kfold = GroupKFold(n_splits=10)\n",
    "#kfold.split(X=train_exprs, y=encoded_Y_train, groups=enc_monkey_train)\n",
    "estimator = KerasClassifier(build_fn=my_model, epochs=200, batch_size = train_exprs.shape[0])\n",
    "results =cross_val_score(estimator, train_exprs, encoded_Y_train, cv=kfold, groups=enc_monkey_train)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "#results\n",
    "\n",
    "#for train_index, test_index in kfold.split(X=train_exprs, y=encoded_Y_train, groups=enc_monkey_train):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index) # setting seed worked to make this consistent\n",
    "#    print(set(train_pheno[\"monkeyid\"][train_index]) & set(train_pheno[\"monkeyid\"][test_index])) # Yes, they are completely disjoint\n",
    "    \n",
    "    \n",
    "# I should really test if this is splitting my data correctly!\n",
    "# set(test_exprs.index) & set(train_exprs.index) #-> They are correctly disjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the 2nd, 3rd and 4th training sets were able to fit the training set, and accuracy was terrible on 1 of those splits (3rd). The first was not able to fit the training data, and I don't know about 5th-10th because they did not print since I was not connected to the server during that time. \n",
    "\n",
    "It is clear to me that I need to me that I need to figure out a way to monitor results well. I need to shorten training times; I need to evaluate for fitting of the test set. Based on running my model below on all training data, I suspect that not being able to train much could have been responsible for some of the later sets having terrible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 59.42% (12.64%)\n",
      "[ 0.5         0.875       0.625       0.79166669  0.5         0.5\n",
      "  0.53571427  0.5714286   0.52173913  0.52173913]\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GSM2227796    M19\n",
       "GSM2227797    M19\n",
       "GSM2227799    M18\n",
       "GSM2227800    M15\n",
       "GSM2227801     M1\n",
       "GSM2227805     M6\n",
       "GSM2227806    M14\n",
       "GSM2227807    M15\n",
       "GSM2227808    M17\n",
       "GSM2227809     M2\n",
       "GSM2227810     M9\n",
       "GSM2227812     M9\n",
       "GSM2227814     M3\n",
       "GSM2227815     M3\n",
       "GSM2227816    M19\n",
       "GSM2227818     M1\n",
       "GSM2227820    M18\n",
       "GSM2227823     M8\n",
       "GSM2227825     M4\n",
       "GSM2227827     M9\n",
       "GSM2227828     M1\n",
       "GSM2227832     M4\n",
       "GSM2227834    M18\n",
       "GSM2227835     M6\n",
       "GSM2227836     M2\n",
       "GSM2227837    M14\n",
       "GSM2227839     M5\n",
       "GSM2227842    M19\n",
       "GSM2227843    M12\n",
       "GSM2227844     M7\n",
       "             ... \n",
       "GSM2228190    M31\n",
       "GSM2228192    M36\n",
       "GSM2228193    M31\n",
       "GSM2228197    M31\n",
       "GSM2228201    M37\n",
       "GSM2228203    M26\n",
       "GSM2228204    M25\n",
       "GSM2228208    M37\n",
       "GSM2228211    M20\n",
       "GSM2228212    M37\n",
       "GSM2228215    M22\n",
       "GSM2228218    M22\n",
       "GSM2228224    M33\n",
       "GSM2228225    M26\n",
       "GSM2228228    M20\n",
       "GSM2228229    M33\n",
       "GSM2228232    M38\n",
       "GSM2228237    M35\n",
       "GSM2228238    M26\n",
       "GSM2228239    M22\n",
       "GSM2228240    M32\n",
       "GSM2228243    M33\n",
       "GSM2228244    M28\n",
       "GSM2228245    M35\n",
       "GSM2228246    M20\n",
       "GSM2228247    M36\n",
       "GSM2228251    M22\n",
       "GSM2228253    M25\n",
       "GSM2228258    M21\n",
       "GSM2228261    M21\n",
       "Name: monkeyid, Length: 246, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pheno[\"monkeyid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.0\n",
    "\n",
    "model = Sequential([\n",
    "    #BatchNormalization(input_shape=train_exprs.shape[1:]),\n",
    "    Dense(5000, activation=\"relu\", input_shape=train_exprs.shape[1:]),\n",
    "    #BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(500, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5000 -> 500 -> 50 -> 10 ->1 model with no dropout\n",
    " - With no minibatches, adam lr=0.00004 in 156 epochs got to 80% accuracy on the training set. At 200 epochs it hit 84% accuracy. There was some substantial oscillation in between but mostly was in the 80% accuracy.\n",
    " - I expect it to do poorly on the test set from overfitting, perhaps 65% accuracy, maybe worse.\n",
    " - WOW! I was dead wrong! It gives 75% accuracy on the test set, 85.78% final accuracy on the training set. I have a high variance problem, but this is creeping up to 80% accurate! I think it may be possible with deep learning or optimizing other machine learning approaches (like gradient boosted machines), or with pathway feature engineering to hit 80% acciracy/\n",
    "     - These test data were monkeys completely randomly set aside, with latent and active TB stratified. Thus this is valid\n",
    " - For model refinement I need to start doing cross-validation, more expensive computationally, but will allow me to get a final unbiased estimate.\n",
    " - Now I have to start with cross-validation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "246/246 [==============================] - 1s - loss: 2.9694 - acc: 0.4837\n",
      "Epoch 2/200\n",
      "246/246 [==============================] - 1s - loss: 1.2799 - acc: 0.5163\n",
      "Epoch 3/200\n",
      "246/246 [==============================] - 1s - loss: 1.1221 - acc: 0.4837\n",
      "Epoch 4/200\n",
      "246/246 [==============================] - 1s - loss: 0.8071 - acc: 0.4837\n",
      "Epoch 5/200\n",
      "246/246 [==============================] - 1s - loss: 0.7744 - acc: 0.5163\n",
      "Epoch 6/200\n",
      "246/246 [==============================] - 1s - loss: 0.8273 - acc: 0.5163\n",
      "Epoch 7/200\n",
      "246/246 [==============================] - 1s - loss: 0.6931 - acc: 0.5163\n",
      "Epoch 8/200\n",
      "246/246 [==============================] - 1s - loss: 0.7421 - acc: 0.4837\n",
      "Epoch 9/200\n",
      "246/246 [==============================] - 1s - loss: 0.7641 - acc: 0.4837\n",
      "Epoch 10/200\n",
      "246/246 [==============================] - 1s - loss: 0.6915 - acc: 0.4919\n",
      "Epoch 11/200\n",
      "246/246 [==============================] - 1s - loss: 0.7041 - acc: 0.5163\n",
      "Epoch 12/200\n",
      "246/246 [==============================] - 1s - loss: 0.7397 - acc: 0.5163\n",
      "Epoch 13/200\n",
      "246/246 [==============================] - 1s - loss: 0.7003 - acc: 0.5163\n",
      "Epoch 14/200\n",
      "246/246 [==============================] - 1s - loss: 0.6806 - acc: 0.6098\n",
      "Epoch 15/200\n",
      "246/246 [==============================] - 1s - loss: 0.7114 - acc: 0.4837\n",
      "Epoch 16/200\n",
      "246/246 [==============================] - 1s - loss: 0.7029 - acc: 0.4837\n",
      "Epoch 17/200\n",
      "246/246 [==============================] - 1s - loss: 0.6753 - acc: 0.6341\n",
      "Epoch 18/200\n",
      "246/246 [==============================] - 1s - loss: 0.6904 - acc: 0.5163\n",
      "Epoch 19/200\n",
      "246/246 [==============================] - 1s - loss: 0.6986 - acc: 0.5163\n",
      "Epoch 20/200\n",
      "246/246 [==============================] - 1s - loss: 0.6754 - acc: 0.5163\n",
      "Epoch 21/200\n",
      "246/246 [==============================] - 1s - loss: 0.6754 - acc: 0.5935\n",
      "Epoch 22/200\n",
      "246/246 [==============================] - 1s - loss: 0.6885 - acc: 0.4919\n",
      "Epoch 23/200\n",
      "246/246 [==============================] - 1s - loss: 0.6747 - acc: 0.5813\n",
      "Epoch 24/200\n",
      "246/246 [==============================] - 1s - loss: 0.6671 - acc: 0.5813\n",
      "Epoch 25/200\n",
      "246/246 [==============================] - 1s - loss: 0.6779 - acc: 0.5163\n",
      "Epoch 26/200\n",
      "246/246 [==============================] - 1s - loss: 0.6709 - acc: 0.5163\n",
      "Epoch 27/200\n",
      "246/246 [==============================] - 1s - loss: 0.6621 - acc: 0.6667\n",
      "Epoch 28/200\n",
      "246/246 [==============================] - 1s - loss: 0.6705 - acc: 0.5894\n",
      "Epoch 29/200\n",
      "246/246 [==============================] - 1s - loss: 0.6634 - acc: 0.6341\n",
      "Epoch 30/200\n",
      "246/246 [==============================] - 1s - loss: 0.6584 - acc: 0.6220\n",
      "Epoch 31/200\n",
      "246/246 [==============================] - 1s - loss: 0.6635 - acc: 0.5285\n",
      "Epoch 32/200\n",
      "246/246 [==============================] - 1s - loss: 0.6572 - acc: 0.5772\n",
      "Epoch 33/200\n",
      "246/246 [==============================] - 1s - loss: 0.6540 - acc: 0.6870\n",
      "Epoch 34/200\n",
      "246/246 [==============================] - 1s - loss: 0.6578 - acc: 0.6423\n",
      "Epoch 35/200\n",
      "246/246 [==============================] - 1s - loss: 0.6523 - acc: 0.6748\n",
      "Epoch 36/200\n",
      "246/246 [==============================] - 1s - loss: 0.6505 - acc: 0.6463\n",
      "Epoch 37/200\n",
      "246/246 [==============================] - 1s - loss: 0.6524 - acc: 0.5813\n",
      "Epoch 38/200\n",
      "246/246 [==============================] - 1s - loss: 0.6473 - acc: 0.6667\n",
      "Epoch 39/200\n",
      "246/246 [==============================] - 1s - loss: 0.6473 - acc: 0.6707\n",
      "Epoch 40/200\n",
      "246/246 [==============================] - 1s - loss: 0.6473 - acc: 0.6748\n",
      "Epoch 41/200\n",
      "246/246 [==============================] - 1s - loss: 0.6428 - acc: 0.6748\n",
      "Epoch 42/200\n",
      "246/246 [==============================] - 1s - loss: 0.6440 - acc: 0.6301\n",
      "Epoch 43/200\n",
      "246/246 [==============================] - 1s - loss: 0.6417 - acc: 0.6585\n",
      "Epoch 44/200\n",
      "246/246 [==============================] - 1s - loss: 0.6393 - acc: 0.6951\n",
      "Epoch 45/200\n",
      "246/246 [==============================] - 1s - loss: 0.6399 - acc: 0.6829\n",
      "Epoch 46/200\n",
      "246/246 [==============================] - 1s - loss: 0.6366 - acc: 0.6951\n",
      "Epoch 47/200\n",
      "246/246 [==============================] - 1s - loss: 0.6362 - acc: 0.6748\n",
      "Epoch 48/200\n",
      "246/246 [==============================] - 1s - loss: 0.6350 - acc: 0.6748\n",
      "Epoch 49/200\n",
      "246/246 [==============================] - 1s - loss: 0.6326 - acc: 0.6829\n",
      "Epoch 50/200\n",
      "246/246 [==============================] - 1s - loss: 0.6326 - acc: 0.6911\n",
      "Epoch 51/200\n",
      "246/246 [==============================] - 1s - loss: 0.6303 - acc: 0.6992\n",
      "Epoch 52/200\n",
      "246/246 [==============================] - 1s - loss: 0.6294 - acc: 0.6789\n",
      "Epoch 53/200\n",
      "246/246 [==============================] - 1s - loss: 0.6283 - acc: 0.6829\n",
      "Epoch 54/200\n",
      "246/246 [==============================] - 1s - loss: 0.6264 - acc: 0.6992\n",
      "Epoch 55/200\n",
      "246/246 [==============================] - 1s - loss: 0.6260 - acc: 0.7033\n",
      "Epoch 56/200\n",
      "246/246 [==============================] - 1s - loss: 0.6241 - acc: 0.7033\n",
      "Epoch 57/200\n",
      "246/246 [==============================] - 1s - loss: 0.6232 - acc: 0.6870\n",
      "Epoch 58/200\n",
      "246/246 [==============================] - 1s - loss: 0.6220 - acc: 0.7033\n",
      "Epoch 59/200\n",
      "246/246 [==============================] - 1s - loss: 0.6205 - acc: 0.7033\n",
      "Epoch 60/200\n",
      "246/246 [==============================] - 1s - loss: 0.6197 - acc: 0.7073\n",
      "Epoch 61/200\n",
      "246/246 [==============================] - 1s - loss: 0.6181 - acc: 0.7114\n",
      "Epoch 62/200\n",
      "246/246 [==============================] - 1s - loss: 0.6173 - acc: 0.7033\n",
      "Epoch 63/200\n",
      "246/246 [==============================] - 1s - loss: 0.6159 - acc: 0.7114\n",
      "Epoch 64/200\n",
      "246/246 [==============================] - 1s - loss: 0.6148 - acc: 0.7114\n",
      "Epoch 65/200\n",
      "246/246 [==============================] - 1s - loss: 0.6137 - acc: 0.7154\n",
      "Epoch 66/200\n",
      "246/246 [==============================] - 1s - loss: 0.6124 - acc: 0.7195\n",
      "Epoch 67/200\n",
      "246/246 [==============================] - 1s - loss: 0.6115 - acc: 0.7033\n",
      "Epoch 68/200\n",
      "246/246 [==============================] - 1s - loss: 0.6101 - acc: 0.7195\n",
      "Epoch 69/200\n",
      "246/246 [==============================] - 1s - loss: 0.6091 - acc: 0.7154\n",
      "Epoch 70/200\n",
      "246/246 [==============================] - 1s - loss: 0.6078 - acc: 0.7195\n",
      "Epoch 71/200\n",
      "246/246 [==============================] - 1s - loss: 0.6068 - acc: 0.7195\n",
      "Epoch 72/200\n",
      "246/246 [==============================] - 1s - loss: 0.6057 - acc: 0.7276\n",
      "Epoch 73/200\n",
      "246/246 [==============================] - 1s - loss: 0.6045 - acc: 0.7195\n",
      "Epoch 74/200\n",
      "246/246 [==============================] - 1s - loss: 0.6035 - acc: 0.7236\n",
      "Epoch 75/200\n",
      "246/246 [==============================] - 1s - loss: 0.6023 - acc: 0.7358\n",
      "Epoch 76/200\n",
      "246/246 [==============================] - 1s - loss: 0.6013 - acc: 0.7276\n",
      "Epoch 77/200\n",
      "246/246 [==============================] - 1s - loss: 0.6003 - acc: 0.7358\n",
      "Epoch 78/200\n",
      "246/246 [==============================] - 1s - loss: 0.5991 - acc: 0.7317\n",
      "Epoch 79/200\n",
      "246/246 [==============================] - 1s - loss: 0.5980 - acc: 0.7276\n",
      "Epoch 80/200\n",
      "246/246 [==============================] - 1s - loss: 0.5971 - acc: 0.7276\n",
      "Epoch 81/200\n",
      "246/246 [==============================] - 1s - loss: 0.5959 - acc: 0.7398\n",
      "Epoch 82/200\n",
      "246/246 [==============================] - 1s - loss: 0.5950 - acc: 0.7398\n",
      "Epoch 83/200\n",
      "246/246 [==============================] - 1s - loss: 0.5940 - acc: 0.7358\n",
      "Epoch 84/200\n",
      "246/246 [==============================] - 1s - loss: 0.5929 - acc: 0.7398\n",
      "Epoch 85/200\n",
      "246/246 [==============================] - 1s - loss: 0.5918 - acc: 0.7398\n",
      "Epoch 86/200\n",
      "246/246 [==============================] - 1s - loss: 0.5908 - acc: 0.7439\n",
      "Epoch 87/200\n",
      "246/246 [==============================] - 1s - loss: 0.5899 - acc: 0.7439\n",
      "Epoch 88/200\n",
      "246/246 [==============================] - 1s - loss: 0.5889 - acc: 0.7398\n",
      "Epoch 89/200\n",
      "246/246 [==============================] - 1s - loss: 0.5879 - acc: 0.7398\n",
      "Epoch 90/200\n",
      "246/246 [==============================] - 1s - loss: 0.5869 - acc: 0.7398\n",
      "Epoch 91/200\n",
      "246/246 [==============================] - 1s - loss: 0.5859 - acc: 0.7398\n",
      "Epoch 92/200\n",
      "246/246 [==============================] - 1s - loss: 0.5848 - acc: 0.7439\n",
      "Epoch 93/200\n",
      "246/246 [==============================] - 1s - loss: 0.5839 - acc: 0.7398\n",
      "Epoch 94/200\n",
      "246/246 [==============================] - 1s - loss: 0.5828 - acc: 0.7398\n",
      "Epoch 95/200\n",
      "246/246 [==============================] - 1s - loss: 0.5818 - acc: 0.7439\n",
      "Epoch 96/200\n",
      "246/246 [==============================] - 1s - loss: 0.5808 - acc: 0.7480\n",
      "Epoch 97/200\n",
      "246/246 [==============================] - 1s - loss: 0.5799 - acc: 0.7480\n",
      "Epoch 98/200\n",
      "246/246 [==============================] - 1s - loss: 0.5789 - acc: 0.7439\n",
      "Epoch 99/200\n",
      "246/246 [==============================] - 1s - loss: 0.5779 - acc: 0.7439\n",
      "Epoch 100/200\n",
      "246/246 [==============================] - 1s - loss: 0.5769 - acc: 0.7520\n",
      "Epoch 101/200\n",
      "246/246 [==============================] - 1s - loss: 0.5759 - acc: 0.7520\n",
      "Epoch 102/200\n",
      "246/246 [==============================] - 1s - loss: 0.5747 - acc: 0.7439\n",
      "Epoch 103/200\n",
      "246/246 [==============================] - 1s - loss: 0.5737 - acc: 0.7439\n",
      "Epoch 104/200\n",
      "246/246 [==============================] - 1s - loss: 0.5727 - acc: 0.7480\n",
      "Epoch 105/200\n",
      "246/246 [==============================] - 1s - loss: 0.5716 - acc: 0.7520\n",
      "Epoch 106/200\n",
      "246/246 [==============================] - 1s - loss: 0.5706 - acc: 0.7439\n",
      "Epoch 107/200\n",
      "246/246 [==============================] - 1s - loss: 0.5696 - acc: 0.7520\n",
      "Epoch 108/200\n",
      "246/246 [==============================] - 1s - loss: 0.5686 - acc: 0.7520\n",
      "Epoch 109/200\n",
      "246/246 [==============================] - 1s - loss: 0.5676 - acc: 0.7480\n",
      "Epoch 110/200\n",
      "246/246 [==============================] - 1s - loss: 0.5666 - acc: 0.7480\n",
      "Epoch 111/200\n",
      "246/246 [==============================] - 1s - loss: 0.5655 - acc: 0.7520\n",
      "Epoch 112/200\n",
      "246/246 [==============================] - 1s - loss: 0.5645 - acc: 0.7520\n",
      "Epoch 113/200\n",
      "246/246 [==============================] - 1s - loss: 0.5635 - acc: 0.7480\n",
      "Epoch 114/200\n",
      "246/246 [==============================] - 1s - loss: 0.5624 - acc: 0.7520\n",
      "Epoch 115/200\n",
      "246/246 [==============================] - 1s - loss: 0.5614 - acc: 0.7520\n",
      "Epoch 116/200\n",
      "246/246 [==============================] - 1s - loss: 0.5604 - acc: 0.7480\n",
      "Epoch 117/200\n",
      "246/246 [==============================] - 1s - loss: 0.5593 - acc: 0.7480\n",
      "Epoch 118/200\n",
      "246/246 [==============================] - 1s - loss: 0.5583 - acc: 0.7520\n",
      "Epoch 119/200\n",
      "246/246 [==============================] - 1s - loss: 0.5573 - acc: 0.7480\n",
      "Epoch 120/200\n",
      "246/246 [==============================] - 1s - loss: 0.5562 - acc: 0.7520\n",
      "Epoch 121/200\n",
      "246/246 [==============================] - 1s - loss: 0.5552 - acc: 0.7520\n",
      "Epoch 122/200\n",
      "246/246 [==============================] - 1s - loss: 0.5541 - acc: 0.7480\n",
      "Epoch 123/200\n",
      "246/246 [==============================] - 1s - loss: 0.5531 - acc: 0.7520\n",
      "Epoch 124/200\n",
      "246/246 [==============================] - 1s - loss: 0.5520 - acc: 0.7561\n",
      "Epoch 125/200\n",
      "246/246 [==============================] - 1s - loss: 0.5510 - acc: 0.7561\n",
      "Epoch 126/200\n",
      "246/246 [==============================] - 1s - loss: 0.5500 - acc: 0.7520\n",
      "Epoch 127/200\n",
      "246/246 [==============================] - 1s - loss: 0.5489 - acc: 0.7561\n",
      "Epoch 128/200\n",
      "246/246 [==============================] - 1s - loss: 0.5478 - acc: 0.7561\n",
      "Epoch 129/200\n",
      "246/246 [==============================] - 1s - loss: 0.5468 - acc: 0.7561\n",
      "Epoch 130/200\n",
      "246/246 [==============================] - 1s - loss: 0.5457 - acc: 0.7561\n",
      "Epoch 131/200\n",
      "246/246 [==============================] - 1s - loss: 0.5446 - acc: 0.7561\n",
      "Epoch 132/200\n",
      "246/246 [==============================] - 1s - loss: 0.5436 - acc: 0.7561\n",
      "Epoch 133/200\n",
      "246/246 [==============================] - 1s - loss: 0.5425 - acc: 0.7561\n",
      "Epoch 134/200\n",
      "246/246 [==============================] - 1s - loss: 0.5414 - acc: 0.7561\n",
      "Epoch 135/200\n",
      "246/246 [==============================] - 1s - loss: 0.5403 - acc: 0.7642\n",
      "Epoch 136/200\n",
      "246/246 [==============================] - 1s - loss: 0.5393 - acc: 0.7642\n",
      "Epoch 137/200\n",
      "246/246 [==============================] - 1s - loss: 0.5382 - acc: 0.7642\n",
      "Epoch 138/200\n",
      "246/246 [==============================] - 1s - loss: 0.5371 - acc: 0.7642\n",
      "Epoch 139/200\n",
      "246/246 [==============================] - 1s - loss: 0.5360 - acc: 0.7642\n",
      "Epoch 140/200\n",
      "246/246 [==============================] - 1s - loss: 0.5349 - acc: 0.7642\n",
      "Epoch 141/200\n",
      "246/246 [==============================] - 1s - loss: 0.5338 - acc: 0.7642\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7873f01beb97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#lr=0.00004 # Learning rate was 0.00004 in my one working keras code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00004\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_exprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_Y_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_exprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# I think encoded_Y_train should not be passed as it is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#lr=0.00004 # Learning rate was 0.00004 in my one working keras code\n",
    "model.compile(Adam(lr=0.00004), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_exprs, [[y] for y in encoded_Y_train], batch_size=train_exprs.shape[0], epochs=200)\n",
    "# I think encoded_Y_train should not be passed as it is\n",
    "\n",
    "#model.fit(train_exprs, encoded_Y_train, validation_data = (test_exprs, test_Y), batch_size=train_exprs.shape[0], epochs=30)\n",
    "# I was getting problems from train_exprs being a pandas object. Probably can learn how to change that closer to \n",
    "\n",
    "\n",
    "#da_dis_model = Sequential(get_my_layers(p))\n",
    "#da_dis_model.compile(optimizer=Adam(lr=0.001),\n",
    "#             loss=\"categorical_crossentropy\",\n",
    "#             metrics=['accuracy'])\n",
    "\n",
    "#da_dis_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    " #                   validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 75% accuracy could have been a fluke, though I think that is somewhat unlikely. It is clear that there is a ton of noise just in training the model. I am reaching different results each time I train. This second training below with the validation score included plataeus at 67% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246 samples, validate on 48 samples\n",
      "Epoch 1/200\n",
      "246/246 [==============================] - 1s - loss: 1.6745 - acc: 0.4837 - val_loss: 5.9497 - val_acc: 0.5000\n",
      "Epoch 2/200\n",
      "246/246 [==============================] - 1s - loss: 5.7946 - acc: 0.5163 - val_loss: 5.3463 - val_acc: 0.5000\n",
      "Epoch 3/200\n",
      "246/246 [==============================] - 1s - loss: 5.5860 - acc: 0.4837 - val_loss: 1.2175 - val_acc: 0.5000\n",
      "Epoch 4/200\n",
      "246/246 [==============================] - 1s - loss: 1.2603 - acc: 0.4837 - val_loss: 4.5171 - val_acc: 0.5000\n",
      "Epoch 5/200\n",
      "246/246 [==============================] - 1s - loss: 4.3789 - acc: 0.5163 - val_loss: 4.5176 - val_acc: 0.5000\n",
      "Epoch 6/200\n",
      "246/246 [==============================] - 1s - loss: 4.3989 - acc: 0.5163 - val_loss: 1.8966 - val_acc: 0.5000\n",
      "Epoch 7/200\n",
      "246/246 [==============================] - 1s - loss: 1.8678 - acc: 0.5163 - val_loss: 2.0570 - val_acc: 0.5000\n",
      "Epoch 8/200\n",
      "246/246 [==============================] - 1s - loss: 2.1277 - acc: 0.4837 - val_loss: 3.0444 - val_acc: 0.5000\n",
      "Epoch 9/200\n",
      "246/246 [==============================] - 1s - loss: 3.1442 - acc: 0.4837 - val_loss: 1.9446 - val_acc: 0.5000\n",
      "Epoch 10/200\n",
      "246/246 [==============================] - 1s - loss: 2.0089 - acc: 0.4837 - val_loss: 0.9070 - val_acc: 0.5000\n",
      "Epoch 11/200\n",
      "246/246 [==============================] - 1s - loss: 0.9006 - acc: 0.5163 - val_loss: 2.0072 - val_acc: 0.5000\n",
      "Epoch 12/200\n",
      "246/246 [==============================] - 1s - loss: 1.9601 - acc: 0.5163 - val_loss: 1.5533 - val_acc: 0.5000\n",
      "Epoch 13/200\n",
      "246/246 [==============================] - 1s - loss: 1.5165 - acc: 0.5163 - val_loss: 0.6857 - val_acc: 0.5000\n",
      "Epoch 14/200\n",
      "246/246 [==============================] - 1s - loss: 0.6973 - acc: 0.5407 - val_loss: 1.4346 - val_acc: 0.5000\n",
      "Epoch 15/200\n",
      "246/246 [==============================] - 1s - loss: 1.4701 - acc: 0.4837 - val_loss: 1.1669 - val_acc: 0.5000\n",
      "Epoch 16/200\n",
      "246/246 [==============================] - 1s - loss: 1.1908 - acc: 0.4837 - val_loss: 0.7002 - val_acc: 0.5000\n",
      "Epoch 17/200\n",
      "246/246 [==============================] - 1s - loss: 0.7007 - acc: 0.5244 - val_loss: 1.2417 - val_acc: 0.5000\n",
      "Epoch 18/200\n",
      "246/246 [==============================] - 1s - loss: 1.2194 - acc: 0.5163 - val_loss: 0.9841 - val_acc: 0.5000\n",
      "Epoch 19/200\n",
      "246/246 [==============================] - 1s - loss: 0.9700 - acc: 0.5163 - val_loss: 0.7037 - val_acc: 0.5208\n",
      "Epoch 20/200\n",
      "246/246 [==============================] - 1s - loss: 0.7084 - acc: 0.5285 - val_loss: 1.0482 - val_acc: 0.5000\n",
      "Epoch 21/200\n",
      "246/246 [==============================] - 1s - loss: 1.0626 - acc: 0.4837 - val_loss: 0.7631 - val_acc: 0.5000\n",
      "Epoch 22/200\n",
      "246/246 [==============================] - 1s - loss: 0.7717 - acc: 0.4878 - val_loss: 0.7749 - val_acc: 0.5000\n",
      "Epoch 23/200\n",
      "246/246 [==============================] - 1s - loss: 0.7720 - acc: 0.5163 - val_loss: 0.9619 - val_acc: 0.5000\n",
      "Epoch 24/200\n",
      "246/246 [==============================] - 1s - loss: 0.9509 - acc: 0.5163 - val_loss: 0.6681 - val_acc: 0.5208\n",
      "Epoch 25/200\n",
      "246/246 [==============================] - 1s - loss: 0.6686 - acc: 0.5447 - val_loss: 0.7963 - val_acc: 0.5000\n",
      "Epoch 26/200\n",
      "246/246 [==============================] - 1s - loss: 0.8072 - acc: 0.4878 - val_loss: 0.8236 - val_acc: 0.5000\n",
      "Epoch 27/200\n",
      "246/246 [==============================] - 1s - loss: 0.8358 - acc: 0.4878 - val_loss: 0.6305 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      "246/246 [==============================] - 1s - loss: 0.6348 - acc: 0.6504 - val_loss: 0.8196 - val_acc: 0.5000\n",
      "Epoch 29/200\n",
      "246/246 [==============================] - 1s - loss: 0.8133 - acc: 0.5163 - val_loss: 0.7175 - val_acc: 0.5000\n",
      "Epoch 30/200\n",
      "246/246 [==============================] - 1s - loss: 0.7162 - acc: 0.5285 - val_loss: 0.6531 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "246/246 [==============================] - 1s - loss: 0.6654 - acc: 0.5935 - val_loss: 0.7548 - val_acc: 0.5000\n",
      "Epoch 32/200\n",
      "246/246 [==============================] - 1s - loss: 0.7717 - acc: 0.5000 - val_loss: 0.6217 - val_acc: 0.6875\n",
      "Epoch 33/200\n",
      "246/246 [==============================] - 1s - loss: 0.6325 - acc: 0.6545 - val_loss: 0.7199 - val_acc: 0.5000\n",
      "Epoch 34/200\n",
      "246/246 [==============================] - 1s - loss: 0.7168 - acc: 0.5244 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 35/200\n",
      "246/246 [==============================] - 1s - loss: 0.6914 - acc: 0.5325 - val_loss: 0.6238 - val_acc: 0.6875\n",
      "Epoch 36/200\n",
      "246/246 [==============================] - 1s - loss: 0.6363 - acc: 0.6585 - val_loss: 0.6934 - val_acc: 0.5417\n",
      "Epoch 37/200\n",
      "246/246 [==============================] - 1s - loss: 0.7112 - acc: 0.5732 - val_loss: 0.6114 - val_acc: 0.6458\n",
      "Epoch 38/200\n",
      "246/246 [==============================] - 1s - loss: 0.6222 - acc: 0.6626 - val_loss: 0.6781 - val_acc: 0.5417\n",
      "Epoch 39/200\n",
      "246/246 [==============================] - 1s - loss: 0.6761 - acc: 0.5447 - val_loss: 0.6513 - val_acc: 0.5833\n",
      "Epoch 40/200\n",
      "246/246 [==============================] - 1s - loss: 0.6515 - acc: 0.5569 - val_loss: 0.6165 - val_acc: 0.7083\n",
      "Epoch 41/200\n",
      "246/246 [==============================] - 1s - loss: 0.6294 - acc: 0.6626 - val_loss: 0.6499 - val_acc: 0.6042\n",
      "Epoch 42/200\n",
      "246/246 [==============================] - 1s - loss: 0.6662 - acc: 0.6057 - val_loss: 0.6033 - val_acc: 0.6875\n",
      "Epoch 43/200\n",
      "246/246 [==============================] - 1s - loss: 0.6111 - acc: 0.6707 - val_loss: 0.6560 - val_acc: 0.5625\n",
      "Epoch 44/200\n",
      "246/246 [==============================] - 1s - loss: 0.6536 - acc: 0.5528 - val_loss: 0.6166 - val_acc: 0.6667\n",
      "Epoch 45/200\n",
      "246/246 [==============================] - 1s - loss: 0.6189 - acc: 0.6504 - val_loss: 0.6160 - val_acc: 0.6875\n",
      "Epoch 46/200\n",
      "246/246 [==============================] - 1s - loss: 0.6281 - acc: 0.6463 - val_loss: 0.6178 - val_acc: 0.6875\n",
      "Epoch 47/200\n",
      "246/246 [==============================] - 1s - loss: 0.6296 - acc: 0.6423 - val_loss: 0.6060 - val_acc: 0.7083\n",
      "Epoch 48/200\n",
      "246/246 [==============================] - 1s - loss: 0.6080 - acc: 0.6748 - val_loss: 0.6341 - val_acc: 0.5417\n",
      "Epoch 49/200\n",
      "246/246 [==============================] - 1s - loss: 0.6310 - acc: 0.5772 - val_loss: 0.5982 - val_acc: 0.6458\n",
      "Epoch 50/200\n",
      "246/246 [==============================] - 1s - loss: 0.6016 - acc: 0.6789 - val_loss: 0.6125 - val_acc: 0.6875\n",
      "Epoch 51/200\n",
      "246/246 [==============================] - 1s - loss: 0.6217 - acc: 0.6504 - val_loss: 0.5990 - val_acc: 0.6458\n",
      "Epoch 52/200\n",
      "246/246 [==============================] - 1s - loss: 0.6044 - acc: 0.6829 - val_loss: 0.6133 - val_acc: 0.6875\n",
      "Epoch 53/200\n",
      "246/246 [==============================] - 1s - loss: 0.6096 - acc: 0.6545 - val_loss: 0.6115 - val_acc: 0.6875\n",
      "Epoch 54/200\n",
      "246/246 [==============================] - 1s - loss: 0.6077 - acc: 0.6667 - val_loss: 0.5960 - val_acc: 0.6458\n",
      "Epoch 55/200\n",
      "246/246 [==============================] - 1s - loss: 0.5999 - acc: 0.6748 - val_loss: 0.6019 - val_acc: 0.7083\n",
      "Epoch 56/200\n",
      "246/246 [==============================] - 1s - loss: 0.6078 - acc: 0.6707 - val_loss: 0.5950 - val_acc: 0.6458\n",
      "Epoch 57/200\n",
      "246/246 [==============================] - 1s - loss: 0.5944 - acc: 0.6870 - val_loss: 0.6103 - val_acc: 0.6875\n",
      "Epoch 58/200\n",
      "246/246 [==============================] - 1s - loss: 0.6048 - acc: 0.6667 - val_loss: 0.5930 - val_acc: 0.6458\n",
      "Epoch 59/200\n",
      "246/246 [==============================] - 1s - loss: 0.5921 - acc: 0.6870 - val_loss: 0.5960 - val_acc: 0.6667\n",
      "Epoch 60/200\n",
      "246/246 [==============================] - 1s - loss: 0.5999 - acc: 0.6789 - val_loss: 0.5903 - val_acc: 0.6667\n",
      "Epoch 61/200\n",
      "246/246 [==============================] - 1s - loss: 0.5912 - acc: 0.7033 - val_loss: 0.6001 - val_acc: 0.6875\n",
      "Epoch 62/200\n",
      "246/246 [==============================] - 1s - loss: 0.5948 - acc: 0.7073 - val_loss: 0.5947 - val_acc: 0.7083\n",
      "Epoch 63/200\n",
      "246/246 [==============================] - 1s - loss: 0.5905 - acc: 0.6951 - val_loss: 0.5892 - val_acc: 0.6250\n",
      "Epoch 64/200\n",
      "246/246 [==============================] - 1s - loss: 0.5903 - acc: 0.6870 - val_loss: 0.5885 - val_acc: 0.6250\n",
      "Epoch 65/200\n",
      "246/246 [==============================] - 1s - loss: 0.5893 - acc: 0.6870 - val_loss: 0.5913 - val_acc: 0.6875\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s - loss: 0.5866 - acc: 0.7033 - val_loss: 0.5936 - val_acc: 0.7083\n",
      "Epoch 67/200\n",
      "246/246 [==============================] - 1s - loss: 0.5875 - acc: 0.6951 - val_loss: 0.5853 - val_acc: 0.6667\n",
      "Epoch 68/200\n",
      "246/246 [==============================] - 1s - loss: 0.5836 - acc: 0.7073 - val_loss: 0.5858 - val_acc: 0.6250\n",
      "Epoch 69/200\n",
      "246/246 [==============================] - 1s - loss: 0.5854 - acc: 0.6951 - val_loss: 0.5858 - val_acc: 0.6458\n",
      "Epoch 70/200\n",
      "246/246 [==============================] - 1s - loss: 0.5811 - acc: 0.7033 - val_loss: 0.5904 - val_acc: 0.7083\n",
      "Epoch 71/200\n",
      "246/246 [==============================] - 1s - loss: 0.5831 - acc: 0.7033 - val_loss: 0.5832 - val_acc: 0.7083\n",
      "Epoch 72/200\n",
      "246/246 [==============================] - 1s - loss: 0.5790 - acc: 0.6992 - val_loss: 0.5829 - val_acc: 0.6458\n",
      "Epoch 73/200\n",
      "246/246 [==============================] - 1s - loss: 0.5806 - acc: 0.7073 - val_loss: 0.5826 - val_acc: 0.7083\n",
      "Epoch 74/200\n",
      "246/246 [==============================] - 1s - loss: 0.5770 - acc: 0.7073 - val_loss: 0.5870 - val_acc: 0.7083\n",
      "Epoch 75/200\n",
      "246/246 [==============================] - 1s - loss: 0.5783 - acc: 0.7114 - val_loss: 0.5817 - val_acc: 0.7083\n",
      "Epoch 76/200\n",
      "246/246 [==============================] - 1s - loss: 0.5751 - acc: 0.7114 - val_loss: 0.5804 - val_acc: 0.6667\n",
      "Epoch 77/200\n",
      "246/246 [==============================] - 1s - loss: 0.5759 - acc: 0.7276 - val_loss: 0.5803 - val_acc: 0.7083\n",
      "Epoch 78/200\n",
      "246/246 [==============================] - 1s - loss: 0.5732 - acc: 0.7073 - val_loss: 0.5838 - val_acc: 0.6667\n",
      "Epoch 79/200\n",
      "246/246 [==============================] - 1s - loss: 0.5737 - acc: 0.7195 - val_loss: 0.5800 - val_acc: 0.6875\n",
      "Epoch 80/200\n",
      "246/246 [==============================] - 1s - loss: 0.5713 - acc: 0.7154 - val_loss: 0.5784 - val_acc: 0.6667\n",
      "Epoch 81/200\n",
      "246/246 [==============================] - 1s - loss: 0.5715 - acc: 0.7236 - val_loss: 0.5785 - val_acc: 0.7083\n",
      "Epoch 82/200\n",
      "246/246 [==============================] - 1s - loss: 0.5694 - acc: 0.7114 - val_loss: 0.5813 - val_acc: 0.6250\n",
      "Epoch 83/200\n",
      "246/246 [==============================] - 1s - loss: 0.5694 - acc: 0.7276 - val_loss: 0.5782 - val_acc: 0.6875\n",
      "Epoch 84/200\n",
      "246/246 [==============================] - 1s - loss: 0.5675 - acc: 0.7154 - val_loss: 0.5767 - val_acc: 0.6875\n",
      "Epoch 85/200\n",
      "246/246 [==============================] - 1s - loss: 0.5673 - acc: 0.7236 - val_loss: 0.5769 - val_acc: 0.7083\n",
      "Epoch 86/200\n",
      "246/246 [==============================] - 1s - loss: 0.5656 - acc: 0.7154 - val_loss: 0.5789 - val_acc: 0.6458\n",
      "Epoch 87/200\n",
      "246/246 [==============================] - 1s - loss: 0.5653 - acc: 0.7276 - val_loss: 0.5762 - val_acc: 0.6875\n",
      "Epoch 88/200\n",
      "246/246 [==============================] - 1s - loss: 0.5637 - acc: 0.7154 - val_loss: 0.5748 - val_acc: 0.6875\n",
      "Epoch 89/200\n",
      "246/246 [==============================] - 1s - loss: 0.5633 - acc: 0.7276 - val_loss: 0.5754 - val_acc: 0.6875\n",
      "Epoch 90/200\n",
      "246/246 [==============================] - 1s - loss: 0.5618 - acc: 0.7154 - val_loss: 0.5767 - val_acc: 0.6667\n",
      "Epoch 91/200\n",
      "246/246 [==============================] - 1s - loss: 0.5613 - acc: 0.7317 - val_loss: 0.5744 - val_acc: 0.6875\n",
      "Epoch 92/200\n",
      "246/246 [==============================] - 1s - loss: 0.5600 - acc: 0.7195 - val_loss: 0.5734 - val_acc: 0.6875\n",
      "Epoch 93/200\n",
      "246/246 [==============================] - 1s - loss: 0.5594 - acc: 0.7317 - val_loss: 0.5742 - val_acc: 0.6875\n",
      "Epoch 94/200\n",
      "246/246 [==============================] - 1s - loss: 0.5581 - acc: 0.7195 - val_loss: 0.5748 - val_acc: 0.6667\n",
      "Epoch 95/200\n",
      "246/246 [==============================] - 1s - loss: 0.5575 - acc: 0.7317 - val_loss: 0.5727 - val_acc: 0.7083\n",
      "Epoch 96/200\n",
      "246/246 [==============================] - 1s - loss: 0.5563 - acc: 0.7195 - val_loss: 0.5720 - val_acc: 0.7083\n",
      "Epoch 97/200\n",
      "246/246 [==============================] - 1s - loss: 0.5555 - acc: 0.7276 - val_loss: 0.5730 - val_acc: 0.6667\n",
      "Epoch 98/200\n",
      "246/246 [==============================] - 1s - loss: 0.5545 - acc: 0.7276 - val_loss: 0.5729 - val_acc: 0.6667\n",
      "Epoch 99/200\n",
      "246/246 [==============================] - 1s - loss: 0.5536 - acc: 0.7236 - val_loss: 0.5712 - val_acc: 0.7083\n",
      "Epoch 100/200\n",
      "246/246 [==============================] - 1s - loss: 0.5526 - acc: 0.7276 - val_loss: 0.5709 - val_acc: 0.7083\n",
      "Epoch 101/200\n",
      "246/246 [==============================] - 1s - loss: 0.5517 - acc: 0.7276 - val_loss: 0.5719 - val_acc: 0.6667\n",
      "Epoch 102/200\n",
      "246/246 [==============================] - 1s - loss: 0.5508 - acc: 0.7236 - val_loss: 0.5711 - val_acc: 0.6667\n",
      "Epoch 103/200\n",
      "246/246 [==============================] - 1s - loss: 0.5498 - acc: 0.7317 - val_loss: 0.5698 - val_acc: 0.7083\n",
      "Epoch 104/200\n",
      "246/246 [==============================] - 1s - loss: 0.5489 - acc: 0.7276 - val_loss: 0.5699 - val_acc: 0.6875\n",
      "Epoch 105/200\n",
      "246/246 [==============================] - 1s - loss: 0.5479 - acc: 0.7317 - val_loss: 0.5706 - val_acc: 0.6667\n",
      "Epoch 106/200\n",
      "246/246 [==============================] - 1s - loss: 0.5471 - acc: 0.7317 - val_loss: 0.5694 - val_acc: 0.6667\n",
      "Epoch 107/200\n",
      "246/246 [==============================] - 1s - loss: 0.5460 - acc: 0.7317 - val_loss: 0.5686 - val_acc: 0.6875\n",
      "Epoch 108/200\n",
      "246/246 [==============================] - 1s - loss: 0.5452 - acc: 0.7317 - val_loss: 0.5691 - val_acc: 0.6667\n",
      "Epoch 109/200\n",
      "246/246 [==============================] - 1s - loss: 0.5442 - acc: 0.7398 - val_loss: 0.5692 - val_acc: 0.6667\n",
      "Epoch 110/200\n",
      "246/246 [==============================] - 1s - loss: 0.5433 - acc: 0.7398 - val_loss: 0.5681 - val_acc: 0.6875\n",
      "Epoch 111/200\n",
      "246/246 [==============================] - 1s - loss: 0.5423 - acc: 0.7276 - val_loss: 0.5678 - val_acc: 0.6875\n",
      "Epoch 112/200\n",
      "246/246 [==============================] - 1s - loss: 0.5414 - acc: 0.7317 - val_loss: 0.5683 - val_acc: 0.6667\n",
      "Epoch 113/200\n",
      "246/246 [==============================] - 1s - loss: 0.5404 - acc: 0.7398 - val_loss: 0.5678 - val_acc: 0.6667\n",
      "Epoch 114/200\n",
      "246/246 [==============================] - 1s - loss: 0.5395 - acc: 0.7398 - val_loss: 0.5669 - val_acc: 0.6875\n",
      "Epoch 115/200\n",
      "246/246 [==============================] - 1s - loss: 0.5385 - acc: 0.7317 - val_loss: 0.5670 - val_acc: 0.6667\n",
      "Epoch 116/200\n",
      "246/246 [==============================] - 1s - loss: 0.5376 - acc: 0.7398 - val_loss: 0.5673 - val_acc: 0.6667\n",
      "Epoch 117/200\n",
      "246/246 [==============================] - 1s - loss: 0.5367 - acc: 0.7439 - val_loss: 0.5664 - val_acc: 0.6667\n",
      "Epoch 118/200\n",
      "246/246 [==============================] - 1s - loss: 0.5357 - acc: 0.7439 - val_loss: 0.5660 - val_acc: 0.6667\n",
      "Epoch 119/200\n",
      "246/246 [==============================] - 1s - loss: 0.5348 - acc: 0.7561 - val_loss: 0.5663 - val_acc: 0.6667\n",
      "Epoch 120/200\n",
      "246/246 [==============================] - 1s - loss: 0.5338 - acc: 0.7439 - val_loss: 0.5660 - val_acc: 0.6667\n",
      "Epoch 121/200\n",
      "246/246 [==============================] - 1s - loss: 0.5329 - acc: 0.7480 - val_loss: 0.5652 - val_acc: 0.6667\n",
      "Epoch 122/200\n",
      "246/246 [==============================] - 1s - loss: 0.5320 - acc: 0.7561 - val_loss: 0.5652 - val_acc: 0.6667\n",
      "Epoch 123/200\n",
      "246/246 [==============================] - 1s - loss: 0.5311 - acc: 0.7520 - val_loss: 0.5653 - val_acc: 0.6667\n",
      "Epoch 124/200\n",
      "246/246 [==============================] - 1s - loss: 0.5302 - acc: 0.7561 - val_loss: 0.5646 - val_acc: 0.6667\n",
      "Epoch 125/200\n",
      "246/246 [==============================] - 1s - loss: 0.5292 - acc: 0.7561 - val_loss: 0.5642 - val_acc: 0.6667\n",
      "Epoch 126/200\n",
      "246/246 [==============================] - 1s - loss: 0.5283 - acc: 0.7561 - val_loss: 0.5644 - val_acc: 0.6667\n",
      "Epoch 127/200\n",
      "246/246 [==============================] - 1s - loss: 0.5274 - acc: 0.7602 - val_loss: 0.5641 - val_acc: 0.6667\n",
      "Epoch 128/200\n",
      "246/246 [==============================] - 1s - loss: 0.5264 - acc: 0.7602 - val_loss: 0.5634 - val_acc: 0.6667\n",
      "Epoch 129/200\n",
      "246/246 [==============================] - 1s - loss: 0.5255 - acc: 0.7561 - val_loss: 0.5634 - val_acc: 0.6667\n",
      "Epoch 130/200\n",
      "246/246 [==============================] - 1s - loss: 0.5246 - acc: 0.7642 - val_loss: 0.5634 - val_acc: 0.6667\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s - loss: 0.5237 - acc: 0.7683 - val_loss: 0.5628 - val_acc: 0.6667\n",
      "Epoch 132/200\n",
      "246/246 [==============================] - 1s - loss: 0.5228 - acc: 0.7683 - val_loss: 0.5625 - val_acc: 0.6667\n",
      "Epoch 133/200\n",
      "246/246 [==============================] - 1s - loss: 0.5218 - acc: 0.7683 - val_loss: 0.5626 - val_acc: 0.6667\n",
      "Epoch 134/200\n",
      "246/246 [==============================] - 1s - loss: 0.5209 - acc: 0.7724 - val_loss: 0.5622 - val_acc: 0.6667\n",
      "Epoch 135/200\n",
      "246/246 [==============================] - 1s - loss: 0.5200 - acc: 0.7724 - val_loss: 0.5618 - val_acc: 0.6667\n",
      "Epoch 136/200\n",
      "246/246 [==============================] - 1s - loss: 0.5191 - acc: 0.7764 - val_loss: 0.5618 - val_acc: 0.6667\n",
      "Epoch 137/200\n",
      "246/246 [==============================] - 1s - loss: 0.5182 - acc: 0.7764 - val_loss: 0.5616 - val_acc: 0.6667\n",
      "Epoch 138/200\n",
      "246/246 [==============================] - 1s - loss: 0.5173 - acc: 0.7805 - val_loss: 0.5612 - val_acc: 0.6667\n",
      "Epoch 139/200\n",
      "246/246 [==============================] - 1s - loss: 0.5164 - acc: 0.7805 - val_loss: 0.5610 - val_acc: 0.6667\n",
      "Epoch 140/200\n",
      "246/246 [==============================] - 1s - loss: 0.5154 - acc: 0.7846 - val_loss: 0.5610 - val_acc: 0.6667\n",
      "Epoch 141/200\n",
      "246/246 [==============================] - 1s - loss: 0.5145 - acc: 0.7805 - val_loss: 0.5606 - val_acc: 0.6667\n",
      "Epoch 142/200\n",
      "246/246 [==============================] - 1s - loss: 0.5136 - acc: 0.7846 - val_loss: 0.5603 - val_acc: 0.6667\n",
      "Epoch 143/200\n",
      "246/246 [==============================] - 1s - loss: 0.5127 - acc: 0.7846 - val_loss: 0.5603 - val_acc: 0.6667\n",
      "Epoch 144/200\n",
      "246/246 [==============================] - 1s - loss: 0.5118 - acc: 0.7846 - val_loss: 0.5600 - val_acc: 0.6667\n",
      "Epoch 145/200\n",
      "246/246 [==============================] - 1s - loss: 0.5109 - acc: 0.7886 - val_loss: 0.5596 - val_acc: 0.6667\n",
      "Epoch 146/200\n",
      "246/246 [==============================] - 1s - loss: 0.5100 - acc: 0.7886 - val_loss: 0.5595 - val_acc: 0.6667\n",
      "Epoch 147/200\n",
      "246/246 [==============================] - 1s - loss: 0.5091 - acc: 0.7886 - val_loss: 0.5594 - val_acc: 0.6667\n",
      "Epoch 148/200\n",
      "246/246 [==============================] - 1s - loss: 0.5082 - acc: 0.7846 - val_loss: 0.5590 - val_acc: 0.6667\n",
      "Epoch 149/200\n",
      "246/246 [==============================] - 1s - loss: 0.5072 - acc: 0.7886 - val_loss: 0.5589 - val_acc: 0.6667\n",
      "Epoch 150/200\n",
      "246/246 [==============================] - 1s - loss: 0.5063 - acc: 0.7886 - val_loss: 0.5587 - val_acc: 0.6667\n",
      "Epoch 151/200\n",
      "246/246 [==============================] - 1s - loss: 0.5054 - acc: 0.7886 - val_loss: 0.5584 - val_acc: 0.6667\n",
      "Epoch 152/200\n",
      "246/246 [==============================] - 1s - loss: 0.5045 - acc: 0.7886 - val_loss: 0.5582 - val_acc: 0.6667\n",
      "Epoch 153/200\n",
      "246/246 [==============================] - 1s - loss: 0.5036 - acc: 0.7886 - val_loss: 0.5581 - val_acc: 0.6667\n",
      "Epoch 154/200\n",
      "246/246 [==============================] - 1s - loss: 0.5027 - acc: 0.7886 - val_loss: 0.5578 - val_acc: 0.6667\n",
      "Epoch 155/200\n",
      "246/246 [==============================] - 1s - loss: 0.5018 - acc: 0.7967 - val_loss: 0.5576 - val_acc: 0.6667\n",
      "Epoch 156/200\n",
      "246/246 [==============================] - 1s - loss: 0.5009 - acc: 0.7967 - val_loss: 0.5575 - val_acc: 0.6667\n",
      "Epoch 157/200\n",
      "246/246 [==============================] - 1s - loss: 0.5000 - acc: 0.8008 - val_loss: 0.5572 - val_acc: 0.6667\n",
      "Epoch 158/200\n",
      "246/246 [==============================] - 1s - loss: 0.4990 - acc: 0.8049 - val_loss: 0.5570 - val_acc: 0.6667\n",
      "Epoch 159/200\n",
      "246/246 [==============================] - 1s - loss: 0.4981 - acc: 0.8049 - val_loss: 0.5569 - val_acc: 0.6667\n",
      "Epoch 160/200\n",
      "246/246 [==============================] - 1s - loss: 0.4972 - acc: 0.8049 - val_loss: 0.5567 - val_acc: 0.6667\n",
      "Epoch 161/200\n",
      "246/246 [==============================] - 1s - loss: 0.4963 - acc: 0.8049 - val_loss: 0.5564 - val_acc: 0.6667\n",
      "Epoch 162/200\n",
      "246/246 [==============================] - 1s - loss: 0.4954 - acc: 0.8049 - val_loss: 0.5563 - val_acc: 0.6667\n",
      "Epoch 163/200\n",
      "246/246 [==============================] - 1s - loss: 0.4945 - acc: 0.8049 - val_loss: 0.5561 - val_acc: 0.6667\n",
      "Epoch 164/200\n",
      "246/246 [==============================] - 1s - loss: 0.4936 - acc: 0.8049 - val_loss: 0.5559 - val_acc: 0.6667\n",
      "Epoch 165/200\n",
      "246/246 [==============================] - 1s - loss: 0.4927 - acc: 0.8089 - val_loss: 0.5558 - val_acc: 0.6667\n",
      "Epoch 166/200\n",
      "246/246 [==============================] - 1s - loss: 0.4918 - acc: 0.8089 - val_loss: 0.5556 - val_acc: 0.6667\n",
      "Epoch 167/200\n",
      "246/246 [==============================] - 1s - loss: 0.4909 - acc: 0.8089 - val_loss: 0.5553 - val_acc: 0.6667\n",
      "Epoch 168/200\n",
      "246/246 [==============================] - 1s - loss: 0.4899 - acc: 0.8089 - val_loss: 0.5552 - val_acc: 0.6667\n",
      "Epoch 169/200\n",
      "246/246 [==============================] - 1s - loss: 0.4890 - acc: 0.8089 - val_loss: 0.5550 - val_acc: 0.6667\n",
      "Epoch 170/200\n",
      "246/246 [==============================] - 1s - loss: 0.4881 - acc: 0.8089 - val_loss: 0.5548 - val_acc: 0.6667\n",
      "Epoch 171/200\n",
      "246/246 [==============================] - 1s - loss: 0.4872 - acc: 0.8089 - val_loss: 0.5546 - val_acc: 0.6667\n",
      "Epoch 172/200\n",
      "246/246 [==============================] - 1s - loss: 0.4863 - acc: 0.8089 - val_loss: 0.5544 - val_acc: 0.6667\n",
      "Epoch 173/200\n",
      "246/246 [==============================] - 1s - loss: 0.4854 - acc: 0.8089 - val_loss: 0.5542 - val_acc: 0.6667\n",
      "Epoch 174/200\n",
      "246/246 [==============================] - 1s - loss: 0.4845 - acc: 0.8130 - val_loss: 0.5541 - val_acc: 0.6667\n",
      "Epoch 175/200\n",
      "246/246 [==============================] - 1s - loss: 0.4836 - acc: 0.8130 - val_loss: 0.5539 - val_acc: 0.6667\n",
      "Epoch 176/200\n",
      "246/246 [==============================] - 1s - loss: 0.4827 - acc: 0.8130 - val_loss: 0.5536 - val_acc: 0.6667\n",
      "Epoch 177/200\n",
      "246/246 [==============================] - 1s - loss: 0.4818 - acc: 0.8130 - val_loss: 0.5535 - val_acc: 0.6667\n",
      "Epoch 178/200\n",
      "246/246 [==============================] - 1s - loss: 0.4809 - acc: 0.8130 - val_loss: 0.5533 - val_acc: 0.6667\n",
      "Epoch 179/200\n",
      "246/246 [==============================] - 1s - loss: 0.4799 - acc: 0.8130 - val_loss: 0.5531 - val_acc: 0.6667\n",
      "Epoch 180/200\n",
      "246/246 [==============================] - 1s - loss: 0.4790 - acc: 0.8130 - val_loss: 0.5529 - val_acc: 0.6667\n",
      "Epoch 181/200\n",
      "246/246 [==============================] - 1s - loss: 0.4781 - acc: 0.8130 - val_loss: 0.5528 - val_acc: 0.6667\n",
      "Epoch 182/200\n",
      "246/246 [==============================] - 1s - loss: 0.4772 - acc: 0.8130 - val_loss: 0.5526 - val_acc: 0.6667\n",
      "Epoch 183/200\n",
      "246/246 [==============================] - 1s - loss: 0.4763 - acc: 0.8211 - val_loss: 0.5524 - val_acc: 0.6667\n",
      "Epoch 184/200\n",
      "246/246 [==============================] - 1s - loss: 0.4754 - acc: 0.8211 - val_loss: 0.5522 - val_acc: 0.6667\n",
      "Epoch 185/200\n",
      "246/246 [==============================] - 1s - loss: 0.4745 - acc: 0.8211 - val_loss: 0.5520 - val_acc: 0.6667\n",
      "Epoch 186/200\n",
      "246/246 [==============================] - 1s - loss: 0.4736 - acc: 0.8252 - val_loss: 0.5519 - val_acc: 0.6667\n",
      "Epoch 187/200\n",
      "246/246 [==============================] - 1s - loss: 0.4727 - acc: 0.8252 - val_loss: 0.5517 - val_acc: 0.6667\n",
      "Epoch 188/200\n",
      "246/246 [==============================] - 1s - loss: 0.4718 - acc: 0.8252 - val_loss: 0.5514 - val_acc: 0.6667\n",
      "Epoch 189/200\n",
      "246/246 [==============================] - 1s - loss: 0.4709 - acc: 0.8252 - val_loss: 0.5513 - val_acc: 0.6667\n",
      "Epoch 190/200\n",
      "246/246 [==============================] - 1s - loss: 0.4700 - acc: 0.8252 - val_loss: 0.5512 - val_acc: 0.6667\n",
      "Epoch 191/200\n",
      "246/246 [==============================] - 1s - loss: 0.4691 - acc: 0.8252 - val_loss: 0.5509 - val_acc: 0.6667\n",
      "Epoch 192/200\n",
      "246/246 [==============================] - 1s - loss: 0.4682 - acc: 0.8252 - val_loss: 0.5507 - val_acc: 0.6667\n",
      "Epoch 193/200\n",
      "246/246 [==============================] - 1s - loss: 0.4673 - acc: 0.8252 - val_loss: 0.5506 - val_acc: 0.6667\n",
      "Epoch 194/200\n",
      "246/246 [==============================] - 1s - loss: 0.4663 - acc: 0.8252 - val_loss: 0.5505 - val_acc: 0.6667\n",
      "Epoch 195/200\n",
      "246/246 [==============================] - 1s - loss: 0.4654 - acc: 0.8293 - val_loss: 0.5503 - val_acc: 0.6667\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 1s - loss: 0.4645 - acc: 0.8293 - val_loss: 0.5501 - val_acc: 0.6667\n",
      "Epoch 197/200\n",
      "246/246 [==============================] - 1s - loss: 0.4636 - acc: 0.8374 - val_loss: 0.5499 - val_acc: 0.6667\n",
      "Epoch 198/200\n",
      "246/246 [==============================] - 1s - loss: 0.4627 - acc: 0.8374 - val_loss: 0.5497 - val_acc: 0.6667\n",
      "Epoch 199/200\n",
      "246/246 [==============================] - 1s - loss: 0.4618 - acc: 0.8415 - val_loss: 0.5496 - val_acc: 0.6667\n",
      "Epoch 200/200\n",
      "246/246 [==============================] - 1s - loss: 0.4609 - acc: 0.8415 - val_loss: 0.5494 - val_acc: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4e5e084780>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(lr=0.00004), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_exprs, encoded_Y_train, batch_size=train_exprs.shape[0], epochs=200, validation_data=(test_exprs, encoded_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/246 [==========================>...] - ETA: 0s[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Now what is ground truth for training data\n",
      "[1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0]\n",
      "[[104  15]\n",
      " [ 23 104]]\n",
      "0.845528455285\n",
      "[[14 10]\n",
      " [ 6 18]]\n",
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(train_exprs))\n",
    "# So the dense 2 activation just predicts all one class for the test data\n",
    "#print(model.predict(test_exprs))\n",
    "\n",
    "# It predicts all the same class for both. How can that be?\n",
    "print(\"Now what is ground truth for training data\")\n",
    "print(encoded_Y_train)\n",
    "\n",
    "# I still don't understand what the model is outputing . It doesn't seem that the predictions that I get from model.predict match the labels that I gave, it is not in a strict 0, 1 prediction\n",
    "# Something is definitely messed up. I don't know what it is. But if randomforest can get 70% accuracy, then I've got to be able to get something.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(encoded_Y_train, model.predict_classes(train_exprs, verbose=0)))\n",
    "print(accuracy_score(encoded_Y_train, model.predict_classes(train_exprs, verbose=0)))\n",
    "\n",
    "print(confusion_matrix(encoded_Y_test, model.predict_classes(test_exprs, verbose=0)))\n",
    "print(accuracy_score(encoded_Y_test, model.predict_classes(test_exprs, verbose=0)))\n",
    "\n",
    "# WOW! FIRST TRY! THIS IS 75% accuracy! \n",
    "\n",
    "\n",
    "#sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is definitely  predicting all 0's for the output, so I definitely still have some trouble with how I am giving the data because it says 100% accuracy when it is not in fact 100% accuracy. I need to figure this out. Basically everything I learned today was incorrect because there is a bug in mapping my outputs to inputs. Maybe If i just put the numbers in a list comprehension it will work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "246/246 [==============================] - 1s - loss: 0.2698 - acc: 0.9024\n",
      "Epoch 2/10\n",
      "246/246 [==============================] - 1s - loss: 0.1895 - acc: 0.9512\n",
      "Epoch 3/10\n",
      "246/246 [==============================] - 1s - loss: 0.1540 - acc: 0.9715\n",
      "Epoch 4/10\n",
      "246/246 [==============================] - 1s - loss: 0.1316 - acc: 0.9878\n",
      "Epoch 5/10\n",
      "246/246 [==============================] - 1s - loss: 0.1164 - acc: 0.9959\n",
      "Epoch 6/10\n",
      "246/246 [==============================] - 1s - loss: 0.1053 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "246/246 [==============================] - 1s - loss: 0.0977 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "246/246 [==============================] - 2s - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "246/246 [==============================] - 1s - loss: 0.0855 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "246/246 [==============================] - 1s - loss: 0.0804 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4173b1e828>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_exprs, train_Y, batch_size=train_exprs.shape[0], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what! The following model architecture worked wonderfully to fit. I can fit the training data perfectly. Now we will see if I can fit test data. I can do a first step with a validation split, maybe 80-20, just to see\n",
    "\n",
    "Also, is my batch normalization helping on the training? Before in R I remember training taking forever. Okay, batch normalization in the middle layesr does speed up training a bit, but it still reliably trains. Okay, when I don't do the batch normalization on the INITIAL layer, then my model doesn't go anywhere from the beginning. Then I have to fiddle with the learning rate. Starting off at 1e-6 then going to 0.001 and then to 0.00001 (when 0.001 didn't really budge) went okay. Thus, the initial batchnormalization (i.e. normalization) was HUGELY critical in getting the model to fit easily, and the batch-normalizations in the middle sped up training.\n",
    "\n",
    "ALSO, REMEMBER! IN CROSS VALIDATION, I IDEALLY NEED TO SEPARATE ACCORDING TO MONKEY, NOT JUST RANDOMLY, SO RANDOM IS NOT GOING TO WORK. But we can try anyway\n",
    "\n",
    "Okay, with 80-20 validation split (among samples, not monkeys), I get 60% accuracy on validation, even as the training data is totally fit. Therefore, huge overfitting. Let's add dropout to see what happens.\n",
    "0.8 Dropout totally killed my ability to train. \n",
    "0.5 dropout gets to 91% accuracy in 30 epochs with 80% of the training set, but over no epoch is validation accuracy changed.\n",
    "\n",
    "Now, using my test data as my validation data, just to start out:\n",
    "0.5 dropout, in 30 epochs I get 91.55 accuracy in full training set, 50% accuracy in test set at every epoch. It is totally training on noise. How about if I lower the complexity of the model\n",
    "\n",
    "One hidden layer with 5000 hidden units gets 98.78% accuracy on training set in 30 epochs, no budge on test (50% accuracy). I wonder if the data is somehow in wrong or randomized. I get same result with just 10 hidden units. I am going to see if random forus works, as I know it works in R.\n",
    "Great fits well at first model:\n",
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=train_exprs.shape[1:]), # this line needs work\n",
    "    Dense(5000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(500, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(p),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check: Try RandomForest with R default parameters (expect 70% test accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs so fast! A lot faster than in R on my computer. The RandomForests classifier trained on the full training set and used to predict on the full test set obtains 72.9% accuracy. Therefore, my data is intact. I don't know why "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.10455459e-04   3.21039856e-05   5.36269603e-05 ...,   3.87808318e-04\n",
      "   2.47868704e-04   3.68127795e-04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500, oob_score=True, bootstrap=True, max_features=\"sqrt\")\n",
    "#clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(train_exprs, encoded_Y_train)\n",
    "#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "#            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#            min_samples_leaf=1, min_samples_split=2,\n",
    "#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "#            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "print(clf.feature_importances_)\n",
    "#print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  7]\n",
      " [ 6 18]]\n",
      "0.729166666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "test_pred = clf.predict(test_exprs)\n",
    "print(confusion_matrix(encoded_Y_test, test_pred)) \n",
    "print(accuracy_score(encoded_Y_test, test_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the incorrect loss display of keras with the monkey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras shows increasing accuracy on the training set when it predicts all of one class at the end of training on the training set.\n",
    "\n",
    "### To debug this I am just going to try to do standard keras with the IRIS dataset, another structured dataset\n",
    "### I am using code from Jason Brownlee found at:\n",
    "\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
    "#dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first I will do his cross-validation code just to reproduce what he did. Then I will do it without cross-validation. Though cross-validation may be the way to go to really show whether my model is working correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ...................................... , score=1.0, total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9333333373069763, total=   3.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.8666666746139526, total=   3.2s\n",
      "Baseline: 96.67% (4.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   32.3s finished\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold, verbose=3)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There appears to be nothing wrong with Keras and Sci-kit learn, as I was able to run this prediction correctly. The next step is to break the IRIS dataset up into a training set and a small test set, like I have done, then use the same training and validation code, then predict on training and predict on test.\n",
    "\n",
    "### If this works, then I need to copy this code line by line to my code and retry it, if that doesn't work, then I should go ahead and go straight to 10-fold cross-validation on my training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.33, random_state=42, stratify=dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(100, 4)\n",
      "(50, 4)\n",
      "(150, 3)\n",
      "(100, 3)\n",
      "(50, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(dummy_y.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.0\n",
    "\n",
    "model = Sequential([\n",
    "   BatchNormalization(input_shape=X_train.shape[1:]),\n",
    "    Dense(5000, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(p),\n",
    "    Dense(500, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(p),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(p),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(p),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=0.001\n",
    "model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 50 samples\n",
      "Epoch 1/300\n",
      "100/100 [==============================] - 0s - loss: 1.0971 - acc: 0.3400 - val_loss: 1.2857 - val_acc: 0.3200\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 0s - loss: 0.9624 - acc: 0.8100 - val_loss: 2.6530 - val_acc: 0.3200\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 0s - loss: 0.7752 - acc: 0.8000 - val_loss: 4.1613 - val_acc: 0.3200\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 0s - loss: 0.6445 - acc: 0.8100 - val_loss: 5.1663 - val_acc: 0.3200\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 0s - loss: 0.5478 - acc: 0.8200 - val_loss: 5.3915 - val_acc: 0.3200\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 0s - loss: 0.4621 - acc: 0.8400 - val_loss: 4.8620 - val_acc: 0.3200\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 0s - loss: 0.3882 - acc: 0.8700 - val_loss: 3.5181 - val_acc: 0.3200\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 0s - loss: 0.3363 - acc: 0.8700 - val_loss: 2.4228 - val_acc: 0.3200\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 0s - loss: 0.2938 - acc: 0.9000 - val_loss: 2.1300 - val_acc: 0.3200\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 0s - loss: 0.2516 - acc: 0.9300 - val_loss: 2.1175 - val_acc: 0.3200\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 0s - loss: 0.2289 - acc: 0.9400 - val_loss: 1.9172 - val_acc: 0.3200\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 0s - loss: 0.2157 - acc: 0.9500 - val_loss: 1.5479 - val_acc: 0.4000\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 0s - loss: 0.1922 - acc: 0.9400 - val_loss: 1.4249 - val_acc: 0.5800\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 0s - loss: 0.1788 - acc: 0.9400 - val_loss: 1.5624 - val_acc: 0.5000\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 0s - loss: 0.1653 - acc: 0.9400 - val_loss: 2.0754 - val_acc: 0.4000\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 0s - loss: 0.1472 - acc: 0.9400 - val_loss: 2.7980 - val_acc: 0.3200\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 0s - loss: 0.1385 - acc: 0.9500 - val_loss: 3.2382 - val_acc: 0.3200\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 0s - loss: 0.1297 - acc: 0.9500 - val_loss: 3.2795 - val_acc: 0.3200\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 0s - loss: 0.1170 - acc: 0.9600 - val_loss: 3.1108 - val_acc: 0.3200\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 0s - loss: 0.1075 - acc: 0.9700 - val_loss: 3.0429 - val_acc: 0.3200\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 0s - loss: 0.0982 - acc: 0.9700 - val_loss: 3.1395 - val_acc: 0.3200\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 0s - loss: 0.0878 - acc: 0.9800 - val_loss: 3.2318 - val_acc: 0.3200\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 0s - loss: 0.0819 - acc: 0.9800 - val_loss: 3.2008 - val_acc: 0.3200\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 0s - loss: 0.0758 - acc: 0.9800 - val_loss: 3.1227 - val_acc: 0.3200\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 0s - loss: 0.0694 - acc: 0.9800 - val_loss: 3.2089 - val_acc: 0.3200\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 0s - loss: 0.0654 - acc: 0.9700 - val_loss: 3.5768 - val_acc: 0.3200\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 0s - loss: 0.0594 - acc: 0.9800 - val_loss: 4.0432 - val_acc: 0.3200\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 0s - loss: 0.0553 - acc: 0.9800 - val_loss: 4.4149 - val_acc: 0.3200\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 0s - loss: 0.0519 - acc: 0.9900 - val_loss: 4.6761 - val_acc: 0.3200\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 0s - loss: 0.0487 - acc: 0.9900 - val_loss: 4.9758 - val_acc: 0.3200\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 0s - loss: 0.0471 - acc: 0.9900 - val_loss: 5.3661 - val_acc: 0.3400\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 0s - loss: 0.0444 - acc: 0.9900 - val_loss: 5.7254 - val_acc: 0.3400\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 0s - loss: 0.0427 - acc: 0.9900 - val_loss: 5.8743 - val_acc: 0.3400\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 0s - loss: 0.0407 - acc: 0.9900 - val_loss: 5.9215 - val_acc: 0.3400\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 0s - loss: 0.0388 - acc: 0.9900 - val_loss: 5.9609 - val_acc: 0.3600\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 0s - loss: 0.0375 - acc: 0.9900 - val_loss: 5.9700 - val_acc: 0.3600\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 0s - loss: 0.0356 - acc: 0.9900 - val_loss: 5.9535 - val_acc: 0.3800\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 0s - loss: 0.0345 - acc: 0.9900 - val_loss: 5.9063 - val_acc: 0.3800\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 0s - loss: 0.0330 - acc: 0.9900 - val_loss: 5.8567 - val_acc: 0.4000\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 0s - loss: 0.0317 - acc: 0.9900 - val_loss: 5.8099 - val_acc: 0.4000\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 0s - loss: 0.0307 - acc: 0.9900 - val_loss: 5.7742 - val_acc: 0.4200\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 0s - loss: 0.0293 - acc: 0.9900 - val_loss: 5.7418 - val_acc: 0.4400\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 0s - loss: 0.0285 - acc: 0.9900 - val_loss: 5.7038 - val_acc: 0.5000\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 0s - loss: 0.0275 - acc: 0.9900 - val_loss: 5.6692 - val_acc: 0.5200\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 0s - loss: 0.0268 - acc: 0.9900 - val_loss: 5.6510 - val_acc: 0.5400\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 0s - loss: 0.0256 - acc: 0.9900 - val_loss: 5.6366 - val_acc: 0.5400\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 0s - loss: 0.0248 - acc: 0.9900 - val_loss: 5.6152 - val_acc: 0.5800\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 0s - loss: 0.0236 - acc: 0.9900 - val_loss: 5.5946 - val_acc: 0.5800\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 0s - loss: 0.0230 - acc: 0.9900 - val_loss: 5.5873 - val_acc: 0.6000\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 0s - loss: 0.0218 - acc: 0.9900 - val_loss: 5.5795 - val_acc: 0.6000\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 0s - loss: 0.0211 - acc: 0.9900 - val_loss: 5.5611 - val_acc: 0.6000\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 0s - loss: 0.0200 - acc: 0.9900 - val_loss: 5.5432 - val_acc: 0.6000\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 0s - loss: 0.0192 - acc: 0.9900 - val_loss: 5.5333 - val_acc: 0.6000\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 0s - loss: 0.0182 - acc: 0.9900 - val_loss: 5.5192 - val_acc: 0.6000\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 0s - loss: 0.0173 - acc: 0.9900 - val_loss: 5.4962 - val_acc: 0.6200\n",
      "Epoch 56/300\n",
      "100/100 [==============================] - 0s - loss: 0.0165 - acc: 0.9900 - val_loss: 5.4787 - val_acc: 0.6200\n",
      "Epoch 57/300\n",
      "100/100 [==============================] - 0s - loss: 0.0156 - acc: 1.0000 - val_loss: 5.4653 - val_acc: 0.6200\n",
      "Epoch 58/300\n",
      "100/100 [==============================] - 0s - loss: 0.0146 - acc: 1.0000 - val_loss: 5.4490 - val_acc: 0.6200\n",
      "Epoch 59/300\n",
      "100/100 [==============================] - 0s - loss: 0.0137 - acc: 1.0000 - val_loss: 5.4302 - val_acc: 0.6200\n",
      "Epoch 60/300\n",
      "100/100 [==============================] - 0s - loss: 0.0129 - acc: 1.0000 - val_loss: 5.4146 - val_acc: 0.6200\n",
      "Epoch 61/300\n",
      "100/100 [==============================] - 0s - loss: 0.0122 - acc: 1.0000 - val_loss: 5.4005 - val_acc: 0.6400\n",
      "Epoch 62/300\n",
      "100/100 [==============================] - 0s - loss: 0.0115 - acc: 1.0000 - val_loss: 5.3809 - val_acc: 0.6400\n",
      "Epoch 63/300\n",
      "100/100 [==============================] - 0s - loss: 0.0107 - acc: 1.0000 - val_loss: 5.3585 - val_acc: 0.6400\n",
      "Epoch 64/300\n",
      "100/100 [==============================] - 0s - loss: 0.0101 - acc: 1.0000 - val_loss: 5.3399 - val_acc: 0.6400\n",
      "Epoch 65/300\n",
      "100/100 [==============================] - 0s - loss: 0.0094 - acc: 1.0000 - val_loss: 5.3204 - val_acc: 0.6400\n",
      "Epoch 66/300\n",
      "100/100 [==============================] - 0s - loss: 0.0088 - acc: 1.0000 - val_loss: 5.2963 - val_acc: 0.6400\n",
      "Epoch 67/300\n",
      "100/100 [==============================] - 0s - loss: 0.0081 - acc: 1.0000 - val_loss: 5.2730 - val_acc: 0.6400\n",
      "Epoch 68/300\n",
      "100/100 [==============================] - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 5.2535 - val_acc: 0.6400\n",
      "Epoch 69/300\n",
      "100/100 [==============================] - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 5.2332 - val_acc: 0.6400\n",
      "Epoch 70/300\n",
      "100/100 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 5.2096 - val_acc: 0.6400\n",
      "Epoch 71/300\n",
      "100/100 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 5.1884 - val_acc: 0.6400\n",
      "Epoch 72/300\n",
      "100/100 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 5.1703 - val_acc: 0.6400\n",
      "Epoch 73/300\n",
      "100/100 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 5.1510 - val_acc: 0.6400\n",
      "Epoch 74/300\n",
      "100/100 [==============================] - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 5.1286 - val_acc: 0.6400\n",
      "Epoch 75/300\n",
      "100/100 [==============================] - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 5.1059 - val_acc: 0.6400\n",
      "Epoch 76/300\n",
      "100/100 [==============================] - 0s - loss: 0.0042 - acc: 1.0000 - val_loss: 5.0848 - val_acc: 0.6400\n",
      "Epoch 77/300\n",
      "100/100 [==============================] - 0s - loss: 0.0039 - acc: 1.0000 - val_loss: 5.0627 - val_acc: 0.6400\n",
      "Epoch 78/300\n",
      "100/100 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 5.0383 - val_acc: 0.6400\n",
      "Epoch 79/300\n",
      "100/100 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 5.0089 - val_acc: 0.6400\n",
      "Epoch 80/300\n",
      "100/100 [==============================] - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 4.9785 - val_acc: 0.6400\n",
      "Epoch 81/300\n",
      "100/100 [==============================] - 0s - loss: 0.0029 - acc: 1.0000 - val_loss: 4.9509 - val_acc: 0.6400\n",
      "Epoch 82/300\n",
      "100/100 [==============================] - 0s - loss: 0.0027 - acc: 1.0000 - val_loss: 4.9185 - val_acc: 0.6800\n",
      "Epoch 83/300\n",
      "100/100 [==============================] - 0s - loss: 0.0026 - acc: 1.0000 - val_loss: 4.8882 - val_acc: 0.6800\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 0s - loss: 0.0024 - acc: 1.0000 - val_loss: 4.8609 - val_acc: 0.6800\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 0s - loss: 0.0022 - acc: 1.0000 - val_loss: 4.8374 - val_acc: 0.6800\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 4.8159 - val_acc: 0.6800\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 0s - loss: 0.0020 - acc: 1.0000 - val_loss: 4.7953 - val_acc: 0.6800\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 4.7745 - val_acc: 0.6800\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 4.7535 - val_acc: 0.6800\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 4.7328 - val_acc: 0.6800\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 4.7134 - val_acc: 0.6800\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 4.6947 - val_acc: 0.6800\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 4.6710 - val_acc: 0.6800\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 4.6433 - val_acc: 0.6800\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 4.6150 - val_acc: 0.6800\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 4.5869 - val_acc: 0.6800\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 4.5585 - val_acc: 0.6800\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 4.5218 - val_acc: 0.6800\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 4.4826 - val_acc: 0.6800\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 4.4416 - val_acc: 0.6800\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 0s - loss: 9.8642e-04 - acc: 1.0000 - val_loss: 4.3992 - val_acc: 0.6800\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 0s - loss: 9.4619e-04 - acc: 1.0000 - val_loss: 4.3502 - val_acc: 0.6800\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 0s - loss: 9.0954e-04 - acc: 1.0000 - val_loss: 4.2999 - val_acc: 0.6800\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 0s - loss: 8.7566e-04 - acc: 1.0000 - val_loss: 4.2504 - val_acc: 0.6800\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 0s - loss: 8.4372e-04 - acc: 1.0000 - val_loss: 4.2010 - val_acc: 0.6800\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 0s - loss: 8.1336e-04 - acc: 1.0000 - val_loss: 4.1518 - val_acc: 0.6800\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 0s - loss: 7.8577e-04 - acc: 1.0000 - val_loss: 4.1021 - val_acc: 0.6800\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 0s - loss: 7.5984e-04 - acc: 1.0000 - val_loss: 4.0513 - val_acc: 0.6800\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 0s - loss: 7.3493e-04 - acc: 1.0000 - val_loss: 3.9998 - val_acc: 0.6800\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 0s - loss: 7.1083e-04 - acc: 1.0000 - val_loss: 3.9485 - val_acc: 0.6800\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 0s - loss: 6.8835e-04 - acc: 1.0000 - val_loss: 3.8995 - val_acc: 0.6800\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 0s - loss: 6.6793e-04 - acc: 1.0000 - val_loss: 3.8535 - val_acc: 0.6800\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 0s - loss: 6.4813e-04 - acc: 1.0000 - val_loss: 3.8104 - val_acc: 0.7000\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 0s - loss: 6.2960e-04 - acc: 1.0000 - val_loss: 3.7698 - val_acc: 0.7000\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 0s - loss: 6.1220e-04 - acc: 1.0000 - val_loss: 3.7302 - val_acc: 0.7000\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 0s - loss: 5.9537e-04 - acc: 1.0000 - val_loss: 3.6915 - val_acc: 0.7200\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 0s - loss: 5.7918e-04 - acc: 1.0000 - val_loss: 3.6544 - val_acc: 0.7200\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 0s - loss: 5.6370e-04 - acc: 1.0000 - val_loss: 3.6192 - val_acc: 0.7200\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 0s - loss: 5.4925e-04 - acc: 1.0000 - val_loss: 3.5860 - val_acc: 0.7200\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 0s - loss: 5.3592e-04 - acc: 1.0000 - val_loss: 3.5547 - val_acc: 0.7200\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 0s - loss: 5.2276e-04 - acc: 1.0000 - val_loss: 3.5244 - val_acc: 0.7200\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 0s - loss: 5.1006e-04 - acc: 1.0000 - val_loss: 3.4942 - val_acc: 0.7200\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 0s - loss: 4.9800e-04 - acc: 1.0000 - val_loss: 3.4580 - val_acc: 0.7200\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 0s - loss: 4.8638e-04 - acc: 1.0000 - val_loss: 3.4155 - val_acc: 0.7200\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 0s - loss: 4.7512e-04 - acc: 1.0000 - val_loss: 3.3694 - val_acc: 0.7200\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 0s - loss: 4.6431e-04 - acc: 1.0000 - val_loss: 3.3220 - val_acc: 0.7200\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 0s - loss: 4.5425e-04 - acc: 1.0000 - val_loss: 3.2759 - val_acc: 0.7200\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 0s - loss: 4.4455e-04 - acc: 1.0000 - val_loss: 3.2306 - val_acc: 0.7200\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 0s - loss: 4.3512e-04 - acc: 1.0000 - val_loss: 3.1802 - val_acc: 0.7200\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s - loss: 4.2594e-04 - acc: 1.0000 - val_loss: 3.1293 - val_acc: 0.7200\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 0s - loss: 4.1746e-04 - acc: 1.0000 - val_loss: 3.0752 - val_acc: 0.7200\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 0s - loss: 4.0909e-04 - acc: 1.0000 - val_loss: 3.0216 - val_acc: 0.7200\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 0s - loss: 4.0086e-04 - acc: 1.0000 - val_loss: 2.9689 - val_acc: 0.7200\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 0s - loss: 3.9301e-04 - acc: 1.0000 - val_loss: 2.9175 - val_acc: 0.7400\n",
      "Epoch 135/300\n",
      "100/100 [==============================] - 0s - loss: 3.8556e-04 - acc: 1.0000 - val_loss: 2.8673 - val_acc: 0.7400\n",
      "Epoch 136/300\n",
      "100/100 [==============================] - 0s - loss: 3.7834e-04 - acc: 1.0000 - val_loss: 2.8189 - val_acc: 0.7400\n",
      "Epoch 137/300\n",
      "100/100 [==============================] - 0s - loss: 3.7124e-04 - acc: 1.0000 - val_loss: 2.7720 - val_acc: 0.7400\n",
      "Epoch 138/300\n",
      "100/100 [==============================] - 0s - loss: 3.6435e-04 - acc: 1.0000 - val_loss: 2.7258 - val_acc: 0.7400\n",
      "Epoch 139/300\n",
      "100/100 [==============================] - 0s - loss: 3.5774e-04 - acc: 1.0000 - val_loss: 2.6804 - val_acc: 0.7400\n",
      "Epoch 140/300\n",
      "100/100 [==============================] - 0s - loss: 3.5128e-04 - acc: 1.0000 - val_loss: 2.6335 - val_acc: 0.7400\n",
      "Epoch 141/300\n",
      "100/100 [==============================] - 0s - loss: 3.4517e-04 - acc: 1.0000 - val_loss: 2.5863 - val_acc: 0.7400\n",
      "Epoch 142/300\n",
      "100/100 [==============================] - 0s - loss: 3.3931e-04 - acc: 1.0000 - val_loss: 2.5410 - val_acc: 0.7600\n",
      "Epoch 143/300\n",
      "100/100 [==============================] - 0s - loss: 3.3355e-04 - acc: 1.0000 - val_loss: 2.4968 - val_acc: 0.7600\n",
      "Epoch 144/300\n",
      "100/100 [==============================] - 0s - loss: 3.2789e-04 - acc: 1.0000 - val_loss: 2.4532 - val_acc: 0.7600\n",
      "Epoch 145/300\n",
      "100/100 [==============================] - 0s - loss: 3.2232e-04 - acc: 1.0000 - val_loss: 2.4113 - val_acc: 0.7800\n",
      "Epoch 146/300\n",
      "100/100 [==============================] - 0s - loss: 3.1698e-04 - acc: 1.0000 - val_loss: 2.3710 - val_acc: 0.7800\n",
      "Epoch 147/300\n",
      "100/100 [==============================] - 0s - loss: 3.1181e-04 - acc: 1.0000 - val_loss: 2.3289 - val_acc: 0.8000\n",
      "Epoch 148/300\n",
      "100/100 [==============================] - 0s - loss: 3.0683e-04 - acc: 1.0000 - val_loss: 2.2874 - val_acc: 0.8000\n",
      "Epoch 149/300\n",
      "100/100 [==============================] - 0s - loss: 3.0186e-04 - acc: 1.0000 - val_loss: 2.2462 - val_acc: 0.8000\n",
      "Epoch 150/300\n",
      "100/100 [==============================] - 0s - loss: 2.9701e-04 - acc: 1.0000 - val_loss: 2.2020 - val_acc: 0.8000\n",
      "Epoch 151/300\n",
      "100/100 [==============================] - 0s - loss: 2.9233e-04 - acc: 1.0000 - val_loss: 2.1581 - val_acc: 0.8000\n",
      "Epoch 152/300\n",
      "100/100 [==============================] - 0s - loss: 2.8781e-04 - acc: 1.0000 - val_loss: 2.1149 - val_acc: 0.8000\n",
      "Epoch 153/300\n",
      "100/100 [==============================] - 0s - loss: 2.8342e-04 - acc: 1.0000 - val_loss: 2.0726 - val_acc: 0.8000\n",
      "Epoch 154/300\n",
      "100/100 [==============================] - 0s - loss: 2.7909e-04 - acc: 1.0000 - val_loss: 2.0308 - val_acc: 0.8000\n",
      "Epoch 155/300\n",
      "100/100 [==============================] - 0s - loss: 2.7493e-04 - acc: 1.0000 - val_loss: 1.9894 - val_acc: 0.8000\n",
      "Epoch 156/300\n",
      "100/100 [==============================] - 0s - loss: 2.7090e-04 - acc: 1.0000 - val_loss: 1.9482 - val_acc: 0.8000\n",
      "Epoch 157/300\n",
      "100/100 [==============================] - 0s - loss: 2.6692e-04 - acc: 1.0000 - val_loss: 1.9078 - val_acc: 0.8000\n",
      "Epoch 158/300\n",
      "100/100 [==============================] - 0s - loss: 2.6305e-04 - acc: 1.0000 - val_loss: 1.8678 - val_acc: 0.8000\n",
      "Epoch 159/300\n",
      "100/100 [==============================] - 0s - loss: 2.5927e-04 - acc: 1.0000 - val_loss: 1.8284 - val_acc: 0.8000\n",
      "Epoch 160/300\n",
      "100/100 [==============================] - 0s - loss: 2.5559e-04 - acc: 1.0000 - val_loss: 1.7891 - val_acc: 0.8000\n",
      "Epoch 161/300\n",
      "100/100 [==============================] - 0s - loss: 2.5194e-04 - acc: 1.0000 - val_loss: 1.7500 - val_acc: 0.8000\n",
      "Epoch 162/300\n",
      "100/100 [==============================] - 0s - loss: 2.4849e-04 - acc: 1.0000 - val_loss: 1.7114 - val_acc: 0.8000\n",
      "Epoch 163/300\n",
      "100/100 [==============================] - 0s - loss: 2.4500e-04 - acc: 1.0000 - val_loss: 1.6727 - val_acc: 0.8000\n",
      "Epoch 164/300\n",
      "100/100 [==============================] - 0s - loss: 2.4161e-04 - acc: 1.0000 - val_loss: 1.6342 - val_acc: 0.8000\n",
      "Epoch 165/300\n",
      "100/100 [==============================] - 0s - loss: 2.3832e-04 - acc: 1.0000 - val_loss: 1.5958 - val_acc: 0.8000\n",
      "Epoch 166/300\n",
      "100/100 [==============================] - 0s - loss: 2.3511e-04 - acc: 1.0000 - val_loss: 1.5577 - val_acc: 0.8000\n",
      "Epoch 167/300\n",
      "100/100 [==============================] - 0s - loss: 2.3200e-04 - acc: 1.0000 - val_loss: 1.5199 - val_acc: 0.8000\n",
      "Epoch 168/300\n",
      "100/100 [==============================] - 0s - loss: 2.2893e-04 - acc: 1.0000 - val_loss: 1.4827 - val_acc: 0.8000\n",
      "Epoch 169/300\n",
      "100/100 [==============================] - 0s - loss: 2.2586e-04 - acc: 1.0000 - val_loss: 1.4460 - val_acc: 0.8000\n",
      "Epoch 170/300\n",
      "100/100 [==============================] - 0s - loss: 2.2286e-04 - acc: 1.0000 - val_loss: 1.4096 - val_acc: 0.8000\n",
      "Epoch 171/300\n",
      "100/100 [==============================] - 0s - loss: 2.1997e-04 - acc: 1.0000 - val_loss: 1.3735 - val_acc: 0.8000\n",
      "Epoch 172/300\n",
      "100/100 [==============================] - 0s - loss: 2.1713e-04 - acc: 1.0000 - val_loss: 1.3375 - val_acc: 0.8000\n",
      "Epoch 173/300\n",
      "100/100 [==============================] - 0s - loss: 2.1439e-04 - acc: 1.0000 - val_loss: 1.3021 - val_acc: 0.8000\n",
      "Epoch 174/300\n",
      "100/100 [==============================] - 0s - loss: 2.1166e-04 - acc: 1.0000 - val_loss: 1.2675 - val_acc: 0.8000\n",
      "Epoch 175/300\n",
      "100/100 [==============================] - 0s - loss: 2.0893e-04 - acc: 1.0000 - val_loss: 1.2337 - val_acc: 0.8000\n",
      "Epoch 176/300\n",
      "100/100 [==============================] - 0s - loss: 2.0630e-04 - acc: 1.0000 - val_loss: 1.2005 - val_acc: 0.8000\n",
      "Epoch 177/300\n",
      "100/100 [==============================] - 0s - loss: 2.0374e-04 - acc: 1.0000 - val_loss: 1.1681 - val_acc: 0.8000\n",
      "Epoch 178/300\n",
      "100/100 [==============================] - 0s - loss: 2.0126e-04 - acc: 1.0000 - val_loss: 1.1365 - val_acc: 0.8000\n",
      "Epoch 179/300\n",
      "100/100 [==============================] - 0s - loss: 1.9883e-04 - acc: 1.0000 - val_loss: 1.1057 - val_acc: 0.8000\n",
      "Epoch 180/300\n",
      "100/100 [==============================] - 0s - loss: 1.9641e-04 - acc: 1.0000 - val_loss: 1.0755 - val_acc: 0.8000\n",
      "Epoch 181/300\n",
      "100/100 [==============================] - 0s - loss: 1.9404e-04 - acc: 1.0000 - val_loss: 1.0461 - val_acc: 0.8200\n",
      "Epoch 182/300\n",
      "100/100 [==============================] - 0s - loss: 1.9169e-04 - acc: 1.0000 - val_loss: 1.0177 - val_acc: 0.8200\n",
      "Epoch 183/300\n",
      "100/100 [==============================] - 0s - loss: 1.8940e-04 - acc: 1.0000 - val_loss: 0.9905 - val_acc: 0.8200\n",
      "Epoch 184/300\n",
      "100/100 [==============================] - 0s - loss: 1.8714e-04 - acc: 1.0000 - val_loss: 0.9646 - val_acc: 0.8400\n",
      "Epoch 185/300\n",
      "100/100 [==============================] - 0s - loss: 1.8495e-04 - acc: 1.0000 - val_loss: 0.9399 - val_acc: 0.8400\n",
      "Epoch 186/300\n",
      "100/100 [==============================] - 0s - loss: 1.8279e-04 - acc: 1.0000 - val_loss: 0.9162 - val_acc: 0.8400\n",
      "Epoch 187/300\n",
      "100/100 [==============================] - 0s - loss: 1.8064e-04 - acc: 1.0000 - val_loss: 0.8934 - val_acc: 0.8400\n",
      "Epoch 188/300\n",
      "100/100 [==============================] - 0s - loss: 1.7854e-04 - acc: 1.0000 - val_loss: 0.8713 - val_acc: 0.8400\n",
      "Epoch 189/300\n",
      "100/100 [==============================] - 0s - loss: 1.7648e-04 - acc: 1.0000 - val_loss: 0.8502 - val_acc: 0.8400\n",
      "Epoch 190/300\n",
      "100/100 [==============================] - 0s - loss: 1.7452e-04 - acc: 1.0000 - val_loss: 0.8300 - val_acc: 0.8600\n",
      "Epoch 191/300\n",
      "100/100 [==============================] - 0s - loss: 1.7253e-04 - acc: 1.0000 - val_loss: 0.8106 - val_acc: 0.8600\n",
      "Epoch 192/300\n",
      "100/100 [==============================] - 0s - loss: 1.7060e-04 - acc: 1.0000 - val_loss: 0.7920 - val_acc: 0.8800\n",
      "Epoch 193/300\n",
      "100/100 [==============================] - 0s - loss: 1.6873e-04 - acc: 1.0000 - val_loss: 0.7742 - val_acc: 0.8800\n",
      "Epoch 194/300\n",
      "100/100 [==============================] - 0s - loss: 1.6685e-04 - acc: 1.0000 - val_loss: 0.7573 - val_acc: 0.8800\n",
      "Epoch 195/300\n",
      "100/100 [==============================] - 0s - loss: 1.6500e-04 - acc: 1.0000 - val_loss: 0.7411 - val_acc: 0.8800\n",
      "Epoch 196/300\n",
      "100/100 [==============================] - 0s - loss: 1.6321e-04 - acc: 1.0000 - val_loss: 0.7257 - val_acc: 0.8800\n",
      "Epoch 197/300\n",
      "100/100 [==============================] - 0s - loss: 1.6141e-04 - acc: 1.0000 - val_loss: 0.7107 - val_acc: 0.8800\n",
      "Epoch 198/300\n",
      "100/100 [==============================] - 0s - loss: 1.5968e-04 - acc: 1.0000 - val_loss: 0.6965 - val_acc: 0.8800\n",
      "Epoch 199/300\n",
      "100/100 [==============================] - 0s - loss: 1.5795e-04 - acc: 1.0000 - val_loss: 0.6829 - val_acc: 0.9000\n",
      "Epoch 200/300\n",
      "100/100 [==============================] - 0s - loss: 1.5627e-04 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.9000\n",
      "Epoch 201/300\n",
      "100/100 [==============================] - 0s - loss: 1.5460e-04 - acc: 1.0000 - val_loss: 0.6576 - val_acc: 0.9000\n",
      "Epoch 202/300\n",
      "100/100 [==============================] - 0s - loss: 1.5298e-04 - acc: 1.0000 - val_loss: 0.6459 - val_acc: 0.9000\n",
      "Epoch 203/300\n",
      "100/100 [==============================] - 0s - loss: 1.5140e-04 - acc: 1.0000 - val_loss: 0.6346 - val_acc: 0.9000\n",
      "Epoch 204/300\n",
      "100/100 [==============================] - 0s - loss: 1.4982e-04 - acc: 1.0000 - val_loss: 0.6236 - val_acc: 0.9000\n",
      "Epoch 205/300\n",
      "100/100 [==============================] - 0s - loss: 1.4826e-04 - acc: 1.0000 - val_loss: 0.6129 - val_acc: 0.9000\n",
      "Epoch 206/300\n",
      "100/100 [==============================] - 0s - loss: 1.4674e-04 - acc: 1.0000 - val_loss: 0.6026 - val_acc: 0.9000\n",
      "Epoch 207/300\n",
      "100/100 [==============================] - 0s - loss: 1.4521e-04 - acc: 1.0000 - val_loss: 0.5925 - val_acc: 0.9000\n",
      "Epoch 208/300\n",
      "100/100 [==============================] - 0s - loss: 1.4375e-04 - acc: 1.0000 - val_loss: 0.5827 - val_acc: 0.9000\n",
      "Epoch 209/300\n",
      "100/100 [==============================] - 0s - loss: 1.4228e-04 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.9000\n",
      "Epoch 210/300\n",
      "100/100 [==============================] - 0s - loss: 1.4082e-04 - acc: 1.0000 - val_loss: 0.5638 - val_acc: 0.9000\n",
      "Epoch 211/300\n",
      "100/100 [==============================] - 0s - loss: 1.3941e-04 - acc: 1.0000 - val_loss: 0.5547 - val_acc: 0.9000\n",
      "Epoch 212/300\n",
      "100/100 [==============================] - 0s - loss: 1.3801e-04 - acc: 1.0000 - val_loss: 0.5458 - val_acc: 0.9000\n",
      "Epoch 213/300\n",
      "100/100 [==============================] - 0s - loss: 1.3663e-04 - acc: 1.0000 - val_loss: 0.5372 - val_acc: 0.9000\n",
      "Epoch 214/300\n",
      "100/100 [==============================] - 0s - loss: 1.3529e-04 - acc: 1.0000 - val_loss: 0.5289 - val_acc: 0.9000\n",
      "Epoch 215/300\n",
      "100/100 [==============================] - 0s - loss: 1.3396e-04 - acc: 1.0000 - val_loss: 0.5206 - val_acc: 0.9000\n",
      "Epoch 216/300\n",
      "100/100 [==============================] - 0s - loss: 1.3266e-04 - acc: 1.0000 - val_loss: 0.5125 - val_acc: 0.9000\n",
      "Epoch 217/300\n",
      "100/100 [==============================] - 0s - loss: 1.3137e-04 - acc: 1.0000 - val_loss: 0.5046 - val_acc: 0.9000\n",
      "Epoch 218/300\n",
      "100/100 [==============================] - 0s - loss: 1.3010e-04 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.9000\n",
      "Epoch 219/300\n",
      "100/100 [==============================] - 0s - loss: 1.2888e-04 - acc: 1.0000 - val_loss: 0.4895 - val_acc: 0.9000\n",
      "Epoch 220/300\n",
      "100/100 [==============================] - 0s - loss: 1.2764e-04 - acc: 1.0000 - val_loss: 0.4823 - val_acc: 0.9000\n",
      "Epoch 221/300\n",
      "100/100 [==============================] - 0s - loss: 1.2639e-04 - acc: 1.0000 - val_loss: 0.4752 - val_acc: 0.9000\n",
      "Epoch 222/300\n",
      "100/100 [==============================] - 0s - loss: 1.2523e-04 - acc: 1.0000 - val_loss: 0.4683 - val_acc: 0.9000\n",
      "Epoch 223/300\n",
      "100/100 [==============================] - 0s - loss: 1.2408e-04 - acc: 1.0000 - val_loss: 0.4616 - val_acc: 0.9000\n",
      "Epoch 224/300\n",
      "100/100 [==============================] - 0s - loss: 1.2292e-04 - acc: 1.0000 - val_loss: 0.4552 - val_acc: 0.9000\n",
      "Epoch 225/300\n",
      "100/100 [==============================] - 0s - loss: 1.2176e-04 - acc: 1.0000 - val_loss: 0.4489 - val_acc: 0.9000\n",
      "Epoch 226/300\n",
      "100/100 [==============================] - 0s - loss: 1.2060e-04 - acc: 1.0000 - val_loss: 0.4429 - val_acc: 0.9000\n",
      "Epoch 227/300\n",
      "100/100 [==============================] - 0s - loss: 1.1950e-04 - acc: 1.0000 - val_loss: 0.4372 - val_acc: 0.9000\n",
      "Epoch 228/300\n",
      "100/100 [==============================] - 0s - loss: 1.1842e-04 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 0.9000\n",
      "Epoch 229/300\n",
      "100/100 [==============================] - 0s - loss: 1.1735e-04 - acc: 1.0000 - val_loss: 0.4263 - val_acc: 0.9000\n",
      "Epoch 230/300\n",
      "100/100 [==============================] - 0s - loss: 1.1628e-04 - acc: 1.0000 - val_loss: 0.4212 - val_acc: 0.9200\n",
      "Epoch 231/300\n",
      "100/100 [==============================] - 0s - loss: 1.1521e-04 - acc: 1.0000 - val_loss: 0.4164 - val_acc: 0.9200\n",
      "Epoch 232/300\n",
      "100/100 [==============================] - 0s - loss: 1.1418e-04 - acc: 1.0000 - val_loss: 0.4119 - val_acc: 0.9200\n",
      "Epoch 233/300\n",
      "100/100 [==============================] - 0s - loss: 1.1316e-04 - acc: 1.0000 - val_loss: 0.4076 - val_acc: 0.9200\n",
      "Epoch 234/300\n",
      "100/100 [==============================] - 0s - loss: 1.1216e-04 - acc: 1.0000 - val_loss: 0.4035 - val_acc: 0.9200\n",
      "Epoch 235/300\n",
      "100/100 [==============================] - 0s - loss: 1.1115e-04 - acc: 1.0000 - val_loss: 0.3996 - val_acc: 0.9200\n",
      "Epoch 236/300\n",
      "100/100 [==============================] - 0s - loss: 1.1019e-04 - acc: 1.0000 - val_loss: 0.3960 - val_acc: 0.9200\n",
      "Epoch 237/300\n",
      "100/100 [==============================] - 0s - loss: 1.0921e-04 - acc: 1.0000 - val_loss: 0.3925 - val_acc: 0.9200\n",
      "Epoch 238/300\n",
      "100/100 [==============================] - 0s - loss: 1.0824e-04 - acc: 1.0000 - val_loss: 0.3893 - val_acc: 0.9200\n",
      "Epoch 239/300\n",
      "100/100 [==============================] - 0s - loss: 1.0729e-04 - acc: 1.0000 - val_loss: 0.3863 - val_acc: 0.9200\n",
      "Epoch 240/300\n",
      "100/100 [==============================] - 0s - loss: 1.0638e-04 - acc: 1.0000 - val_loss: 0.3835 - val_acc: 0.9200\n",
      "Epoch 241/300\n",
      "100/100 [==============================] - 0s - loss: 1.0545e-04 - acc: 1.0000 - val_loss: 0.3810 - val_acc: 0.9200\n",
      "Epoch 242/300\n",
      "100/100 [==============================] - 0s - loss: 1.0455e-04 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.9200\n",
      "Epoch 243/300\n",
      "100/100 [==============================] - 0s - loss: 1.0367e-04 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.9600\n",
      "Epoch 244/300\n",
      "100/100 [==============================] - 0s - loss: 1.0278e-04 - acc: 1.0000 - val_loss: 0.3742 - val_acc: 0.9600\n",
      "Epoch 245/300\n",
      "100/100 [==============================] - 0s - loss: 1.0191e-04 - acc: 1.0000 - val_loss: 0.3723 - val_acc: 0.9600\n",
      "Epoch 246/300\n",
      "100/100 [==============================] - 0s - loss: 1.0106e-04 - acc: 1.0000 - val_loss: 0.3705 - val_acc: 0.9600\n",
      "Epoch 247/300\n",
      "100/100 [==============================] - 0s - loss: 1.0021e-04 - acc: 1.0000 - val_loss: 0.3688 - val_acc: 0.9600\n",
      "Epoch 248/300\n",
      "100/100 [==============================] - 0s - loss: 9.9369e-05 - acc: 1.0000 - val_loss: 0.3673 - val_acc: 0.9600\n",
      "Epoch 249/300\n",
      "100/100 [==============================] - 0s - loss: 9.8539e-05 - acc: 1.0000 - val_loss: 0.3660 - val_acc: 0.9600\n",
      "Epoch 250/300\n",
      "100/100 [==============================] - 0s - loss: 9.7709e-05 - acc: 1.0000 - val_loss: 0.3647 - val_acc: 0.9600\n",
      "Epoch 251/300\n",
      "100/100 [==============================] - 0s - loss: 9.6905e-05 - acc: 1.0000 - val_loss: 0.3635 - val_acc: 0.9600\n",
      "Epoch 252/300\n",
      "100/100 [==============================] - 0s - loss: 9.6126e-05 - acc: 1.0000 - val_loss: 0.3616 - val_acc: 0.9600\n",
      "Epoch 253/300\n",
      "100/100 [==============================] - 0s - loss: 9.5336e-05 - acc: 1.0000 - val_loss: 0.3593 - val_acc: 0.9600\n",
      "Epoch 254/300\n",
      "100/100 [==============================] - 0s - loss: 9.4549e-05 - acc: 1.0000 - val_loss: 0.3572 - val_acc: 0.9600\n",
      "Epoch 255/300\n",
      "100/100 [==============================] - 0s - loss: 9.3801e-05 - acc: 1.0000 - val_loss: 0.3552 - val_acc: 0.9600\n",
      "Epoch 256/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s - loss: 9.3036e-05 - acc: 1.0000 - val_loss: 0.3532 - val_acc: 0.9600\n",
      "Epoch 257/300\n",
      "100/100 [==============================] - 0s - loss: 9.2286e-05 - acc: 1.0000 - val_loss: 0.3514 - val_acc: 0.9600\n",
      "Epoch 258/300\n",
      "100/100 [==============================] - 0s - loss: 9.1556e-05 - acc: 1.0000 - val_loss: 0.3496 - val_acc: 0.9600\n",
      "Epoch 259/300\n",
      "100/100 [==============================] - 0s - loss: 9.0818e-05 - acc: 1.0000 - val_loss: 0.3478 - val_acc: 0.9600\n",
      "Epoch 260/300\n",
      "100/100 [==============================] - 0s - loss: 9.0101e-05 - acc: 1.0000 - val_loss: 0.3462 - val_acc: 0.9600\n",
      "Epoch 261/300\n",
      "100/100 [==============================] - 0s - loss: 8.9395e-05 - acc: 1.0000 - val_loss: 0.3446 - val_acc: 0.9600\n",
      "Epoch 262/300\n",
      "100/100 [==============================] - 0s - loss: 8.8700e-05 - acc: 1.0000 - val_loss: 0.3431 - val_acc: 0.9600\n",
      "Epoch 263/300\n",
      "100/100 [==============================] - 0s - loss: 8.7992e-05 - acc: 1.0000 - val_loss: 0.3417 - val_acc: 0.9600\n",
      "Epoch 264/300\n",
      "100/100 [==============================] - 0s - loss: 8.7303e-05 - acc: 1.0000 - val_loss: 0.3403 - val_acc: 0.9600\n",
      "Epoch 265/300\n",
      "100/100 [==============================] - 0s - loss: 8.6639e-05 - acc: 1.0000 - val_loss: 0.3390 - val_acc: 0.9600\n",
      "Epoch 266/300\n",
      "100/100 [==============================] - 0s - loss: 8.5974e-05 - acc: 1.0000 - val_loss: 0.3377 - val_acc: 0.9600\n",
      "Epoch 267/300\n",
      "100/100 [==============================] - 0s - loss: 8.5298e-05 - acc: 1.0000 - val_loss: 0.3365 - val_acc: 0.9600\n",
      "Epoch 268/300\n",
      "100/100 [==============================] - 0s - loss: 8.4634e-05 - acc: 1.0000 - val_loss: 0.3354 - val_acc: 0.9600\n",
      "Epoch 269/300\n",
      "100/100 [==============================] - 0s - loss: 8.4001e-05 - acc: 1.0000 - val_loss: 0.3343 - val_acc: 0.9600\n",
      "Epoch 270/300\n",
      "100/100 [==============================] - 0s - loss: 8.3360e-05 - acc: 1.0000 - val_loss: 0.3333 - val_acc: 0.9600\n",
      "Epoch 271/300\n",
      "100/100 [==============================] - 0s - loss: 8.2736e-05 - acc: 1.0000 - val_loss: 0.3322 - val_acc: 0.9600\n",
      "Epoch 272/300\n",
      "100/100 [==============================] - 0s - loss: 8.2122e-05 - acc: 1.0000 - val_loss: 0.3312 - val_acc: 0.9600\n",
      "Epoch 273/300\n",
      "100/100 [==============================] - 0s - loss: 8.1498e-05 - acc: 1.0000 - val_loss: 0.3303 - val_acc: 0.9600\n",
      "Epoch 274/300\n",
      "100/100 [==============================] - 0s - loss: 8.0882e-05 - acc: 1.0000 - val_loss: 0.3293 - val_acc: 0.9600\n",
      "Epoch 275/300\n",
      "100/100 [==============================] - 0s - loss: 8.0281e-05 - acc: 1.0000 - val_loss: 0.3284 - val_acc: 0.9600\n",
      "Epoch 276/300\n",
      "100/100 [==============================] - 0s - loss: 7.9683e-05 - acc: 1.0000 - val_loss: 0.3276 - val_acc: 0.9600\n",
      "Epoch 277/300\n",
      "100/100 [==============================] - 0s - loss: 7.9089e-05 - acc: 1.0000 - val_loss: 0.3267 - val_acc: 0.9600\n",
      "Epoch 278/300\n",
      "100/100 [==============================] - 0s - loss: 7.8508e-05 - acc: 1.0000 - val_loss: 0.3259 - val_acc: 0.9600\n",
      "Epoch 279/300\n",
      "100/100 [==============================] - 0s - loss: 7.7930e-05 - acc: 1.0000 - val_loss: 0.3251 - val_acc: 0.9600\n",
      "Epoch 280/300\n",
      "100/100 [==============================] - 0s - loss: 7.7362e-05 - acc: 1.0000 - val_loss: 0.3244 - val_acc: 0.9600\n",
      "Epoch 281/300\n",
      "100/100 [==============================] - 0s - loss: 7.6789e-05 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 0.9600\n",
      "Epoch 282/300\n",
      "100/100 [==============================] - 0s - loss: 7.6239e-05 - acc: 1.0000 - val_loss: 0.3229 - val_acc: 0.9600\n",
      "Epoch 283/300\n",
      "100/100 [==============================] - 0s - loss: 7.5682e-05 - acc: 1.0000 - val_loss: 0.3222 - val_acc: 0.9600\n",
      "Epoch 284/300\n",
      "100/100 [==============================] - 0s - loss: 7.5148e-05 - acc: 1.0000 - val_loss: 0.3215 - val_acc: 0.9600\n",
      "Epoch 285/300\n",
      "100/100 [==============================] - 0s - loss: 7.4599e-05 - acc: 1.0000 - val_loss: 0.3209 - val_acc: 0.9600\n",
      "Epoch 286/300\n",
      "100/100 [==============================] - 0s - loss: 7.4067e-05 - acc: 1.0000 - val_loss: 0.3203 - val_acc: 0.9600\n",
      "Epoch 287/300\n",
      "100/100 [==============================] - 0s - loss: 7.3539e-05 - acc: 1.0000 - val_loss: 0.3197 - val_acc: 0.9600\n",
      "Epoch 288/300\n",
      "100/100 [==============================] - 0s - loss: 7.3016e-05 - acc: 1.0000 - val_loss: 0.3191 - val_acc: 0.9600\n",
      "Epoch 289/300\n",
      "100/100 [==============================] - 0s - loss: 7.2499e-05 - acc: 1.0000 - val_loss: 0.3186 - val_acc: 0.9600\n",
      "Epoch 290/300\n",
      "100/100 [==============================] - 0s - loss: 7.1987e-05 - acc: 1.0000 - val_loss: 0.3181 - val_acc: 0.9600\n",
      "Epoch 291/300\n",
      "100/100 [==============================] - 0s - loss: 7.1484e-05 - acc: 1.0000 - val_loss: 0.3176 - val_acc: 0.9600\n",
      "Epoch 292/300\n",
      "100/100 [==============================] - 0s - loss: 7.0985e-05 - acc: 1.0000 - val_loss: 0.3172 - val_acc: 0.9600\n",
      "Epoch 293/300\n",
      "100/100 [==============================] - 0s - loss: 7.0495e-05 - acc: 1.0000 - val_loss: 0.3167 - val_acc: 0.9600\n",
      "Epoch 294/300\n",
      "100/100 [==============================] - 0s - loss: 6.9995e-05 - acc: 1.0000 - val_loss: 0.3162 - val_acc: 0.9600\n",
      "Epoch 295/300\n",
      "100/100 [==============================] - 0s - loss: 6.9506e-05 - acc: 1.0000 - val_loss: 0.3158 - val_acc: 0.9600\n",
      "Epoch 296/300\n",
      "100/100 [==============================] - 0s - loss: 6.9033e-05 - acc: 1.0000 - val_loss: 0.3154 - val_acc: 0.9600\n",
      "Epoch 297/300\n",
      "100/100 [==============================] - 0s - loss: 6.8557e-05 - acc: 1.0000 - val_loss: 0.3150 - val_acc: 0.9600\n",
      "Epoch 298/300\n",
      "100/100 [==============================] - 0s - loss: 6.8093e-05 - acc: 1.0000 - val_loss: 0.3147 - val_acc: 0.9600\n",
      "Epoch 299/300\n",
      "100/100 [==============================] - 0s - loss: 6.7633e-05 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9600\n",
      "Epoch 300/300\n",
      "100/100 [==============================] - 0s - loss: 6.7180e-05 - acc: 1.0000 - val_loss: 0.3139 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95f929fa20>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=X_train.shape[0], epochs=300, validation_data = (X_test, y_test), verbose=1)\n",
    "\n",
    "# Same problem where training loss goes down with increased accuracy, but validation accuracy doesn't change. Let's see how it predicts things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 50 samples\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 0s - loss: 2.1376 - acc: 0.3400 - val_loss: 2.0439 - val_acc: 0.3200\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 0s - loss: 1.8323 - acc: 0.3400 - val_loss: 1.7715 - val_acc: 0.3200\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 0s - loss: 1.6054 - acc: 0.3400 - val_loss: 1.5255 - val_acc: 0.3200\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 0s - loss: 1.3991 - acc: 0.3400 - val_loss: 1.3278 - val_acc: 0.3200\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 0s - loss: 1.2611 - acc: 0.3400 - val_loss: 1.2279 - val_acc: 0.3200\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 0s - loss: 1.1946 - acc: 0.3300 - val_loss: 1.1753 - val_acc: 0.2600\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 0s - loss: 1.1576 - acc: 0.3100 - val_loss: 1.1321 - val_acc: 0.2600\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 0s - loss: 1.1184 - acc: 0.3200 - val_loss: 1.0992 - val_acc: 0.2800\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 0s - loss: 1.0821 - acc: 0.3500 - val_loss: 1.0619 - val_acc: 0.3000\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 0s - loss: 1.0521 - acc: 0.3400 - val_loss: 1.0272 - val_acc: 0.2800\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 0s - loss: 1.0178 - acc: 0.3500 - val_loss: 0.9961 - val_acc: 0.3000\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 0s - loss: 0.9880 - acc: 0.3700 - val_loss: 0.9663 - val_acc: 0.3800\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 0s - loss: 0.9598 - acc: 0.4600 - val_loss: 0.9372 - val_acc: 0.5400\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 0s - loss: 0.9289 - acc: 0.5500 - val_loss: 0.9072 - val_acc: 0.6000\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 0s - loss: 0.9015 - acc: 0.6300 - val_loss: 0.8803 - val_acc: 0.6600\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 0s - loss: 0.8748 - acc: 0.6800 - val_loss: 0.8520 - val_acc: 0.7000\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 0s - loss: 0.8479 - acc: 0.6900 - val_loss: 0.8260 - val_acc: 0.7000\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 0s - loss: 0.8244 - acc: 0.7000 - val_loss: 0.8026 - val_acc: 0.6800\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 0s - loss: 0.7982 - acc: 0.7100 - val_loss: 0.7772 - val_acc: 0.7200\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 0s - loss: 0.7753 - acc: 0.7200 - val_loss: 0.7565 - val_acc: 0.7000\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 0s - loss: 0.7527 - acc: 0.7100 - val_loss: 0.7357 - val_acc: 0.7200\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6742 - acc: 1.000 - 0s - loss: 0.7323 - acc: 0.7300 - val_loss: 0.7139 - val_acc: 0.7400\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 0s - loss: 0.7119 - acc: 0.7400 - val_loss: 0.6958 - val_acc: 0.7400\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 0s - loss: 0.6915 - acc: 0.7400 - val_loss: 0.6771 - val_acc: 0.7600\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 0s - loss: 0.6736 - acc: 0.7500 - val_loss: 0.6596 - val_acc: 0.7600\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 0s - loss: 0.6567 - acc: 0.7700 - val_loss: 0.6432 - val_acc: 0.7600\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 0s - loss: 0.6401 - acc: 0.7700 - val_loss: 0.6277 - val_acc: 0.7600\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 0s - loss: 0.6268 - acc: 0.7700 - val_loss: 0.6128 - val_acc: 0.7600\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 0s - loss: 0.6111 - acc: 0.7800 - val_loss: 0.5995 - val_acc: 0.7600\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 0s - loss: 0.5963 - acc: 0.7800 - val_loss: 0.5870 - val_acc: 0.7600\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 0s - loss: 0.5847 - acc: 0.7800 - val_loss: 0.5737 - val_acc: 0.7800\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 0s - loss: 0.5732 - acc: 0.7800 - val_loss: 0.5634 - val_acc: 0.7600\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 0s - loss: 0.5597 - acc: 0.8000 - val_loss: 0.5512 - val_acc: 0.7800\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 0s - loss: 0.5491 - acc: 0.8100 - val_loss: 0.5402 - val_acc: 0.8200\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 0s - loss: 0.5383 - acc: 0.8400 - val_loss: 0.5292 - val_acc: 0.8400\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 0s - loss: 0.5284 - acc: 0.8500 - val_loss: 0.5195 - val_acc: 0.8400\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 0s - loss: 0.5191 - acc: 0.8600 - val_loss: 0.5108 - val_acc: 0.8400\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 0s - loss: 0.5102 - acc: 0.8400 - val_loss: 0.5027 - val_acc: 0.8200\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 0s - loss: 0.5008 - acc: 0.8600 - val_loss: 0.4928 - val_acc: 0.8400\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 0s - loss: 0.4938 - acc: 0.8900 - val_loss: 0.4844 - val_acc: 0.8600\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 0s - loss: 0.4847 - acc: 0.8900 - val_loss: 0.4772 - val_acc: 0.8400\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 0s - loss: 0.4772 - acc: 0.8900 - val_loss: 0.4689 - val_acc: 0.8600\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 0s - loss: 0.4705 - acc: 0.9000 - val_loss: 0.4622 - val_acc: 0.8400\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 0s - loss: 0.4631 - acc: 0.8900 - val_loss: 0.4540 - val_acc: 0.8600\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 0s - loss: 0.4562 - acc: 0.8900 - val_loss: 0.4472 - val_acc: 0.8600\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 0s - loss: 0.4501 - acc: 0.8900 - val_loss: 0.4413 - val_acc: 0.8600\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 0s - loss: 0.4444 - acc: 0.8900 - val_loss: 0.4344 - val_acc: 0.8600\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 0s - loss: 0.4380 - acc: 0.9100 - val_loss: 0.4286 - val_acc: 0.8600\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 0s - loss: 0.4312 - acc: 0.9000 - val_loss: 0.4217 - val_acc: 0.8800\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 0s - loss: 0.4261 - acc: 0.9000 - val_loss: 0.4140 - val_acc: 0.9400\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 0s - loss: 0.4211 - acc: 0.9000 - val_loss: 0.4104 - val_acc: 0.8800\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 0s - loss: 0.4163 - acc: 0.9000 - val_loss: 0.4026 - val_acc: 0.9400\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 0s - loss: 0.4144 - acc: 0.9100 - val_loss: 0.3965 - val_acc: 0.9800\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 0s - loss: 0.4070 - acc: 0.9200 - val_loss: 0.3938 - val_acc: 0.8800\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 0s - loss: 0.4004 - acc: 0.9000 - val_loss: 0.3892 - val_acc: 0.8800\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 0s - loss: 0.3975 - acc: 0.9100 - val_loss: 0.3811 - val_acc: 0.9800\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 0s - loss: 0.3915 - acc: 0.9300 - val_loss: 0.3798 - val_acc: 0.8800\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 0s - loss: 0.3870 - acc: 0.9100 - val_loss: 0.3725 - val_acc: 0.9800\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 0s - loss: 0.3818 - acc: 0.9300 - val_loss: 0.3680 - val_acc: 0.9800\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 0s - loss: 0.3804 - acc: 0.9300 - val_loss: 0.3623 - val_acc: 0.9800\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 0s - loss: 0.3729 - acc: 0.9300 - val_loss: 0.3603 - val_acc: 0.9000\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 0s - loss: 0.3694 - acc: 0.9200 - val_loss: 0.3535 - val_acc: 0.9800\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 0s - loss: 0.3652 - acc: 0.9300 - val_loss: 0.3493 - val_acc: 0.9800\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 0s - loss: 0.3608 - acc: 0.9300 - val_loss: 0.3457 - val_acc: 0.9800\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 0s - loss: 0.3583 - acc: 0.9300 - val_loss: 0.3407 - val_acc: 0.9800\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 0s - loss: 0.3541 - acc: 0.9300 - val_loss: 0.3376 - val_acc: 0.9800\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 0s - loss: 0.3504 - acc: 0.9300 - val_loss: 0.3317 - val_acc: 0.9800\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 0s - loss: 0.3461 - acc: 0.9300 - val_loss: 0.3279 - val_acc: 0.9800\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 0s - loss: 0.3423 - acc: 0.9300 - val_loss: 0.3242 - val_acc: 0.9800\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 0s - loss: 0.3383 - acc: 0.9300 - val_loss: 0.3217 - val_acc: 0.9800\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 0s - loss: 0.3350 - acc: 0.9300 - val_loss: 0.3180 - val_acc: 0.9800\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 0s - loss: 0.3311 - acc: 0.9300 - val_loss: 0.3136 - val_acc: 0.9800\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 0s - loss: 0.3279 - acc: 0.9300 - val_loss: 0.3075 - val_acc: 0.9800\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 0s - loss: 0.3250 - acc: 0.9300 - val_loss: 0.3061 - val_acc: 0.9800\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 0s - loss: 0.3225 - acc: 0.9300 - val_loss: 0.3010 - val_acc: 0.9800\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 0s - loss: 0.3179 - acc: 0.9300 - val_loss: 0.3000 - val_acc: 0.9800\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 0s - loss: 0.3164 - acc: 0.9300 - val_loss: 0.2929 - val_acc: 0.9800\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 0s - loss: 0.3128 - acc: 0.9300 - val_loss: 0.2915 - val_acc: 0.9800\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 0s - loss: 0.3079 - acc: 0.9300 - val_loss: 0.2867 - val_acc: 0.9800\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 0s - loss: 0.3062 - acc: 0.9300 - val_loss: 0.2853 - val_acc: 0.9800\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 0s - loss: 0.3020 - acc: 0.9300 - val_loss: 0.2807 - val_acc: 0.9800\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 0s - loss: 0.3007 - acc: 0.9300 - val_loss: 0.2801 - val_acc: 0.9800\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 0s - loss: 0.2950 - acc: 0.9300 - val_loss: 0.2731 - val_acc: 0.9800\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 0s - loss: 0.2961 - acc: 0.9300 - val_loss: 0.2708 - val_acc: 0.9800\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 0s - loss: 0.2904 - acc: 0.9300 - val_loss: 0.2676 - val_acc: 0.9800\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 0s - loss: 0.2879 - acc: 0.9400 - val_loss: 0.2614 - val_acc: 0.9800\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 0s - loss: 0.2849 - acc: 0.9300 - val_loss: 0.2607 - val_acc: 0.9800\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 0s - loss: 0.2831 - acc: 0.9300 - val_loss: 0.2614 - val_acc: 0.9800\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 0s - loss: 0.2803 - acc: 0.9400 - val_loss: 0.2526 - val_acc: 0.9800\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 0s - loss: 0.2801 - acc: 0.9400 - val_loss: 0.2564 - val_acc: 0.9800\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 0s - loss: 0.2726 - acc: 0.9300 - val_loss: 0.2485 - val_acc: 0.9800\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 0s - loss: 0.2761 - acc: 0.9400 - val_loss: 0.2447 - val_acc: 0.9800\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 0s - loss: 0.2695 - acc: 0.9400 - val_loss: 0.2460 - val_acc: 0.9800\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 0s - loss: 0.2659 - acc: 0.9300 - val_loss: 0.2400 - val_acc: 0.9800\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 0s - loss: 0.2627 - acc: 0.9300 - val_loss: 0.2380 - val_acc: 0.9800\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 0s - loss: 0.2603 - acc: 0.9300 - val_loss: 0.2342 - val_acc: 0.9800\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 0s - loss: 0.2578 - acc: 0.9300 - val_loss: 0.2313 - val_acc: 0.9800\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 0s - loss: 0.2579 - acc: 0.9400 - val_loss: 0.2286 - val_acc: 0.9800\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 0s - loss: 0.2588 - acc: 0.9400 - val_loss: 0.2312 - val_acc: 0.9800\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 0s - loss: 0.2494 - acc: 0.9300 - val_loss: 0.2229 - val_acc: 0.9800\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 0s - loss: 0.2490 - acc: 0.9400 - val_loss: 0.2194 - val_acc: 0.9800\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 0s - loss: 0.2466 - acc: 0.9300 - val_loss: 0.2197 - val_acc: 0.9800\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 0s - loss: 0.2452 - acc: 0.9300 - val_loss: 0.2143 - val_acc: 0.9800\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 0s - loss: 0.2434 - acc: 0.9300 - val_loss: 0.2156 - val_acc: 0.9800\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 0s - loss: 0.2392 - acc: 0.9300 - val_loss: 0.2122 - val_acc: 0.9800\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 0s - loss: 0.2391 - acc: 0.9300 - val_loss: 0.2083 - val_acc: 0.9800\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 0s - loss: 0.2398 - acc: 0.9300 - val_loss: 0.2077 - val_acc: 0.9800\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 0s - loss: 0.2319 - acc: 0.9300 - val_loss: 0.2018 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 0s - loss: 0.2324 - acc: 0.9400 - val_loss: 0.2007 - val_acc: 0.9800\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 0s - loss: 0.2288 - acc: 0.9300 - val_loss: 0.2013 - val_acc: 0.9800\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 0s - loss: 0.2272 - acc: 0.9300 - val_loss: 0.1998 - val_acc: 0.9800\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 0s - loss: 0.2251 - acc: 0.9300 - val_loss: 0.1956 - val_acc: 0.9800\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 0s - loss: 0.2238 - acc: 0.9300 - val_loss: 0.1946 - val_acc: 0.9800\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 0s - loss: 0.2232 - acc: 0.9400 - val_loss: 0.1890 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 0s - loss: 0.2194 - acc: 0.9400 - val_loss: 0.1899 - val_acc: 0.9800\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 0s - loss: 0.2204 - acc: 0.9300 - val_loss: 0.1897 - val_acc: 0.9800\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 0s - loss: 0.2159 - acc: 0.9300 - val_loss: 0.1846 - val_acc: 0.9800\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 0s - loss: 0.2168 - acc: 0.9300 - val_loss: 0.1819 - val_acc: 0.9800\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 0s - loss: 0.2149 - acc: 0.9400 - val_loss: 0.1840 - val_acc: 0.9800\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 0s - loss: 0.2107 - acc: 0.9300 - val_loss: 0.1780 - val_acc: 0.9800\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 0s - loss: 0.2159 - acc: 0.9300 - val_loss: 0.1827 - val_acc: 0.9800\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 0s - loss: 0.2070 - acc: 0.9300 - val_loss: 0.1739 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 0s - loss: 0.2059 - acc: 0.9400 - val_loss: 0.1715 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 0s - loss: 0.2045 - acc: 0.9300 - val_loss: 0.1751 - val_acc: 0.9800\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 0s - loss: 0.2060 - acc: 0.9400 - val_loss: 0.1681 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 0s - loss: 0.2008 - acc: 0.9400 - val_loss: 0.1739 - val_acc: 0.9800\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 0s - loss: 0.1994 - acc: 0.9300 - val_loss: 0.1687 - val_acc: 0.9800\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 0s - loss: 0.1981 - acc: 0.9400 - val_loss: 0.1646 - val_acc: 0.9800\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 0s - loss: 0.1958 - acc: 0.9300 - val_loss: 0.1648 - val_acc: 0.9800\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s - loss: 0.1974 - acc: 0.9300 - val_loss: 0.1688 - val_acc: 0.9800\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 0s - loss: 0.1931 - acc: 0.9300 - val_loss: 0.1588 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 0s - loss: 0.1951 - acc: 0.9300 - val_loss: 0.1599 - val_acc: 0.9800\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 0s - loss: 0.1918 - acc: 0.9300 - val_loss: 0.1598 - val_acc: 0.9800\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 0s - loss: 0.1900 - acc: 0.9300 - val_loss: 0.1541 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 0s - loss: 0.1887 - acc: 0.9300 - val_loss: 0.1574 - val_acc: 0.9800\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 0s - loss: 0.1861 - acc: 0.9300 - val_loss: 0.1546 - val_acc: 0.9800\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 0s - loss: 0.1847 - acc: 0.9300 - val_loss: 0.1508 - val_acc: 0.9800\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 0s - loss: 0.1837 - acc: 0.9300 - val_loss: 0.1510 - val_acc: 0.9800\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 0s - loss: 0.1854 - acc: 0.9300 - val_loss: 0.1522 - val_acc: 0.9800\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 0s - loss: 0.1815 - acc: 0.9400 - val_loss: 0.1457 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 0s - loss: 0.1800 - acc: 0.9400 - val_loss: 0.1467 - val_acc: 0.9800\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 0s - loss: 0.1809 - acc: 0.9300 - val_loss: 0.1447 - val_acc: 0.9800\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 0s - loss: 0.1774 - acc: 0.9300 - val_loss: 0.1453 - val_acc: 0.9800\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1701 - acc: 1.000 - 0s - loss: 0.1779 - acc: 0.9300 - val_loss: 0.1433 - val_acc: 0.9800\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 0s - loss: 0.1762 - acc: 0.9300 - val_loss: 0.1397 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 0s - loss: 0.1736 - acc: 0.9300 - val_loss: 0.1420 - val_acc: 0.9800\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 0s - loss: 0.1740 - acc: 0.9400 - val_loss: 0.1413 - val_acc: 0.9800\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 0s - loss: 0.1713 - acc: 0.9400 - val_loss: 0.1382 - val_acc: 0.9800\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 0s - loss: 0.1704 - acc: 0.9300 - val_loss: 0.1358 - val_acc: 0.9800\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 0s - loss: 0.1694 - acc: 0.9300 - val_loss: 0.1352 - val_acc: 0.9800\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 0s - loss: 0.1680 - acc: 0.9300 - val_loss: 0.1347 - val_acc: 0.9800\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 0s - loss: 0.1670 - acc: 0.9400 - val_loss: 0.1348 - val_acc: 0.9800\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 0s - loss: 0.1682 - acc: 0.9300 - val_loss: 0.1316 - val_acc: 0.9800\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 0s - loss: 0.1661 - acc: 0.9400 - val_loss: 0.1347 - val_acc: 0.9800\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 0s - loss: 0.1655 - acc: 0.9400 - val_loss: 0.1331 - val_acc: 0.9800\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 0s - loss: 0.1658 - acc: 0.9300 - val_loss: 0.1279 - val_acc: 0.9800\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 0s - loss: 0.1619 - acc: 0.9400 - val_loss: 0.1316 - val_acc: 0.9800\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 0s - loss: 0.1609 - acc: 0.9400 - val_loss: 0.1295 - val_acc: 0.9800\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 0s - loss: 0.1602 - acc: 0.9400 - val_loss: 0.1257 - val_acc: 0.9800\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 0s - loss: 0.1611 - acc: 0.9400 - val_loss: 0.1267 - val_acc: 0.9800\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 0s - loss: 0.1579 - acc: 0.9400 - val_loss: 0.1223 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 0s - loss: 0.1589 - acc: 0.9300 - val_loss: 0.1217 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 0s - loss: 0.1565 - acc: 0.9300 - val_loss: 0.1219 - val_acc: 0.9800\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 0s - loss: 0.1576 - acc: 0.9400 - val_loss: 0.1204 - val_acc: 0.9800\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 0s - loss: 0.1544 - acc: 0.9400 - val_loss: 0.1224 - val_acc: 0.9800\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 0s - loss: 0.1562 - acc: 0.9300 - val_loss: 0.1193 - val_acc: 0.9800\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 0s - loss: 0.1545 - acc: 0.9500 - val_loss: 0.1229 - val_acc: 0.9800\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 0s - loss: 0.1583 - acc: 0.9400 - val_loss: 0.1129 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 0s - loss: 0.1503 - acc: 0.9400 - val_loss: 0.1196 - val_acc: 0.9800\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 0s - loss: 0.1520 - acc: 0.9500 - val_loss: 0.1201 - val_acc: 0.9800\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 0s - loss: 0.1497 - acc: 0.9500 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 0s - loss: 0.1505 - acc: 0.9400 - val_loss: 0.1155 - val_acc: 0.9800\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 0s - loss: 0.1483 - acc: 0.9400 - val_loss: 0.1122 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 0s - loss: 0.1482 - acc: 0.9300 - val_loss: 0.1095 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 0s - loss: 0.1466 - acc: 0.9400 - val_loss: 0.1131 - val_acc: 0.9800\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 0s - loss: 0.1464 - acc: 0.9400 - val_loss: 0.1101 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 0s - loss: 0.1467 - acc: 0.9400 - val_loss: 0.1126 - val_acc: 0.9800\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 0s - loss: 0.1447 - acc: 0.9400 - val_loss: 0.1090 - val_acc: 0.9800\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 0s - loss: 0.1445 - acc: 0.9400 - val_loss: 0.1085 - val_acc: 0.9800\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 0s - loss: 0.1438 - acc: 0.9400 - val_loss: 0.1074 - val_acc: 0.9800\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 0s - loss: 0.1460 - acc: 0.9500 - val_loss: 0.1085 - val_acc: 0.9800\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 0s - loss: 0.1420 - acc: 0.9400 - val_loss: 0.1029 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 0s - loss: 0.1436 - acc: 0.9400 - val_loss: 0.1046 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 0s - loss: 0.1419 - acc: 0.9400 - val_loss: 0.1023 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 0s - loss: 0.1402 - acc: 0.9400 - val_loss: 0.1064 - val_acc: 0.9800\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 0s - loss: 0.1389 - acc: 0.9400 - val_loss: 0.1051 - val_acc: 0.9800\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 0s - loss: 0.1384 - acc: 0.9400 - val_loss: 0.1015 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 0s - loss: 0.1402 - acc: 0.9400 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 0s - loss: 0.1384 - acc: 0.9500 - val_loss: 0.0983 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 0s - loss: 0.1353 - acc: 0.9500 - val_loss: 0.1040 - val_acc: 0.9800\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 0s - loss: 0.1363 - acc: 0.9400 - val_loss: 0.1030 - val_acc: 0.9800\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 0s - loss: 0.1357 - acc: 0.9400 - val_loss: 0.0985 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 0s - loss: 0.1367 - acc: 0.9500 - val_loss: 0.1004 - val_acc: 0.9800\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 0s - loss: 0.1350 - acc: 0.9400 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 0s - loss: 0.1361 - acc: 0.9400 - val_loss: 0.0973 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 0s - loss: 0.1351 - acc: 0.9300 - val_loss: 0.0953 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 0s - loss: 0.1334 - acc: 0.9400 - val_loss: 0.0996 - val_acc: 0.9800\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 0s - loss: 0.1330 - acc: 0.9400 - val_loss: 0.0951 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 0s - loss: 0.1331 - acc: 0.9400 - val_loss: 0.0949 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 0s - loss: 0.1312 - acc: 0.9400 - val_loss: 0.0949 - val_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f961dc7bc88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "model = baseline_model()\n",
    "model.fit(X_train, y_train, batch_size=5, epochs=200, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_classes(X_train))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " model.predict is giving all of one class. THis doesn't make sense given the output of accuracy for training loss.\n",
    " What if I simplify the model?\n",
    "\n",
    " With Jason Brownlee's model I am still predicting most of one class after 9 epochs\n",
    " \n",
    " After 500 epochs I get to 83% accuracy on training, 82% accuracy on test\n",
    " \n",
    " After 1000 epochs, it actually didn't do so well, only 57% accuracy, maybe from something stochastic.\n",
    " \n",
    " After 5000 epochs, it gets to 98% training accuracy, 100% test accuracy. Let's see what it looks like on predicting. Yes, it predicts totally correctly\n",
    "\n",
    " 5000 epochs seems like a long time. What about if I decrease the batch size?\n",
    " \n",
    " Yes, batch size of 5 instead of the whole training set did alow me to train quicker (in just 200 epochs, 94% training accuracy, 100% test accuracy)\n",
    " \n",
    " -  I guess that's part of the benefit of minibatching\n",
    "\n",
    "NOw, I still do not know why my previous model can get 100% training accuracy but not actually train well. Might it have to do with the batch normalization?\n",
    "\n",
    "So I repeat my previous error of high training accuracy, no validation accuracy, but predicts one class on training and test. Now I will remove batch normalization.\n",
    "\n",
    "Removing the first batch normalization allowed me to predict all samples as belonging to one of two classes. But I still have the training loss bug of high accuracy but actually poor prediction. I have no idea why this is occuring. I am going to put back intial batch normalization and remove all batch normalization in the middle.\n",
    "\n",
    "Having first batch normalization and removing all middle batch normalizations did not remove the bug of high training accuracy in model fit output but still just predicting 1 class at prediction time. Going 50 epochs with this model the first time did eventually increase validation score.\n",
    "\n",
    "Removing all dropoout lines only did not remove the bug. How about I remove all batch normalization etc., but still have a deep network\n",
    "\n",
    "Just dense layers and the bug is gone. I have 66% accuracy on both training and test, and this is reflected in 1 class correct and all others predicted a different class. Can I go up all the way with more epochs (and this is batch size full dataset by the way...)\n",
    "\n",
    "50 epochs with the full network, I get up to near 100% accuracy on both sets. I want to try 50 epochs on Jason's network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the results of these experiments? It looks preliminarily that having several layers speeds up training when there are no minibatches. I do not know yet if it speeds of uptraining relative to the shallow network when batch size is 5. That is something to try.\n",
    "\n",
    "I hypothesize now that something with batch normalization is causing the bug of apparent high training accuracy from the output but actually poor training accuracy when I predict on the currently trained model on the training set. This may just be some code bug. The validation set accurately depicts the error. Therefore, I could potentially still use such a network as long as I use a confusion matrix to compare it with other networks. To test this hypothesis, I just need to add back in all batch normalization to get the error.\n",
    "\n",
    "So add back in first normalization, add back in last and add in all.\n",
    "Add in all with 50 epochs, still have the bug and the model doesn't converge yet.\n",
    "Remove first batch normalization, still just predicts 3 classes.\n",
    "\n",
    "Remove all batch normalization, gets to 95% accuracty in 8 epochs, 98-100% accuracy in 50.\n",
    "\n",
    "Now just first batch normalization, predicting 1 of 2 classes 66% accuracy by epoch 7 and it doesn't change by epoch 50. There is no mismatch bug in accuracy. Can we go more epochs and get it to converge? Yes, after 300 epochs we get convergence with near 100% accuracty on training and validation. It may be now that the bug is still present because the training accuracy went high initially (basically 100%) but the test accuracy took a lot longer, 243 epochs until it was 96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end message is that batchnormalization is bad for the IRIS dataset. Maybe it is bad for structured data in general. Perhaps i should look to Jeremy Howard's structured dataset network for inspiration on structured data.\n",
    "\n",
    "I still don't know what hte origin of the bug is, but it is clearly related to batch normalization, especially in the middle layers. Maybe i am using it incorrectly with the theano backend? I should look this up, but in the meantime I can try with the monkey data with no batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(model.predict_classes(X_train))\n",
    "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "lb.classes_\n",
    "#array([1, 2, 4, 6])\n",
    "lb.transform([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 1 2 2 2 0 2 2 1 2 1 1 0 0 2 2 0 2 0 2 2 0 2 1 0 1 2 1 2 0 1 0 1 2 0\n",
      " 2 0 2 1 1 2 0 1 1 2 0 1 0 2 1 2 0 1 1 2 1 1 1 0 1 0 0 2 0 1 1 0 2 0 0 0 2\n",
      " 0 2 2 0 0 0 2 1 0 0 2 2 1 1 1 1 2 0 1 0 2 2 1 2 1 2]\n",
      "[0 2 0 1 2 2 2 0 2 2 1 2 1 1 0 0 2 2 0 2 0 2 2 0 2 1 0 1 2 1 2 0 1 0 1 2 0\n",
      " 2 0 2 1 1 2 0 1 1 2 0 1 0 2 1 2 0 1 1 1 1 1 1 0 1 0 0 2 0 1 1 0 2 0 0 0 2\n",
      " 0 2 2 0 0 0 2 1 0 0 2 1 1 1 1 1 2 0 1 0 2 2 1 2 1 2]\n",
      "Training Confusion Matrix\n",
      "[[33  0  0]\n",
      " [ 0 31  2]\n",
      " [ 0  0 34]]\n",
      "Test Confusion Matrix\n",
      "[[16  1  0]\n",
      " [ 0 16  1]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(X_train, verbose=False))\n",
    "\n",
    "print(lb.inverse_transform(y_train))\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(confusion_matrix(lb.inverse_transform(y_train),model.predict_classes(X_train, verbose=False)))\n",
    "print(\"Test Confusion Matrix\")\n",
    "print(confusion_matrix(lb.inverse_transform(y_test),model.predict_classes(X_test, verbose=False)))\n",
    "#array([[1, 0, 0, 0],\n",
    "#       [0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.model_selection.train_test_split(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y, test_size=0.33, random_state=42)\n",
    "    X, dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(100, 4)\n",
      "(50, 4)\n",
      "(150, 3)\n",
      "(100, 3)\n",
      "(50, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from my state farm distracted driver code\n",
    "import random\n",
    "random.seed(100)   # So subjects selected are consistent\n",
    "b =set(np.random.permutation(a['subject']))\n",
    "subs_val = random.sample(b - set('p072'), 3)# Decided on 3 drivers with further consultation from Jeremy Howard's notebook\n",
    "print(\"Validation subjects: \" + ', '.join(subs_val))\n",
    "\n",
    "a['val.file'] = a[['classname', 'img']].apply(lambda x: '/'.join(x), axis=1)\n",
    "tab_val = a.loc[a['subject'].isin(subs_val)]\n",
    "val_files =tab_val['val.file'].tolist()\n",
    "val_files[0:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
